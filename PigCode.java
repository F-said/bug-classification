/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.PigWarning;
import org.apache.pig.builtin.BinStorage;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.builtin.FindQuantiles;
import org.apache.pig.impl.builtin.RandomSampleLoader;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.executionengine.ExecutionEngine;
import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROpPlanVisitor;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.UDFFinder;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
import org.apache.pig.impl.plan.CompilationMessageCollector;
import org.apache.pig.impl.plan.DepthFirstWalker;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.OperatorPlan;
import org.apache.pig.impl.plan.PlanException;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;

/**
 * The compiler that compiles a given physical plan
 * into a DAG of MapReduce operators which can then 
 * be converted into the JobControl structure.
 * 
 * Is implemented as a visitor of the PhysicalPlan it
 * is compiling.
 * 
 * Currently supports all operators except the MR Sort
 * operator 
 * 
 * Uses a predecessor based depth first traversal. 
 * To compile an operator, first compiles
 * the predecessors into MapReduce Operators and tries to
 * merge the current operator into one of them. The goal
 * being to keep the number of MROpers to a minimum.
 * 
 * It also merges multiple Map jobs, created by compiling
 * the inputs individually, into a single job. Here a new
 * map job is created and then the contents of the previous
 * map plans are added. However, any other state that was in
 * the previous map plans, should be manually moved over. So,
 * if you are adding something new take care about this.
 * Ex of this is in requestedParallelism
 * 
 * Only in case of blocking operators and splits, a new 
 * MapReduce operator is started using a store-load combination
 * to connect the two operators. Whenever this happens
 * care is taken to add the MROper into the MRPlan and connect it
 * appropriately.
 * 
 *
 */
public class MRCompiler extends PhyPlanVisitor {
    
    private Log log = LogFactory.getLog(getClass());
    
    PigContext pigContext;
    
    //The plan that is being compiled
    PhysicalPlan plan;

    //The plan of MapReduce Operators
    MROperPlan MRPlan;
    
    //The current MapReduce Operator
    //that is being compiled
    MapReduceOper curMROp;
    
    //The output of compiling the inputs
    MapReduceOper[] compiledInputs = null;

    //Mapping of which MapReduceOper a store belongs to.
    Map<POStore, MapReduceOper> storeToMapReduceMap;
    
    //The split operators seen till now. If not
    //maintained they will haunt you.
    //During the traversal a split is the only
    //operator that can be revisited from a different
    //path. So this map stores the split job. So 
    //whenever we hit the split, we create a new MROper
    //and connect the split job using load-store and also
    //in the MRPlan
    Map<OperatorKey, MapReduceOper> splitsSeen;
    
    NodeIdGenerator nig;

    private String scope;
    
    private Random r;
    
    private UDFFinder udfFinder;
    
    private CompilationMessageCollector messageCollector = null;
    
    public static String USER_COMPARATOR_MARKER = "user.comparator.func:";
    
    public MRCompiler(PhysicalPlan plan) throws MRCompilerException {
        this(plan,null);
    }
    
    public MRCompiler(PhysicalPlan plan,
            PigContext pigContext) throws MRCompilerException {
        super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(plan));
        this.plan = plan;
        this.pigContext = pigContext;
        splitsSeen = new HashMap<OperatorKey, MapReduceOper>();
        MRPlan = new MROperPlan();
        nig = NodeIdGenerator.getGenerator();
        r = new Random(1331);
        FileLocalizer.setR(r);
        udfFinder = new UDFFinder();
        List<PhysicalOperator> roots = plan.getRoots();
        if((roots == null) || (roots.size() <= 0)) {
        	int errCode = 2053;
        	String msg = "Internal error. Did not find roots in the physical plan.";
        	throw new MRCompilerException(msg, errCode, PigException.BUG);
        }
        scope = roots.get(0).getOperatorKey().getScope();
        messageCollector = new CompilationMessageCollector() ;
        storeToMapReduceMap = new HashMap<POStore, MapReduceOper>();
    }
    
    public void randomizeFileLocalizer(){
        FileLocalizer.setR(new Random());
    }
    
    /**
     * Used to get the compiled plan
     * @return map reduce plan built by the compiler
     */
    public MROperPlan getMRPlan() {
        return MRPlan;
    }
    
    /**
     * Used to get the plan that was compiled
     * @return physical plan
     */
    public PhysicalPlan getPlan() {
        return plan;
    }
    
    public CompilationMessageCollector getMessageCollector() {
    	return messageCollector;
    }
    
    /**
     * The front-end method that the user calls to compile
     * the plan. Assumes that all submitted plans have a Store
     * operators as the leaf.
     * @return A map reduce plan
     * @throws IOException
     * @throws PlanException
     * @throws VisitorException
     */
    public MROperPlan compile() throws IOException, PlanException, VisitorException {
        List<PhysicalOperator> leaves = plan.getLeaves();

        for (PhysicalOperator op : leaves) {
            if (!(op instanceof POStore)) {
                int errCode = 2025;
                String msg = "Expected leaf of reduce plan to " +
                    "always be POStore. Found " + op.getClass().getSimpleName();
                throw new MRCompilerException(msg, errCode, PigException.BUG);
            }
        }

        List<POStore> stores = PlanHelper.getStores(plan);
        for (POStore store: stores) {
            compile(store);
        }

        // I'm quite certain this is not the best way to do this.  The issue
        // is that for jobs that take multiple map reduce passes, for
        // non-sort jobs, the POLocalRearrange is being put into the reduce
        // of MR job n, with the map for MR job n+1 empty and the POPackage
        // in reduce of MR job n+1.  This causes problems in the collect of
        // the map MR job n+1.  To resolve this, the following visitor
        // walks the resulting compiled jobs, looks for the pattern described
        // above, and then moves the POLocalRearrange to the map of MR job
        // n+1.  It seems to me there are two possible better solutions:
        // 1) Change the logic in this compiler to put POLocalRearrange in
        // the correct place to begin with instead of patching it up later.
        // I'd do this but I don't fully understand the logic here and it's
        // complex.
        // 2) Change our map reduce execution to have a reduce only mode.  In
        // this case the map would not even try to parse the input, it would
        // just be 100% pass through.  I suspect this might be better though
        // I don't fully understand the consequences of this.
        // Given these issues, the following works for now, and we can fine
        // tune it when Shravan returns.
        RearrangeAdjuster ra = new RearrangeAdjuster(MRPlan);
        ra.visit();
        
        LimitAdjuster la = new LimitAdjuster(MRPlan);
        la.visit();
        la.adjust();
        
        return MRPlan;
    }
    
    /**
     * Compiles the plan below op into a MapReduce Operator
     * and stores it in curMROp.
     * @param op
     * @throws IOException
     * @throws PlanException
     * @throws VisitorException
     */
    private void compile(PhysicalOperator op) throws IOException,
    PlanException, VisitorException {
        //An artifact of the Visitor. Need to save
        //this so that it is not overwritten.
        MapReduceOper[] prevCompInp = compiledInputs;
        
        //Compile each predecessor into the MROper and 
        //store them away so that we can use them for compiling
        //op.
        List<PhysicalOperator> predecessors = plan.getPredecessors(op);
        if (predecessors != null && predecessors.size() > 0) {
            // When processing an entire script (multiquery), we can
            // get into a situation where a load has
            // predecessors. This means that it depends on some store
            // earlier in the plan. We need to take that dependency
            // and connect the respective MR operators, while at the
            // same time removing the connection between the Physical
            // operators. That way the jobs will run in the right
            // order.
            if (op instanceof POLoad) {

                if (predecessors.size() != 1) {
                    int errCode = 2125;
                    String msg = "Expected at most one predecessor of load. Got "+predecessors.size();
                    throw new PlanException(msg, errCode, PigException.BUG);
                }

                PhysicalOperator p = predecessors.get(0);
                if (!(p instanceof POStore)) {
                    int errCode = 2126;
                    String msg = "Predecessor of load should be a store. Got "+p.getClass();
                    throw new PlanException(msg, errCode, PigException.BUG);
                }

                // Need new operator
                curMROp = getMROp();
                curMROp.mapPlan.add(op);
                MRPlan.add(curMROp);
                
                MapReduceOper oper = storeToMapReduceMap.get((POStore)p);

                plan.disconnect(op, p);
                MRPlan.connect(oper, curMROp);
                return;
            }
            
            Collections.sort(predecessors);
            compiledInputs = new MapReduceOper[predecessors.size()];
            int i = -1;
            for (PhysicalOperator pred : predecessors) {
                if(pred instanceof POSplit && splitsSeen.containsKey(pred.getOperatorKey())){
                    compiledInputs[++i] = startNew(((POSplit)pred).getSplitStore(), splitsSeen.get(pred.getOperatorKey()));
                    continue;
                }
                compile(pred);
                compiledInputs[++i] = curMROp;
            }
        } else {
            //No predecessors. Mostly a load. But this is where
            //we start. We create a new MROp and add its first
            //operator op. Also this should be added to the MRPlan.
            curMROp = getMROp();
            curMROp.mapPlan.add(op);
            MRPlan.add(curMROp);
            return;
        }
        
        //Now we have the inputs compiled. Do something
        //with the input oper op.
        op.visit(this);
        if(op.getRequestedParallelism() > curMROp.requestedParallelism)
            curMROp.requestedParallelism = op.getRequestedParallelism();
        compiledInputs = prevCompInp;
    }
    
    private MapReduceOper getMROp(){
        return new MapReduceOper(new OperatorKey(scope,nig.getNextNodeId(scope)));
    }
    
    private POLoad getLoad(){
        POLoad ld = new POLoad(new OperatorKey(scope,nig.getNextNodeId(scope)), true);
        ld.setPc(pigContext);
        return ld;
    }
    
    private POStore getStore(){
        POStore st = new POStore(new OperatorKey(scope,nig.getNextNodeId(scope)));
        // mark store as tmp store. These could be removed by the
        // optimizer, because it wasn't the user requesting it.
        st.setIsTmpStore(true);
        return st;
    }
    
    /**
     * A map MROper is an MROper whose map plan is still open
     * for taking more non-blocking operators.
     * A reduce MROper is an MROper whose map plan is done but
     * the reduce plan is open for taking more non-blocking opers.
     * 
     * Used for compiling non-blocking operators. The logic here
     * is simple. If there is a single input, just push the operator
     * into whichever phase is open. Otherwise, we merge the compiled
     * inputs into a list of MROpers where the first oper is the merged
     * oper consisting of all map MROpers and the rest are reduce MROpers
     * as reduce plans can't be merged.
     * Then we add the input oper op into the merged map MROper's map plan
     * as a leaf and connect the reduce MROpers using store-load combinations
     * to the input operator which is the leaf. Also care is taken to 
     * connect the MROpers according to the dependencies.
     * @param op
     * @throws PlanException
     * @throws IOException
     */
    private void nonBlocking(PhysicalOperator op) throws PlanException, IOException{
        
        if (compiledInputs.length == 1) {
            //For speed
            MapReduceOper mro = compiledInputs[0];
            if (!mro.isMapDone()) {
                mro.mapPlan.addAsLeaf(op);
            } else if (mro.isMapDone() && !mro.isReduceDone()) {
                mro.reducePlan.addAsLeaf(op);
            } else {
                int errCode = 2022;
                String msg = "Both map and reduce phases have been done. This is unexpected while compiling.";                
                throw new PlanException(msg, errCode, PigException.BUG);
            }
            curMROp = mro;
        } else {
            List<MapReduceOper> mergedPlans = merge(compiledInputs);
            
            //The first MROper is always the merged map MROper
            MapReduceOper mro = mergedPlans.remove(0);
            //Push the input operator into the merged map MROper
            mro.mapPlan.addAsLeaf(op);
            
            //Connect all the reduce MROpers
            if(mergedPlans.size()>0)
                connRedOper(mergedPlans, mro);
            
            //return the compiled MROper
            curMROp = mro;
        }
    }
    
    /**
     * Used for compiling blocking operators. If there is a single input
     * and its map phase is still open, then close it so that further
     * operators can be compiled into the reduce phase. If its reduce phase
     * is open, add a store and close it. Start a new map MROper into which
     * further operators can be compiled into. 
     * 
     * If there are multiple inputs, the logic 
     * is to merge all map MROpers into one map MROper and retain
     * the reduce MROpers. Since the operator is blocking, it has
     * to be a Global Rerrange at least now. This operator need not
     * be inserted into our plan as it is implemented by hadoop.
     * But this creates the map-reduce boundary. So the merged map MROper
     * is closed and its reduce phase is started. Depending on the number
     * of reduce MROpers and the number of pipelines in the map MRoper
     * a Union operator is inserted whenever necessary. This also leads to the 
     * possibility of empty map plans. So have to be careful while handling
     * it in the PigMapReduce class. If there are no map
     * plans, then a new one is created as a side effect of the merge
     * process. If there are no reduce MROpers, and only a single pipeline
     * in the map, then no union oper is added. Otherwise a Union oper is 
     * added to the merged map MROper to which all the reduce MROpers 
     * are connected by store-load combinations. Care is taken
     * to connect the MROpers in the MRPlan.  
     * @param op
     * @throws IOException
     * @throws PlanException
     */
    private void blocking(PhysicalOperator op) throws IOException, PlanException{
        if(compiledInputs.length==1){
            MapReduceOper mro = compiledInputs[0];
            if (!mro.isMapDone()) {
                mro.setMapDoneSingle(true);
                curMROp = mro;
            }
            else if(mro.isMapDone() && !mro.isReduceDone()){
                FileSpec fSpec = getTempFileSpec();
                
                POStore st = getStore();
                st.setSFile(fSpec);
                mro.reducePlan.addAsLeaf(st);
                mro.setReduceDone(true);
                curMROp = startNew(fSpec, mro);
                curMROp.setMapDone(true);
            }
        }
        else{
            List<MapReduceOper> mergedPlans = merge(compiledInputs);
            MapReduceOper mro = mergedPlans.remove(0);
            
            if(mergedPlans.size()>0)
                mro.setMapDoneMultiple(true);
            else
                mro.setMapDoneSingle(true);

            // Connect all the reduce MROpers
            if(mergedPlans.size()>0)
                connRedOper(mergedPlans, mro);
            curMROp = mro;
        }
    }
    
    /**
     * Connect the reduce MROpers to the leaf node in the map MROper mro
     * by adding appropriate loads
     * @param mergedPlans - The list of reduce MROpers
     * @param mro - The map MROper
     * @throws PlanException 
     * @throws IOException
     */
    private void connRedOper(List<MapReduceOper> mergedPlans, MapReduceOper mro) throws PlanException, IOException{
        PhysicalOperator leaf = null;
        List<PhysicalOperator> leaves = mro.mapPlan.getLeaves();
        if(leaves!=null && leaves.size()>0)
            leaf = leaves.get(0);

        for (MapReduceOper mmro : mergedPlans) {
            mmro.setReduceDone(true);
            FileSpec fileSpec = getTempFileSpec();
            POLoad ld = getLoad();
            ld.setLFile(fileSpec);
            POStore str = getStore();
            str.setSFile(fileSpec);
            mmro.reducePlan.addAsLeaf(str);
            mro.mapPlan.add(ld);
            if(leaf!=null)
                mro.mapPlan.connect(ld, leaf);
            MRPlan.connect(mmro, mro);
        }
    }
    
    
    private MapReduceOper endSingleInputPlanWithStr(FileSpec fSpec) throws PlanException{
        if(compiledInputs.length>1) {
            int errCode = 2023;
            String msg = "Received a multi input plan when expecting only a single input one.";
            throw new PlanException(msg, errCode, PigException.BUG);
        }
        MapReduceOper mro = compiledInputs[0];
        POStore str = getStore();
        str.setSFile(fSpec);
        if (!mro.isMapDone()) {
            mro.mapPlan.addAsLeaf(str);
            mro.setMapDoneSingle(true);
        } else if (mro.isMapDone() && !mro.isReduceDone()) {
            mro.reducePlan.addAsLeaf(str);
            mro.setReduceDone(true);
        } else {
            int errCode = 2022;
            String msg = "Both map and reduce phases have been done. This is unexpected while compiling.";
            throw new PlanException(msg, errCode, PigException.BUG);
        }
        return mro;
    }
    
    /**
     * Starts a new MRoper and connects it to the old
     * one by load-store. The assumption is that the 
     * store is already inserted into the old MROper.
     * @param fSpec
     * @param old
     * @return
     * @throws IOException
     * @throws PlanException 
     */
    private MapReduceOper startNew(FileSpec fSpec, MapReduceOper old) throws PlanException{
        POLoad ld = getLoad();
        ld.setLFile(fSpec);
        MapReduceOper ret = getMROp();
        ret.mapPlan.add(ld);
        MRPlan.add(ret);
        MRPlan.connect(old, ret);
        return ret;
    }
 
    /**
     * Returns a temporary DFS Path
     * @return
     * @throws IOException
     */
    private FileSpec getTempFileSpec() throws IOException {
        return new FileSpec(FileLocalizer.getTemporaryPath(null, pigContext).toString(),
                new FuncSpec(BinStorage.class.getName()));
    }
    
    /**
     * Merges the map MROpers in the compiledInputs into a single
     * merged map MRoper and returns a List with the merged map MROper
     * as the first oper and the rest being reduce MROpers.
     * 
     * Care is taken to remove the map MROpers that are merged from the
     * MRPlan and their connections moved over to the merged map MROper.
     * 
     * Merge is implemented as a sequence of binary merges.
     * merge(PhyPlan finPlan, List<PhyPlan> lst) := finPlan,merge(p) foreach p in lst 
     *   
     * @param compiledInputs
     * @return
     * @throws PlanException
     * @throws IOException
     */
    private List<MapReduceOper> merge(MapReduceOper[] compiledInputs)
            throws PlanException {
        List<MapReduceOper> ret = new ArrayList<MapReduceOper>();
        
        MapReduceOper mergedMap = getMROp();
        ret.add(mergedMap);
        MRPlan.add(mergedMap);
        
        Set<MapReduceOper> toBeConnected = new HashSet<MapReduceOper>();
        List<MapReduceOper> remLst = new ArrayList<MapReduceOper>();

        List<PhysicalPlan> mpLst = new ArrayList<PhysicalPlan>();

        for (MapReduceOper mro : compiledInputs) {
            if (!mro.isMapDone()) {
                remLst.add(mro);
                mpLst.add(mro.mapPlan);
                List<MapReduceOper> pmros = MRPlan.getPredecessors(mro);
                if(pmros!=null){
                    for(MapReduceOper pmro : pmros)
                        toBeConnected.add(pmro);
                }
            } else if (mro.isMapDone() && !mro.isReduceDone()) {
                ret.add(mro);
            } else {
                int errCode = 2027;
                String msg = "Both map and reduce phases have been done. This is unexpected for a merge."; 
                throw new PlanException(msg, errCode, PigException.BUG);
            }
        }
        merge(ret.get(0).mapPlan, mpLst);
        
        Iterator<MapReduceOper> it = toBeConnected.iterator();
        while(it.hasNext())
            MRPlan.connect(it.next(), mergedMap);
        for(MapReduceOper rmro : remLst){
            if(rmro.requestedParallelism > mergedMap.requestedParallelism)
                mergedMap.requestedParallelism = rmro.requestedParallelism;
            MRPlan.remove(rmro);
        }
        return ret;
    }
    
    /**
     * The merge of a list of map plans
     * @param <O>
     * @param <E>
     * @param finPlan - Final Plan into which the list of plans is merged
     * @param plans - list of map plans to be merged
     * @throws PlanException
     */
    private <O extends Operator, E extends OperatorPlan<O>> void merge(
            E finPlan, List<E> plans) throws PlanException {
        for (E e : plans) {
            finPlan.merge(e);
        }
    }

    private void addUDFs(PhysicalPlan plan) throws VisitorException{
        if(plan!=null){
            udfFinder.setPlan(plan);
            udfFinder.visit();
            curMROp.UDFs.addAll(udfFinder.getUDFs());
        }
    }
    
    
    /* The visitOp methods that decide what to do with the current operator */
    
    /**
     * Compiles a split operator. The logic is to
     * close the split job by replacing the split oper by
     * a store and creating a new Map MRoper and return
     * that as the current MROper to which other operators
     * would be compiled into. The new MROper would be connected
     * to the split job by load-store. Also add the split oper 
     * to the splitsSeen map.
     * @param op - The split operator
     * @throws VisitorException
     */
    public void visitSplit(POSplit op) throws VisitorException{
        try{
            FileSpec fSpec = op.getSplitStore();
            MapReduceOper mro = endSingleInputPlanWithStr(fSpec);
            mro.setSplitter(true);
            splitsSeen.put(op.getOperatorKey(), mro);
            curMROp = startNew(fSpec, mro);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitLoad(POLoad op) throws VisitorException{
        try{
            nonBlocking(op);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitStore(POStore op) throws VisitorException{
        try{
            storeToMapReduceMap.put(op, curMROp);
            nonBlocking(op);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitFilter(POFilter op) throws VisitorException{
        try{
            nonBlocking(op);
            addUDFs(op.getPlan());
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitStream(POStream op) throws VisitorException{
        try{
            nonBlocking(op);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void simpleConnectMapToReduce(MapReduceOper mro) throws PlanException
    {
    	PhysicalPlan ep = new PhysicalPlan();
        POProject prjStar = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
        prjStar.setResultType(DataType.TUPLE);
        prjStar.setStar(true);
        ep.add(prjStar);
        
        List<PhysicalPlan> eps = new ArrayList<PhysicalPlan>();
        eps.add(ep);
        
        POLocalRearrange lr = new POLocalRearrange(new OperatorKey(scope,nig.getNextNodeId(scope)));
        try {
            lr.setIndex(0);
        } catch (ExecException e) {
        	int errCode = 2058;
        	String msg = "Unable to set index on the newly created POLocalRearrange.";
            throw new PlanException(msg, errCode, PigException.BUG, e);
        }
        lr.setKeyType(DataType.TUPLE);
        lr.setPlans(eps);
        lr.setResultType(DataType.TUPLE);
        
        mro.mapPlan.addAsLeaf(lr);
        
        POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
        pkg.setKeyType(DataType.TUPLE);
        pkg.setNumInps(1);
        boolean[] inner = {false};
        pkg.setInner(inner);
        mro.reducePlan.add(pkg);
        
        List<PhysicalPlan> eps1 = new ArrayList<PhysicalPlan>();
        List<Boolean> flat1 = new ArrayList<Boolean>();
        PhysicalPlan ep1 = new PhysicalPlan();
        POProject prj1 = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
        prj1.setResultType(DataType.TUPLE);
        prj1.setStar(false);
        prj1.setColumn(1);
        prj1.setOverloaded(true);
        ep1.add(prj1);
        eps1.add(ep1);
        flat1.add(true);
        POForEach nfe1 = new POForEach(new OperatorKey(scope, nig
                .getNextNodeId(scope)), -1, eps1, flat1);
        nfe1.setResultType(DataType.BAG);
        
        mro.reducePlan.addAsLeaf(nfe1);
    }
    
    public void visitLimit(POLimit op) throws VisitorException{
        try{
        	
            MapReduceOper mro = compiledInputs[0];
            mro.limit = op.getLimit();
            if (!mro.isMapDone()) {
            	// if map plan is open, add a limit for optimization, eventually we
            	// will add another limit to reduce plan
                mro.mapPlan.addAsLeaf(op);
                mro.setMapDone(true);
                
                if (mro.reducePlan.isEmpty())
                {
                    simpleConnectMapToReduce(mro);
                    mro.requestedParallelism = 1;
                    POLimit pLimit2 = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
                    pLimit2.setLimit(op.getLimit());
                    mro.reducePlan.addAsLeaf(pLimit2);
                }
                else
                {
                    messageCollector.collect("Something in the reduce plan while map plan is not done. Something wrong!", 
                    		MessageType.Warning, PigWarning.REDUCE_PLAN_NOT_EMPTY_WHILE_MAP_PLAN_UNDER_PROCESS);
                }
            } else if (mro.isMapDone() && !mro.isReduceDone()) {
            	// limit should add into reduce plan
                mro.reducePlan.addAsLeaf(op);
            } else {
            	messageCollector.collect("Both map and reduce phases have been done. This is unexpected while compiling!",
            			MessageType.Warning, PigWarning.UNREACHABLE_CODE_BOTH_MAP_AND_REDUCE_PLANS_PROCESSED);
            }
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }

    public void visitLocalRearrange(POLocalRearrange op) throws VisitorException {
        try{
            nonBlocking(op);
            List<PhysicalPlan> plans = op.getPlans();
            if(plans!=null)
                for(PhysicalPlan ep : plans)
                    addUDFs(ep);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitPOForEach(POForEach op) throws VisitorException{
        try{
            nonBlocking(op);
            List<PhysicalPlan> plans = op.getInputPlans();
            if(plans!=null)
                for (PhysicalPlan plan : plans) {
                    addUDFs(plan);
                }
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitGlobalRearrange(POGlobalRearrange op) throws VisitorException{
        try{
            blocking(op);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitPackage(POPackage op) throws VisitorException{
        try{
            nonBlocking(op);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    public void visitUnion(POUnion op) throws VisitorException{
        try{
            nonBlocking(op);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    /**
     * This is an operator which will have multiple inputs(= to number of join inputs)
     * But it prunes off all inputs but the fragment input and creates separate MR jobs
     * for each of the replicated inputs and uses these as the replicated files that
     * are configured in the POFRJoin operator. It also sets that this is FRJoin job
     * and some parametes associated with it.
     */
    @Override
    public void visitFRJoin(POFRJoin op) throws VisitorException {
        try{
            FileSpec[] replFiles = new FileSpec[op.getInputs().size()];
            for (int i=0; i<replFiles.length; i++) {
                if(i==op.getFragment()) continue;
                replFiles[i] = getTempFileSpec();
            }
            op.setReplFiles(replFiles);
            
            List<OperatorKey> opKeys = new ArrayList<OperatorKey>(op.getInputs().size());
            for (PhysicalOperator pop : op.getInputs()) {
                opKeys.add(pop.getOperatorKey());
            }
            int fragPlan = 0;
            for(int i=0;i<compiledInputs.length;i++){
                MapReduceOper mro = compiledInputs[i];
                OperatorKey opKey = (!mro.isMapDone()) ?  mro.mapPlan.getLeaves().get(0).getOperatorKey()
                                                       :  mro.reducePlan.getLeaves().get(0).getOperatorKey();
                if(opKeys.indexOf(opKey)==op.getFragment()){
                    curMROp = mro;
                    fragPlan = i;
                    continue;
                }
                POStore str = getStore();
                str.setSFile(replFiles[opKeys.indexOf(opKey)]);
                if (!mro.isMapDone()) {
                    mro.mapPlan.addAsLeaf(str);
                    mro.setMapDoneSingle(true);
                } else if (mro.isMapDone() && !mro.isReduceDone()) {
                    mro.reducePlan.addAsLeaf(str);
                    mro.setReduceDone(true);
                } else {
                	int errCode = 2022;
                    String msg = "Both map and reduce phases have been done. This is unexpected while compiling.";
                    throw new PlanException(msg, errCode, PigException.BUG);
                }
            }
            for(int i=0;i<compiledInputs.length;i++){
                if(i==fragPlan) continue;
                MRPlan.connect(compiledInputs[i], curMROp);
            }
            
            if (!curMROp.isMapDone()) {
                curMROp.mapPlan.addAsLeaf(op);
            } else if (curMROp.isMapDone() && !curMROp.isReduceDone()) {
                curMROp.reducePlan.addAsLeaf(op);
            } else {
            	int errCode = 2022;
                String msg = "Both map and reduce phases have been done. This is unexpected while compiling.";
                throw new PlanException(msg, errCode, PigException.BUG);
            }
            List<List<PhysicalPlan>> joinPlans = op.getJoinPlans();
            if(joinPlans!=null)
                for (List<PhysicalPlan> joinPlan : joinPlans) {
                    if(joinPlan!=null)
                        for (PhysicalPlan plan : joinPlan) {
                            addUDFs(plan);
                        }
                }
            curMROp.setFrjoin(true);
            curMROp.setFragment(op.getFragment());
            curMROp.setReplFiles(op.getReplFiles());
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visitDistinct(PODistinct op) throws VisitorException {
        try{
            MapReduceOper mro = compiledInputs[0];
            PhysicalPlan ep = new PhysicalPlan();
            POProject prjStar = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
            prjStar.setResultType(DataType.TUPLE);
            prjStar.setStar(true);
            ep.add(prjStar);
            
            List<PhysicalPlan> eps = new ArrayList<PhysicalPlan>();
            eps.add(ep);
            
            POLocalRearrange lr = new POLocalRearrange(new OperatorKey(scope,nig.getNextNodeId(scope)));
            lr.setIndex(0);
            lr.setKeyType(DataType.TUPLE);
            lr.setPlans(eps);
            lr.setResultType(DataType.TUPLE);
            lr.setDistinct(true);
            if(!mro.isMapDone()){
                mro.mapPlan.addAsLeaf(lr);
            }
            else if(mro.isMapDone() && ! mro.isReduceDone()){
                mro.reducePlan.addAsLeaf(lr);
            }
            
            blocking(op);
            
            POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
            pkg.setKeyType(DataType.TUPLE);
            pkg.setDistinct(true);
            pkg.setNumInps(1);
            boolean[] inner = {false}; 
            pkg.setInner(inner);
            curMROp.reducePlan.add(pkg);
            
            List<PhysicalPlan> eps1 = new ArrayList<PhysicalPlan>();
            List<Boolean> flat1 = new ArrayList<Boolean>();
            PhysicalPlan ep1 = new PhysicalPlan();
            POProject prj1 = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
            prj1.setResultType(DataType.TUPLE);
            prj1.setStar(false);
            prj1.setColumn(0);
            prj1.setOverloaded(false);
            ep1.add(prj1);
            eps1.add(ep1);
            flat1.add(true);
            POForEach nfe1 = new POForEach(new OperatorKey(scope, nig
                    .getNextNodeId(scope)), op.getRequestedParallelism(), eps1,
                    flat1);
            nfe1.setResultType(DataType.BAG);
            curMROp.reducePlan.addAsLeaf(nfe1);
            curMROp.setNeedsDistinctCombiner(true);
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visitSort(POSort op) throws VisitorException {
        try{
            FileSpec fSpec = getTempFileSpec();
            MapReduceOper mro = endSingleInputPlanWithStr(fSpec);
            FileSpec quantFile = getTempFileSpec();
            int rp = op.getRequestedParallelism();
            Pair<Integer,Byte>[] fields = getSortCols(op);
            Pair<MapReduceOper, Integer> quantJobParallelismPair = 
                getQuantileJob(op, mro, fSpec, quantFile, rp, fields);
            curMROp = getSortJob(op, quantJobParallelismPair.first, fSpec, quantFile, 
                    quantJobParallelismPair.second, fields);
            
            if(op.isUDFComparatorUsed){
                curMROp.UDFs.add(op.getMSortFunc().getFuncSpec().toString());
            }
        }catch(Exception e){
            int errCode = 2034;
            String msg = "Error compiling operator " + op.getClass().getSimpleName();
            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    private Pair<Integer, Byte>[] getSortCols(POSort sort) throws PlanException, ExecException {
        List<PhysicalPlan> plans = sort.getSortPlans();
        if(plans!=null){
            Pair[] ret = new Pair[plans.size()]; 
            int i=-1;
            for (PhysicalPlan plan : plans) {
                if (((POProject)plan.getLeaves().get(0)).isStar()) return null;
                int first = ((POProject)plan.getLeaves().get(0)).getColumn();
                byte second = ((POProject)plan.getLeaves().get(0)).getResultType();
                ret[++i] = new Pair<Integer,Byte>(first,second);
            }
            return ret;
        }
        int errCode = 2026;
        String msg = "No expression plan found in POSort.";
        throw new PlanException(msg, errCode, PigException.BUG);
    }
    
    public MapReduceOper getSortJob(
            POSort sort,
            MapReduceOper quantJob,
            FileSpec lFile,
            FileSpec quantFile,
            int rp,
            Pair<Integer,Byte>[] fields) throws PlanException{
        MapReduceOper mro = startNew(lFile, quantJob);
        mro.setQuantFile(quantFile.getFileName());
        mro.setGlobalSort(true);
        mro.requestedParallelism = rp;

        long limit = sort.getLimit();
        mro.limit = limit;
        
        List<PhysicalPlan> eps1 = new ArrayList<PhysicalPlan>();

        byte keyType = DataType.UNKNOWN;
        
        boolean[] sortOrder;

        List<Boolean> sortOrderList = sort.getMAscCols();
        if(sortOrderList != null) {
            sortOrder = new boolean[sortOrderList.size()];
            for(int i = 0; i < sortOrderList.size(); ++i) {
                sortOrder[i] = sortOrderList.get(i);
            }
            mro.setSortOrder(sortOrder);
        }

        if (fields == null) {
            // This is project *
            PhysicalPlan ep = new PhysicalPlan();
            POProject prj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
            prj.setStar(true);
            prj.setOverloaded(false);
            prj.setResultType(DataType.TUPLE);
            ep.add(prj);
            eps1.add(ep);
        } else {
            /*
            for (int i : fields) {
                PhysicalPlan ep = new PhysicalPlan();
                POProject prj = new POProject(new OperatorKey(scope,
                    nig.getNextNodeId(scope)));
                prj.setColumn(i);
                prj.setOverloaded(false);
                prj.setResultType(DataType.BYTEARRAY);
                ep.add(prj);
                eps1.add(ep);
            }
            */
            // Attach the sort plans to the local rearrange to get the
            // projection.
            eps1.addAll(sort.getSortPlans());

            // Visit the first sort plan to figure out our key type.  We only
            // have to visit the first because if we have more than one plan,
            // then the key type will be tuple.
            try {
                FindKeyTypeVisitor fktv =
                    new FindKeyTypeVisitor(sort.getSortPlans().get(0));
                fktv.visit();
                keyType = fktv.keyType;
            } catch (VisitorException ve) {
                int errCode = 2035;
                String msg = "Internal error. Could not compute key type of sort operator.";
                throw new PlanException(msg, errCode, PigException.BUG, ve);
            }
        }
        
        POLocalRearrange lr = new POLocalRearrange(new OperatorKey(scope,nig.getNextNodeId(scope)));
        try {
            lr.setIndex(0);
        } catch (ExecException e) {
        	int errCode = 2058;
        	String msg = "Unable to set index on newly created POLocalRearrange.";
            throw new PlanException(msg, errCode, PigException.BUG, e);
        }
        lr.setKeyType((fields == null || fields.length>1) ? DataType.TUPLE :
            keyType);
        lr.setPlans(eps1);
        lr.setResultType(DataType.TUPLE);
        mro.mapPlan.addAsLeaf(lr);
        
        mro.setMapDone(true);
        
        if (limit!=-1) {
       	    POPackageLite pkg_c = new POPackageLite(new OperatorKey(scope,nig.getNextNodeId(scope)));
       	    pkg_c.setKeyType((fields.length>1) ? DataType.TUPLE : keyType);
            pkg_c.setNumInps(1);
            //pkg.setResultType(DataType.TUPLE);
            mro.combinePlan.add(pkg_c);
        	
            List<PhysicalPlan> eps_c1 = new ArrayList<PhysicalPlan>();
            List<Boolean> flat_c1 = new ArrayList<Boolean>();
            PhysicalPlan ep_c1 = new PhysicalPlan();
            POProject prj_c1 = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
            prj_c1.setColumn(1);
            prj_c1.setOverloaded(false);
            prj_c1.setResultType(DataType.BAG);
            ep_c1.add(prj_c1);
            eps_c1.add(ep_c1);
            flat_c1.add(true);
            POForEach fe_c1 = new POForEach(new OperatorKey(scope,nig.getNextNodeId(scope)), 
            		-1, eps_c1, flat_c1);
            fe_c1.setResultType(DataType.TUPLE);
            mro.combinePlan.addAsLeaf(fe_c1);
            
            POLimit pLimit = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
        	pLimit.setLimit(limit);
        	mro.combinePlan.addAsLeaf(pLimit);
            
            List<PhysicalPlan> eps_c2 = new ArrayList<PhysicalPlan>();
            eps_c2.addAll(sort.getSortPlans());
        
	        POLocalRearrange lr_c2 = new POLocalRearrange(new OperatorKey(scope,nig.getNextNodeId(scope)));
	        try {
                lr_c2.setIndex(0);
            } catch (ExecException e) {
            	int errCode = 2058;
            	String msg = "Unable to set index on newly created POLocalRearrange.";            	
                throw new PlanException(msg, errCode, PigException.BUG, e);
            }
	        lr_c2.setKeyType((fields.length>1) ? DataType.TUPLE : keyType);
	        lr_c2.setPlans(eps_c2);
	        lr_c2.setResultType(DataType.TUPLE);
	        mro.combinePlan.addAsLeaf(lr_c2);
        }
        
        POPackageLite pkg = new POPackageLite(new OperatorKey(scope,nig.getNextNodeId(scope)));
        pkg.setKeyType((fields == null || fields.length>1) ? DataType.TUPLE :
            keyType);
        pkg.setNumInps(1);
        mro.reducePlan.add(pkg);
        
        PhysicalPlan ep = new PhysicalPlan();
        POProject prj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
        prj.setColumn(1);
        prj.setOverloaded(false);
        prj.setResultType(DataType.BAG);
        ep.add(prj);
        List<PhysicalPlan> eps2 = new ArrayList<PhysicalPlan>();
        eps2.add(ep);
        List<Boolean> flattened = new ArrayList<Boolean>();
        flattened.add(true);
        POForEach nfe1 = new POForEach(new OperatorKey(scope,nig.getNextNodeId(scope)),-1,eps2,flattened);
        mro.reducePlan.add(nfe1);
        mro.reducePlan.connect(pkg, nfe1);
        
        if (limit!=-1)
        {
	        POLimit pLimit2 = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
	    	pLimit2.setLimit(limit);
	    	mro.reducePlan.addAsLeaf(pLimit2);
        }

//        ep1.add(innGen);
        return mro;
    }

    public Pair<MapReduceOper,Integer> getQuantileJob(POSort inpSort, MapReduceOper prevJob, FileSpec lFile, FileSpec quantFile, int rp, Pair<Integer,Byte>[] fields) throws PlanException, VisitorException {
        FileSpec quantLdFilName = new FileSpec(lFile.getFileName(), new FuncSpec(RandomSampleLoader.class.getName()));
        MapReduceOper mro = startNew(quantLdFilName, prevJob);
        POSort sort = new POSort(inpSort.getOperatorKey(), inpSort
                .getRequestedParallelism(), null, inpSort.getSortPlans(),
                inpSort.getMAscCols(), inpSort.getMSortFunc());
        if(sort.isUDFComparatorUsed) {
            mro.UDFs.add(sort.getMSortFunc().getFuncSpec().toString());
        }
        
        List<PhysicalPlan> eps1 = new ArrayList<PhysicalPlan>();
        List<Boolean> flat1 = new ArrayList<Boolean>();
        // Set up the projections of the key columns 
        if (fields == null) {
            PhysicalPlan ep = new PhysicalPlan();
            POProject prj = new POProject(new OperatorKey(scope,
                nig.getNextNodeId(scope)));
            prj.setStar(true);
            prj.setOverloaded(false);
            prj.setResultType(DataType.TUPLE);
            ep.add(prj);
            eps1.add(ep);
            flat1.add(true);
        } else {
            for (Pair<Integer,Byte> i : fields) {
                PhysicalPlan ep = new PhysicalPlan();
                POProject prj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
                prj.setColumn(i.first);
                prj.setOverloaded(false);
                prj.setResultType(i.second);
                ep.add(prj);
                eps1.add(ep);
                flat1.add(true);
            }
        }
        // This foreach will pick the sort key columns from the RandomSampleLoader output 
        POForEach nfe1 = new POForEach(new OperatorKey(scope,nig.getNextNodeId(scope)),-1,eps1,flat1);
        mro.mapPlan.addAsLeaf(nfe1);
        
        // Now set up a POLocalRearrange which has "all" as the key and the output of the
        // foreach will be the "value" out of POLocalRearrange
        PhysicalPlan ep1 = new PhysicalPlan();
        ConstantExpression ce = new ConstantExpression(new OperatorKey(scope,nig.getNextNodeId(scope)));
        ce.setValue("all");
        ce.setResultType(DataType.CHARARRAY);
        ep1.add(ce);
        
        List<PhysicalPlan> eps = new ArrayList<PhysicalPlan>();
        eps.add(ep1);
        
        POLocalRearrange lr = new POLocalRearrange(new OperatorKey(scope,nig.getNextNodeId(scope)));
        try {
            lr.setIndex(0);
        } catch (ExecException e) {
        	int errCode = 2058;
        	String msg = "Unable to set index on newly created POLocalRearrange.";
            throw new PlanException(msg, errCode, PigException.BUG, e);
        }
        lr.setKeyType(DataType.CHARARRAY);
        lr.setPlans(eps);
        lr.setResultType(DataType.TUPLE);
        mro.mapPlan.add(lr);
        mro.mapPlan.connect(nfe1, lr);
        
        mro.setMapDone(true);
        
        POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
        pkg.setKeyType(DataType.CHARARRAY);
        pkg.setNumInps(1);
        boolean[] inner = {false}; 
        pkg.setInner(inner);
        mro.reducePlan.add(pkg);
        
        // Lets start building the plan which will have the sort
        // for the foreach
        PhysicalPlan fe2Plan = new PhysicalPlan();
        // Top level project which just projects the tuple which is coming 
        // from the foreach after the package
        POProject topPrj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
        topPrj.setColumn(1);
        topPrj.setResultType(DataType.TUPLE);
        topPrj.setOverloaded(true);
        fe2Plan.add(topPrj);
        
        // the projections which will form sort plans
        List<PhysicalPlan> nesSortPlanLst = new ArrayList<PhysicalPlan>();
        if (fields == null) {
            PhysicalPlan ep = new PhysicalPlan();
            POProject prj = new POProject(new OperatorKey(scope,
                nig.getNextNodeId(scope)));
            prj.setStar(true);
            prj.setOverloaded(false);
            prj.setResultType(DataType.TUPLE);
            ep.add(prj);
            nesSortPlanLst.add(ep);
        } else {
            for (int i=0; i<fields.length;i++) {
                PhysicalPlan ep = new PhysicalPlan();
                POProject prj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
                prj.setColumn(i);
                prj.setOverloaded(false);
                prj.setResultType(fields[i].second);
                ep.add(prj);
                nesSortPlanLst.add(ep);
            }
        }
        
        sort.setSortPlans(nesSortPlanLst);
        sort.setResultType(DataType.BAG);
        fe2Plan.add(sort);
        fe2Plan.connect(topPrj, sort);
        
        // The plan which will have a constant representing the
        // degree of parallelism for the final order by map-reduce job
        // this will either come from a "order by parallel x" in the script
        // or will be the default number of reducers for the cluster if
        // "parallel x" is not used in the script
        PhysicalPlan rpep = new PhysicalPlan();
        ConstantExpression rpce = new ConstantExpression(new OperatorKey(scope,nig.getNextNodeId(scope)));
        rpce.setRequestedParallelism(rp);
        int val = rp;
        if(val<=0){
            ExecutionEngine eng = pigContext.getExecutionEngine();
            if(eng instanceof HExecutionEngine){
                try {
                    val = Math.round(0.9f * ((HExecutionEngine)eng).getJobClient().getDefaultReduces());
                    if(val<=0)
                        val = 1;
                } catch (IOException e) {
                    int errCode = 6015;
                    String msg = "Problem getting the default number of reduces from the Job Client.";
                    throw new MRCompilerException(msg, errCode, PigException.REMOTE_ENVIRONMENT, e);
                }
            } else {
            	val = 1; // local mode, set it to 1
            }
        }
        int parallelismForSort = (rp <= 0 ? val : rp);
        rpce.setValue(parallelismForSort);
        
        rpce.setResultType(DataType.INTEGER);
        rpep.add(rpce);
        
        List<PhysicalPlan> genEps = new ArrayList<PhysicalPlan>();
        genEps.add(rpep);
        genEps.add(fe2Plan);
        
        List<Boolean> flattened2 = new ArrayList<Boolean>();
        flattened2.add(false);
        flattened2.add(false);
        
        POForEach nfe2 = new POForEach(new OperatorKey(scope,nig.getNextNodeId(scope)),-1, genEps, flattened2);
        mro.reducePlan.add(nfe2);
        mro.reducePlan.connect(pkg, nfe2);
        
        // Let's connect the output from the foreach containing
        // number of quantiles and the sorted bag of samples to
        // another foreach with the FindQuantiles udf. The input
        // to the FindQuantiles udf is a project(*) which takes the 
        // foreach input and gives it to the udf
        PhysicalPlan ep4 = new PhysicalPlan();
        POProject prjStar4 = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
        prjStar4.setResultType(DataType.TUPLE);
        prjStar4.setStar(true);
        ep4.add(prjStar4);
        
        List<PhysicalOperator> ufInps = new ArrayList<PhysicalOperator>();
        ufInps.add(prjStar4);
        // Turn the asc/desc array into an array of strings so that we can pass it
        // to the FindQuantiles function.
        List<Boolean> ascCols = inpSort.getMAscCols();
        String[] ascs = new String[ascCols.size()];
        for (int i = 0; i < ascCols.size(); i++) ascs[i] = ascCols.get(i).toString();
        // check if user defined comparator is used in the sort, if so
        // prepend the name of the comparator as the first fields in the
        // constructor args array to the FindQuantiles udf
        String[] ctorArgs = ascs;
        if(sort.isUDFComparatorUsed) {
            String userComparatorFuncSpec = sort.getMSortFunc().getFuncSpec().toString();
            ctorArgs = new String[ascs.length + 1];
            ctorArgs[0] = USER_COMPARATOR_MARKER + userComparatorFuncSpec;
            for(int j = 0; j < ascs.length; j++) {
                ctorArgs[j+1] = ascs[j];
            }
        }
        
        POUserFunc uf = new POUserFunc(new OperatorKey(scope,nig.getNextNodeId(scope)), -1, ufInps, 
            new FuncSpec(FindQuantiles.class.getName(), ctorArgs));
        ep4.add(uf);
        ep4.connect(prjStar4, uf);
        
        List<PhysicalPlan> ep4s = new ArrayList<PhysicalPlan>();
        ep4s.add(ep4);
        List<Boolean> flattened3 = new ArrayList<Boolean>();
        flattened3.add(false);
        POForEach nfe3 = new POForEach(new OperatorKey(scope,nig.getNextNodeId(scope)), -1, ep4s, flattened3);
        
        mro.reducePlan.add(nfe3);
        mro.reducePlan.connect(nfe2, nfe3);
        
        POStore str = getStore();
        str.setSFile(quantFile);
        mro.reducePlan.add(str);
        mro.reducePlan.connect(nfe3, str);
        
        mro.setReduceDone(true);
        mro.requestedParallelism = 1;
        return new Pair<MapReduceOper, Integer>(mro, parallelismForSort);
    }

    static class LastInputStreamingOptimizer extends MROpPlanVisitor {
        
        Log log = LogFactory.getLog(this.getClass());
        String chunkSize;
        LastInputStreamingOptimizer(MROperPlan plan, String chunkSize) {
            super(plan, new DepthFirstWalker<MapReduceOper, MROperPlan>(plan));
            this.chunkSize = chunkSize;
        }
        
        /**indTupIter
         * Look for pattern POPackage->POForEach(if both are flatten), change it to POJoinPackage
         * We can avoid materialize the input and construct the result of join on the fly
         * 
         * @param mr - map-reduce plan to optimize
         */ 
        @Override
        public void visitMROp(MapReduceOper mr) throws VisitorException {
            // Only optimize:
            // 1. POPackage->POForEach is the root of reduce plan
            // 2. POUnion is the leaf of map plan (so that we exclude distinct, sort...)
            // 3. No combiner plan
            // 4. POForEach nested plan only contains POProject in any depth
            // 5. Inside POForEach, all occurrences of the last input are flattened
            
            if (mr.mapPlan.isEmpty()) return;
            if (mr.reducePlan.isEmpty()) return;

            // Check combiner plan
            if (!mr.combinePlan.isEmpty()) {
                return;
            }
            
            // Check map plan
            List<PhysicalOperator> mpLeaves = mr.mapPlan.getLeaves();
            if (mpLeaves.size()!=1) {
                return;
            }
            PhysicalOperator op = mpLeaves.get(0);
            
            if (!(op instanceof POUnion)) {
                return;
            }
            
            // Check reduce plan
            List<PhysicalOperator> mrRoots = mr.reducePlan.getRoots();
            if (mrRoots.size()!=1) {
                return;
            }
            
            op = mrRoots.get(0);
            if (!(op instanceof POPackage)) {
                return;
            }
            POPackage pack = (POPackage)op;
            
            List<PhysicalOperator> sucs = mr.reducePlan.getSuccessors(pack);
            if (sucs.size()!=1) {
                return;
            }
            
            op = sucs.get(0);
            boolean lastInputFlattened = true;
            boolean allSimple = true;
            if (op instanceof POForEach)
            {
                POForEach forEach = (POForEach)op;
                List<PhysicalPlan> planList = forEach.getInputPlans();
                List<Boolean> flatten = forEach.getToBeFlattened();
                POProject projOfLastInput = null;
                int i = 0;
                // check all nested foreach plans
                // 1. If it is simple projection
                // 2. If last input is all flattened
                for (PhysicalPlan p:planList)
                {
                    PhysicalOperator opProj = p.getRoots().get(0);
                    if (!(opProj instanceof POProject))
                    {
                        allSimple = false;
                        break;
                    }
                    POProject proj = (POProject)opProj;
                    // the project should just be for one column
                    // from the input
                    if(proj.getColumns().size() != 1) {
                        allSimple = false;
                        break;
                    }
                    
                    try {
                        // if input to project is the last input
                        if (proj.getColumn() == pack.getNumInps())
                        {
                            // if we had already seen another project
                            // which was also for the last input, then
                            // we might be trying to flatten twice on the
                            // last input in which case we can't optimize by
                            // just streaming the tuple to those projects
                            // IMPORTANT NOTE: THIS WILL NEED TO CHANGE WHEN WE
                            // OPTIMIZE BUILTINS LIKE SUM() AND COUNT() TO
                            // TAKE IN STREAMING INPUT
                            if(projOfLastInput != null) {
                                allSimple = false;
                                break;
                            }
                            projOfLastInput = proj;
                            // make sure the project is on a bag which needs to be
                            // flattened
                            if (!flatten.get(i) || proj.getResultType() != DataType.BAG)
                            {
                                lastInputFlattened = false;
                                break;
                            }
                        }
                    } catch (ExecException e) {
                        int errCode = 2069;
                        String msg = "Error during map reduce compilation. Problem in accessing column from project operator.";
                        throw new MRCompilerException(msg, errCode, PigException.BUG, e);
                    }
                    
                    // if all deeper operators are all project
                    PhysicalOperator succ = p.getSuccessors(proj)!=null?p.getSuccessors(proj).get(0):null;
                    while (succ!=null)
                    {
                        if (!(succ instanceof POProject))
                        {
                            allSimple = false;
                            break;
                        }
                        // make sure successors of the last project also project bags
                        // we will be changing it to project tuples
                        if(proj == projOfLastInput && ((POProject)succ).getResultType() != DataType.BAG) {
                            allSimple = false;
                            break;
                        }
                        succ = p.getSuccessors(succ)!=null?p.getSuccessors(succ).get(0):null;
                    }
                    i++;
                    if (allSimple==false)
                        break;
                }
                
                if (lastInputFlattened && allSimple && projOfLastInput != null)
                {
                    // Now we can optimize the map-reduce plan
                    // Replace POPackage->POForeach to POJoinPackage
                    replaceWithPOJoinPackage(mr.reducePlan, pack, forEach, chunkSize);
                }
            }
        }

        public static void replaceWithPOJoinPackage(PhysicalPlan plan,
                POPackage pack, POForEach forEach, String chunkSize) throws VisitorException {
            String scope = pack.getOperatorKey().scope;
            NodeIdGenerator nig = NodeIdGenerator.getGenerator();
            POJoinPackage joinPackage;
            joinPackage = new POJoinPackage(
                        new OperatorKey(scope, nig.getNextNodeId(scope)), 
                        -1, pack, forEach);
            joinPackage.setChunkSize(Long.parseLong(chunkSize));
            List<PhysicalOperator> succs = plan.getSuccessors(forEach);
            if (succs!=null)
            {
                if (succs.size()!=1)
                {
                    int errCode = 2028;
                    String msg = "ForEach can only have one successor. Found " + succs.size() + " successors.";
                    throw new MRCompilerException(msg, errCode, PigException.BUG);
                }
            }
            plan.remove(pack);
            
            try {
                plan.replace(forEach, joinPackage);
            } catch (PlanException e) {
                int errCode = 2029;
                String msg = "Error rewriting POJoinPackage.";
                throw new MRCompilerException(msg, errCode, PigException.BUG, e);
            }
            
            LogFactory.
            getLog(LastInputStreamingOptimizer.class).info("Rewrite: POPackage->POForEach to POJoinPackage");
        }

    }
    
    
    private class RearrangeAdjuster extends MROpPlanVisitor {

        RearrangeAdjuster(MROperPlan plan) {
            super(plan, new DepthFirstWalker<MapReduceOper, MROperPlan>(plan));
        }

        @Override
        public void visitMROp(MapReduceOper mr) throws VisitorException {
            // Look for map reduce operators whose reduce starts in a local
            // rearrange.  If it has a successor and that predecessor's map
            // plan is just a load, push the porearrange to the successor.
            // Else, throw an error.
            if (mr.reducePlan.isEmpty()) return;
            List<PhysicalOperator> mpLeaves = mr.reducePlan.getLeaves();
            if (mpLeaves.size() != 1) {
                int errCode = 2024; 
                String msg = "Expected reduce to have single leaf. Found " + mpLeaves.size() + " leaves.";
                throw new MRCompilerException(msg, errCode, PigException.BUG);
            }
            PhysicalOperator mpLeaf = mpLeaves.get(0);
            if (!(mpLeaf instanceof POStore)) {
                int errCode = 2025;
                String msg = "Expected leaf of reduce plan to " +
                    "always be POStore. Found " + mpLeaf.getClass().getSimpleName();
                throw new MRCompilerException(msg, errCode, PigException.BUG);
            }
            List<PhysicalOperator> preds =
                mr.reducePlan.getPredecessors(mpLeaf);
            if (preds == null) return;
            if (preds.size() > 1) {
                int errCode = 2030;
                String msg ="Expected reduce plan leaf to have a single predecessor. Found " + preds.size() + " predecessors.";
                throw new MRCompilerException(msg, errCode, PigException.BUG);
            }
            PhysicalOperator pred = preds.get(0);
            if (!(pred instanceof POLocalRearrange)) return;

            // Next question, does the next MROper have an empty map?
            List<MapReduceOper> succs = mPlan.getSuccessors(mr);
            if (succs == null) {
                int errCode = 2031;
                String msg = "Found map reduce operator with POLocalRearrange as"
                    + " last oper but with no succesor.";
                throw new MRCompilerException(msg, errCode, PigException.BUG);
            }
            if (succs.size() > 1) {
                int errCode = 2032;
                String msg = "Expected map reduce operator to have a single successor. Found " + succs.size() + " successors.";
                throw new MRCompilerException(msg, errCode, PigException.BUG);
            }
            MapReduceOper succ = succs.get(0);
            List<PhysicalOperator> succMpLeaves = succ.mapPlan.getLeaves();
            List<PhysicalOperator> succMpRoots = succ.mapPlan.getRoots();
            if (succMpLeaves == null || succMpLeaves.size() > 1 ||
                    succMpRoots == null || succMpRoots.size() > 1 ||
                    succMpLeaves.get(0) != succMpRoots.get(0)) {
            		messageCollector.collect("Expected to find subsequent map " +
                    "with just a load, but didn't",
                    MessageType.Warning, PigWarning.DID_NOT_FIND_LOAD_ONLY_MAP_PLAN);
                return;
            }
            PhysicalOperator load = succMpRoots.get(0);

            try {
                mr.reducePlan.removeAndReconnect(pred);
                succ.mapPlan.add(pred);
                succ.mapPlan.connect(load, pred);
            } catch (PlanException pe) {
                int errCode = 2033;
                String msg = "Problems in rearranging map reduce operators in plan.";
                throw new MRCompilerException(msg, errCode, PigException.BUG, pe);
            }
        }
    }

    private class LimitAdjuster extends MROpPlanVisitor {
        ArrayList<MapReduceOper> opsToAdjust = new ArrayList<MapReduceOper>();  

        LimitAdjuster(MROperPlan plan) {
            super(plan, new DepthFirstWalker<MapReduceOper, MROperPlan>(plan));
        }

        @Override
        public void visitMROp(MapReduceOper mr) throws VisitorException {
            // Look for map reduce operators which contains limit operator.
            // If so and the requestedParallelism > 1, add one additional map-reduce
            // operator with 1 reducer into the original plan
            if (mr.limit!=-1 && mr.requestedParallelism!=1)
            {
                opsToAdjust.add(mr);
            }
        }
        
        public void adjust() throws IOException, PlanException
        {
            for (MapReduceOper mr:opsToAdjust)
            {
                if (mr.reducePlan.isEmpty()) return;
                List<PhysicalOperator> mpLeaves = mr.reducePlan.getLeaves();
                if (mpLeaves.size() != 1) {
                    int errCode = 2024; 
                    String msg = "Expected reduce to have single leaf. Found " + mpLeaves.size() + " leaves.";
                    throw new MRCompilerException(msg, errCode, PigException.BUG);
                }
                PhysicalOperator mpLeaf = mpLeaves.get(0);
                if (!(mpLeaf instanceof POStore)) {
                    int errCode = 2025;
                    String msg = "Expected leaf of reduce plan to " +
                        "always be POStore. Found " + mpLeaf.getClass().getSimpleName();
                    throw new MRCompilerException(msg, errCode, PigException.BUG);
                }
                FileSpec oldSpec = ((POStore)mpLeaf).getSFile();
                
                FileSpec fSpec = getTempFileSpec();
                ((POStore)mpLeaf).setSFile(fSpec);
                mr.setReduceDone(true);
                MapReduceOper limitAdjustMROp = getMROp();
                POLoad ld = getLoad();
                ld.setLFile(fSpec);
                limitAdjustMROp.mapPlan.add(ld);
                POLimit pLimit = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
                pLimit.setLimit(mr.limit);
                limitAdjustMROp.mapPlan.addAsLeaf(pLimit);
                simpleConnectMapToReduce(limitAdjustMROp);
                POLimit pLimit2 = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
                pLimit2.setLimit(mr.limit);
                limitAdjustMROp.reducePlan.addAsLeaf(pLimit2);
                POStore st = getStore();
                st.setSFile(oldSpec);
                limitAdjustMROp.reducePlan.addAsLeaf(st);
                limitAdjustMROp.requestedParallelism = 1;
                // If the operator we're following has global sort set, we
                // need to indicate that this is a limit after a sort.
                // This will assure that we get the right sort comparator
                // set.  Otherwise our order gets wacked (PIG-461).
                if (mr.isGlobalSort()) limitAdjustMROp.setLimitAfterSort(true);
                
                List<MapReduceOper> successorList = MRPlan.getSuccessors(mr);
                MapReduceOper successors[] = null;
                
                // Save a snapshot for successors, since we will modify MRPlan, 
                // use the list directly will be problematic
                if (successorList!=null && successorList.size()>0)
                {
                    successors = new MapReduceOper[successorList.size()];
                    int i=0;
                    for (MapReduceOper op:successorList)
                        successors[i++] = op;
                }
                
                MRPlan.add(limitAdjustMROp);
                MRPlan.connect(mr, limitAdjustMROp);
                
                if (successors!=null)
                {
                    for (int i=0;i<successors.length;i++)
                    {
                        MapReduceOper nextMr = successors[i];
                        if (nextMr!=null)
                            MRPlan.disconnect(mr, nextMr);
                        
                        if (nextMr!=null)
                            MRPlan.connect(limitAdjustMROp, nextMr);                        
                    }
                }
            }
        }
    }

    private class FindKeyTypeVisitor extends PhyPlanVisitor {

        byte keyType = DataType.UNKNOWN;

        FindKeyTypeVisitor(PhysicalPlan plan) {
            super(plan,
                new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(plan));
        }

        @Override
        public void visitProject(POProject p) throws VisitorException {
            keyType = p.getResultType();
        }
    }

}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOAnd extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOAnd.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOAnd(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "And " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.io.PrintStream;
import java.util.List;
import java.util.LinkedList;
import java.util.Collection;
import org.apache.pig.impl.util.MultiMap;
import java.util.HashSet;
import java.util.Set;

import org.apache.pig.impl.plan.DotPlanDumper;
import org.apache.pig.impl.plan.Operator;

/**
 * This class can print a logical plan in the DOT format. It uses
 * clusters to illustrate nesting. If "verbose" is off, it will skip
 * any nesting.
 */
public class DotLOPrinter extends DotPlanDumper<LogicalOperator, LogicalPlan,
                                  LogicalOperator, LogicalPlan> {

    public DotLOPrinter(LogicalPlan plan, PrintStream ps) {
        this(plan, ps, false, new HashSet<Operator>(), new HashSet<Operator>(),
             new HashSet<Operator>());
    }

    private DotLOPrinter(LogicalPlan plan, PrintStream ps, boolean isSubGraph,
                         Set<Operator> subgraphs, 
                         Set<Operator> multiInSubgraphs,
                         Set<Operator> multiOutSubgraphs) {
        super(plan, ps, isSubGraph, subgraphs, 
              multiInSubgraphs, multiOutSubgraphs);
    }

    @Override
    protected DotPlanDumper makeDumper(LogicalPlan plan, PrintStream ps) {
        return new DotLOPrinter(plan, ps, true, mSubgraphs, 
                                mMultiInputSubgraphs,
                                mMultiOutputSubgraphs);
    }

    @Override
    protected String getName(LogicalOperator op) {
        String info = (op.name().split("\\d+-\\d+"))[0];
        if (op instanceof LOProject) {
            LOProject pr = (LOProject)op;
            info += pr.isStar()?" [*]": pr.getProjection();
        }
        return info;
    }

    @Override
    protected String[] getAttributes(LogicalOperator op) {
        if (op instanceof LOStore || op instanceof LOLoad) {
            String[] attributes = new String[3];
            attributes[0] = "label=\""+getName(op).replace(":",",\\n")+"\"";
            attributes[1] = "style=\"filled\"";
            attributes[2] = "fillcolor=\"gray\"";
            return attributes;
        }
        else {
            return super.getAttributes(op);
        }
    }

    @Override
    protected MultiMap<LogicalOperator, LogicalPlan> 
        getMultiInputNestedPlans(LogicalOperator op) {
        
        if(op instanceof LOCogroup){
            return  ((LOCogroup)op).getGroupByPlans();
        }
        else if(op instanceof LOFRJoin){
            return ((LOFRJoin)op).getJoinColPlans();
        }
        return new MultiMap<LogicalOperator, LogicalPlan>();
    }

    @Override
    protected Collection<LogicalPlan> getNestedPlans(LogicalOperator op) {
        Collection<LogicalPlan> plans = new LinkedList<LogicalPlan>();

        if(op instanceof LOFilter){
            plans.add(((LOFilter)op).getComparisonPlan());
        }
        else if(op instanceof LOForEach){
            plans.addAll(((LOForEach)op).getForEachPlans());
        }
        else if(op instanceof LOGenerate){
            plans.addAll(((LOGenerate)op).getGeneratePlans());
        }
        else if(op instanceof LOSort){
            plans.addAll(((LOSort)op).getSortColPlans()); 
        }
        else if(op instanceof LOSplitOutput){
            plans.add(((LOSplitOutput)op).getConditionPlan());
        }
        
        return plans;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import java.io.IOException;
import java.io.InputStream;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Properties;
import java.net.URL;
import java.util.List;
import java.util.ArrayList;
import java.util.Set;

import junit.framework.AssertionFailedError;

import org.junit.Test;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.LoadFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigServer;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.PigContext;
import org.apache.pig.ExecType;
import org.apache.pig.impl.builtin.GFAny;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.logicalLayer.*;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.parser.ParseException ;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.test.utils.Identity;
import org.apache.pig.impl.util.LogUtils;
import org.apache.pig.PigException;


public class TestLogicalPlanBuilder extends junit.framework.TestCase {

    private final Log log = LogFactory.getLog(getClass());
    
    @Test
    public void testQuery1() {
        String query = "foreach (load 'a') generate $1,$2;";
        buildPlan(query);
    }

    @Test
    public void testQuery2() {
        String query = "foreach (load 'a' using " + PigStorage.class.getName() + "(':')) generate $1, 'aoeuaoeu' ;";
        buildPlan(query);
    }

    // TODO FIX Query3 and Query4
    @Test
    public void testQuery3() {
        String query = "foreach (cogroup (load 'a') by $1, (load 'b') by $1) generate org.apache.pig.builtin.AVG($1) ;";
        buildPlan(query);
    }

    @Test
    public void testQuery4() {
        String query = "foreach (load 'a') generate AVG($1, $2) ;";
        buildPlan(query);
    }

    @Test
    public void testQuery5() {
        String query = "foreach (group (load 'a') ALL) generate $1 ;";
        buildPlan(query);
    }

    
    @Test
    public void testQuery6() {
        String query = "foreach (group (load 'a') by $1) generate group, '1' ;";
        buildPlan(query);
    }

    
    @Test
    public void testQuery7() {
        String query = "foreach (load 'a' using " + PigStorage.class.getName() + "()) generate $1 ;";
        buildPlan(query);
    }

    
    @Test
    public void testQuery10() {
        String query = "foreach (cogroup (load 'a') by ($1), (load 'b') by ($1)) generate $1.$1, $2.$1 ;";
        buildPlan(query);
    }

    // TODO FIX Query11 and Query12
    @Test
    public void testQuery11() {
        String query = " foreach (group (load 'a') by $1, (load 'b') by $2) generate group, AVG($1) ;";
        buildPlan(query);
    }
    
    @Test
    public void testQuery12() {
        String query = "foreach (load 'a' using " + PigStorage.class.getName() + "()) generate AVG($1) ;";
        buildPlan(query);
    }

    @Test
    public void testQuery13() {
        String query = "foreach (cogroup (load 'a') ALL) generate group ;";
        buildPlan(query);
    }

    @Test
    public void testQuery14() {
        String query = "foreach (group (load 'a') by ($6, $7)) generate flatten(group) ;";
        buildPlan(query);
    }

    @Test
    public void testQuery15() {
        String query = " foreach (load 'a') generate $1, 'hello', $3 ;";
        buildPlan(query);
    }
    
    @Test
    public void testQuery100() {
        // test define syntax
        String query = "define FUNC ARITY();";
        LogicalOperator lo = buildPlan(query).getRoots().get(0);
        assertTrue(lo instanceof LODefine);
    }



    @Test
    public void testQueryFail1() {
        String query = " foreach (group (A = load 'a') by $1) generate A.'1' ;";
        try {
            buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    @Test
    public void testQueryFail2() {
        String query = "foreach group (load 'a') by $1 generate $1.* ;";
        try {
            buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    

    @Test
    public void testQueryFail3() {
        String query = "generate DISTINCT foreach (load 'a');";
        try {
            buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQueryFail4() {
        String query = "generate [ORDER BY $0][$3, $4] foreach (load 'a');";
        try {
            buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    @Test
    public void testQueryFail5() {
        String query = "generate " + TestApplyFunc.class.getName() + "($2.*) foreach (load 'a');";
        try {
            buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    
    
    /**
     * User generate functions must be in default package Bug 831620 - fixed
     */
 
    // TODO FIX Query17
    @Test
    public void testQuery17() {
        String query =  "foreach (load 'A')" + "generate " + TestApplyFunc.class.getName() + "($1);";
        buildPlan(query);
    }


    static public class TestApplyFunc extends org.apache.pig.EvalFunc<Tuple> {
        @Override
        public Tuple exec(Tuple input) throws IOException {
            Tuple output = TupleFactory.getInstance().newTuple(input.getAll());
            return output;
        }
    }
    
    
    /**
     * Validate that parallel is parsed correctly Bug 831714 - fixed
     */
    
    @Test
    public void testQuery18() {
        String query = "FOREACH (group (load 'a') ALL PARALLEL 16) generate group;";
        LogicalPlan lp = buildPlan(query);
        LogicalOperator root = lp.getRoots().get(0);   
        
        List<LogicalOperator> listOp = lp.getSuccessors(root);
        
        LogicalOperator lo = listOp.get(0);
        
        if (lo instanceof LOCogroup) {
            assertTrue(((LOCogroup) lo).getRequestedParallelism() == 16);
        } else {
            fail("Error: Unexpected Parse Tree output");
        }  
    }
    
    
    
    
    @Test
    public void testQuery19() {
        buildPlan("a = load 'a';");
        buildPlan("b = filter a by $1 == '3';");
    }
    
    
    @Test
    public void testQuery20() {
        String query = "foreach (load 'a') generate ($1 == '3'? $2 : $3) ;";
        buildPlan(query);
    }
    
    @Test
    public void testQuery21() {
        buildPlan("A = load 'a';");
        buildPlan("B = load 'b';");
        buildPlan("foreach (cogroup A by ($1), B by ($1)) generate A, flatten(B.($1, $2, $3));");
    }
    
    @Test
    public void testQuery22() {
        buildPlan("A = load 'a';");
        buildPlan("B = load 'b';");
        buildPlan("C = cogroup A by ($1), B by ($1);");
        String query = "foreach C { " +
                "B = order B by $0; " +
                "generate FLATTEN(A), B.($1, $2, $3) ;" +
                "};" ;
        buildPlan(query);
    }
    
    @Test
    public void testQuery22Fail() {
        buildPlan("A = load 'a';");
        try {
            buildPlan("B = group A by (*, $0);");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Grouping attributes can either be star (*"));
        }
    }
    
    @Test
    public void testQuery23() {
        buildPlan("A = load 'a';");
        buildPlan("B = load 'b';");
        
        buildPlan("C = cogroup A by ($1), B by ($1);");
        
        String query = "foreach C { " +
        "A = Distinct A; " +
        "B = FILTER A BY $1 < 'z'; " +
        //TODO
        //A sequence of filters within a foreach translates to
        //a split statement. Currently it breaks as adding an
        //additional output to the filter fails as filter supports
        //single output
        "C = FILTER A BY $2 == $3;" +
        "B = ARRANGE B BY $1;" +
        "GENERATE A, FLATTEN(B.$0);" +
        "};";
        buildPlan(query);
    }

    @Test
    public void testQuery23Fail() {
        buildPlan("A = load 'a';");
        buildPlan("B = load 'b';");
        try {
            buildPlan("C = group A by (*, $0), B by ($0, $1);");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Grouping attributes can either be star (*"));
        }
    }

    @Test
    public void testQuery24() {
        buildPlan("a = load 'a';");
        
        String query = "foreach a generate (($0 == $1) ? 'a' : $2), $4 ;";
        buildPlan(query);
    }

    @Test
    public void testQuery25() {
        String query = "foreach (load 'a') {" +
                "B = FILTER $0 BY (($1 == $2) AND ('a' < 'b'));" +
                "generate B;" +
                "};";
        buildPlan(query);
    }
    
    
    @Test
    public void testQuery26() {
        String query = "foreach (load 'a') generate  ((NOT (($1 == $2) OR ('a' < 'b'))) ? 'a' : $2), 'x' ;";
        buildPlan(query);
    }
    
    // TODO FIX Query27 and Query28
    @Test
    public void testQuery27() {
        String query =  "foreach (load 'a'){" +
                "A = DISTINCT $3.$1;" +
                " generate " + TestApplyFunc.class.getName() + "($2, $1.($1, $4));" +
                        "};";
        buildPlan(query);
    }
    
    @Test
    public void testQuery28() {
        String query = "foreach (load 'a') generate " + TestApplyFunc.class.getName() + "($2, " + TestApplyFunc.class.getName() + "($2.$3));";
        buildPlan(query);
    }
    
    @Test
    public void testQuery29() {
        String query = "load 'myfile' using " + TestStorageFunc.class.getName() + "() as (col1);";
        buildPlan(query);
    }


    @Test
    public void testQuery30() {
        String query = "load 'myfile' using " + TestStorageFunc.class.getName() + "() as (col1, col2);";
        buildPlan(query);
    }
    
    
    public static class TestStorageFunc implements LoadFunc{
        public void bindTo(String fileName, BufferedPositionedInputStream is, long offset, long end) throws IOException {
            
        }
        
        public Tuple getNext() throws IOException {
            return null;
        }
        
        public void fieldsToRead(Schema schema) {
            
        }
        
        public DataBag bytesToBag(byte[] b) throws IOException {
            return null;
        }

        public Boolean bytesToBoolean(byte[] b) throws IOException {
            return null;
        }
        
        public String bytesToCharArray(byte[] b) throws IOException {
            return null;
        }
        
        public Double bytesToDouble(byte[] b) throws IOException {
            return null;
        }
        
        public Float bytesToFloat(byte[] b) throws IOException {
            return null;
        }
        
        public Integer bytesToInteger(byte[] b) throws IOException {
            return null;
        }

        public Long bytesToLong(byte[] b) throws IOException {
            return null;
        }

        public Map<Object, Object> bytesToMap(byte[] b) throws IOException {
            return null;
        }

        public Tuple bytesToTuple(byte[] b) throws IOException {
            return null;
        }        

	    public byte[] toBytes(DataBag bag) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(String s) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(Double d) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(Float f) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(Integer i) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(Long l) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(Map<Object, Object> m) throws IOException {
            return null;
	    }
	
	    public byte[] toBytes(Tuple t) throws IOException {
            return null;
	    }

        /* (non-Javadoc)
         * @see org.apache.pig.LoadFunc#determineSchema(java.lang.String, org.apache.pig.ExecType, org.apache.pig.backend.datastorage.DataStorage)
         */
        public Schema determineSchema(String fileName, ExecType execType,
                DataStorage storage) throws IOException {
            // TODO Auto-generated method stub
            return null;
        }
    }
    
    
    @Test
    public void testQuery31() {
        String query = "load 'myfile' as (col1, col2);";
        buildPlan(query);
    }
    
    @Test
    public void testQuery32() {
        String query = "foreach (load 'myfile' as (col1, col2 : tuple(sub1, sub2), col3 : tuple(bag1))) generate col1 ;";
        buildPlan(query);
    }
    
    @Test
    public void testQuery33() {
        buildPlan("A = load 'a' as (aCol1, aCol2);");
        buildPlan("B = load 'b' as (bCol1, bCol2);");
        buildPlan("C = cogroup A by (aCol1), B by bCol1;");
        String query = "foreach C generate group, A.aCol1;";
        buildPlan(query);
    }
    
    
    @Test
    //TODO: Nested schemas don't work now. Probably a bug in the new parser.
    public void testQuery34() {
        buildPlan("A = load 'a' as (aCol1, aCol2 : tuple(subCol1, subCol2));");
        buildPlan("A = filter A by aCol2 == '1';");
        buildPlan("B = load 'b' as (bCol1, bCol2);");
        String query = "foreach (cogroup A by (aCol1), B by bCol1 ) generate A.aCol2, B.bCol2 ;";
        buildPlan(query);
    }
    
    
    
    @Test
    public void testQuery35() {
        String query = "foreach (load 'a' as (col1, col2)) generate col1, col2 ;";
        buildPlan(query);
    }
    
    @Test
    public void testQuery36() {
        String query = "foreach (cogroup ( load 'a' as (col1, col2)) by col1) generate $1.(col2, col1);";
        buildPlan(query);
    }
    
    @Test
    public void testQueryFail37() {
        String query = "A = load 'a'; asdasdas";
        try{
            buildPlan(query);
        }catch(AssertionFailedError e){
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQuery38(){
        String query = "c = cross (load 'a'), (load 'b');";
        buildPlan(query);
    }
    
    
    // TODO FIX Query39 and Query40
    @Test
    public void testQuery39(){
        buildPlan("a = load 'a' as (url, host, rank);");
        buildPlan("b = group a by (url,host); ");
        LogicalPlan lp = buildPlan("c = foreach b generate flatten(group.url), SUM(a.rank) as totalRank;");
        buildPlan("d = filter c by totalRank > '10';");
        buildPlan("e = foreach d generate totalRank;");
    }
    
    @Test
    public void testQueryFail39(){
        buildPlan("a = load 'a' as (url, host, rank);");
        buildPlan("b = group a by (url,host); ");
        LogicalPlan lp = buildPlan("c = foreach b generate flatten(group.url), SUM(a.rank) as totalRank;");
        buildPlan("d = filter c by totalRank > '10';");
        try {
            buildPlan("e = foreach d generate url;");//url has been falttened and hence the failure
        } catch(AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQuery40() {
        buildPlan("a = FILTER (load 'a') BY IsEmpty($2);");
        buildPlan("a = FILTER (load 'a') BY (IsEmpty($2) AND ($3 == $2));");
    }
    
    @Test
    public void testQueryFail41() {
        buildPlan("a = load 'a';");
        try {
            buildPlan("b = a as (host,url);");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Currently PIG does not support assigning an existing relation"));
        }
        // TODO
        // the following statement was earlier present
        // eventually when we do allow assignments of the form
        // above, we should test with the line below
        // uncommented
        //buildPlan("foreach b generate host;");
    }
    
    @Test
    public void testQuery42() {
        buildPlan("a = load 'a';");
        buildPlan("b = foreach a generate $0 as url, $1 as rank;");
        buildPlan("foreach b generate url;");
    }

    @Test
    public void testQuery43() {
        buildPlan("a = load 'a' as (url,hitCount);");
        buildPlan("b = load 'a' as (url,rank);");
        buildPlan("c = cogroup a by url, b by url;");
        buildPlan("d = foreach c generate group,flatten(a),flatten(b);");
        buildPlan("e = foreach d generate group, a::url, b::url, b::rank, rank;");
    }

    @Test
    public void testQueryFail43() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
        try {
            String query = "c = cogroup a by (name, age), b by (height);";
            buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    } 


    @Test
    public void testQuery44() {
        buildPlan("a = load 'a' as (url, pagerank);");
        buildPlan("b = load 'b' as (url, query, rank);");
        buildPlan("c = cogroup a by (pagerank#'nonspam', url) , b by (rank/'2', url) ;");
        buildPlan("foreach c generate group.url;");
    }

//TODO
//Commented out testQueryFail44 as I am not able to include org.apache.pig.PigServer;
    @Test
    public void testQueryFail44() throws Throwable {
        PigServer pig = null;
        try {
            pig = new PigServer("local");
        } catch (IOException e) {
            assertTrue(false);  // pig server failed for some reason
        }
        pig.registerFunction("myTr",
            new FuncSpec(GFAny.class.getName() + "('tr o 0')"));
        try{
            pig.registerQuery("b = foreach (load 'a') generate myTr(myTr(*));");
        }catch(Exception e){
            return;
        }
        assertTrue(false);
    }
    
    /*
    // Select
    public void testQuery45() {
        buildPlan("A = load 'a' as (url,hitCount);");
        buildPlan("B = select url, hitCount from A;");
        buildPlan("C = select url, hitCount from B;");
    }

    //Select + Join
    public void testQuery46() {
        buildPlan("A = load 'a' as (url,hitCount);");
        buildPlan("B = load 'b' as (url,pageRank);");
        buildPlan("C = select A.url, A.hitCount, B.pageRank from A join B on A.url == B.url;");        
    }

    // Mutliple Joins
    public void testQuery47() {
        buildPlan("A = load 'a' as (url,hitCount);");
        buildPlan("B = load 'b' as (url,pageRank);");
        buildPlan("C = load 'c' as (pageRank, position);");
        buildPlan("B = select A.url, A.hitCount, B.pageRank from (A join B on A.url == B.url) join C on B.pageRank == C.pageRank;");
    }

    // Group
    public void testQuery48() {
        buildPlan("A = load 'a' as (url,hitCount);");        
        buildPlan("C = select A.url, AVG(A.hitCount) from A group by url;");
    }

    // Join + Group
    public void testQuery49() {
        buildPlan("A = load 'a' as (url,hitCount);");
        buildPlan("B = load 'b' as (url,pageRank);");
        buildPlan("C = select A.url, AVG(B.pageRank), SUM(A.hitCount) from A join B on A.url == B.url group by A.url;");
    }

    // Group + Having
    public void testQuery50() {
        buildPlan("A = load 'a' as (url,hitCount);");        
        buildPlan("C = select A.url, AVG(A.hitCount) from A group by url having AVG(A.hitCount) > '6';");
    }

 // Group + Having + Order
    public void testQuery51() {
        buildPlan("A = load 'a' as (url,hitCount);");        
        buildPlan("C = select A.url, AVG(A.hitCount) from A group by url order by A.url;");
    }
    
    // Group + Having + Order
    public void testQuery52() {
        buildPlan("A = load 'a' as (url,hitCount);");        
        buildPlan("C = select A.url, AVG(A.hitCount) from A group by url having AVG(A.hitCount) > '6' order by A.url;");
    }

    // Group + Having + Order 2
    public void testQuery53() {
        buildPlan("A = load 'a' as (url,hitCount);");
        buildPlan("C = select A.url, AVG(A.hitCount) from A group by url having AVG(A.hitCount) > '6' order by AVG(A.hitCount);");
    }

    // Group + Having + Order 2
    public void testQuery54() {
        buildPlan("A = load 'a' as (url,hitCount, size);");
        buildPlan("C = select A.url, AVG(A.hitCount) from A group by url having AVG(A.size) > '6' order by AVG(A.hitCount);");
    }

    // Group + Having + Order 2
    public void testQuery55() {
        buildPlan("A = load 'a' as (url,hitCount, size);");
        buildPlan("C = select A.url, AVG(A.hitCount), SUM(A.size) from A group by url having AVG(A.size) > '6' order by AVG(A.hitCount);");
    }

    // Group + Having + Order 2
    public void testQuery56() {
        buildPlan("A = load 'a' as (url,hitCount, date);");
        buildPlan("C = select A.url, A.date, SUM(A.hitCount) from A group by url, date having AVG(A.hitCount) > '6' order by A.date;");
    }
    */

    @Test
    public void testQuery57() {
        String query = "foreach (load 'a') generate ($1+$2), ($1-$2), ($1*$2), ($1/$2), ($1%$2), -($1) ;";
        buildPlan(query);
    }

    
    @Test
    public void testQuery58() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = group a by name;");
        String query = "foreach b {d = a.name; generate group, d;};";
        buildPlan(query);
    } 

	@Test
    public void testQueryFail58(){
        buildPlan("a = load 'a' as (url, host, rank);");
        buildPlan("b = group a by url; ");
        try {
        	LogicalPlan lp = buildPlan("c = foreach b generate group.url;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    @Test
    public void testQuery59() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
        String query = "c = join a by name, b by name;";
        buildPlan(query);
    } 
    
    @Test
    public void testQuery60() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
        String query = "c = cross a,b;";
        buildPlan(query);
    } 

    @Test
    public void testQuery61() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
        String query = "c = union a,b;";
        buildPlan(query);
    }

    @Test
    public void testQuery62() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
        String query = "c = cross a,b;";
        buildPlan(query);
        buildPlan("d = order c by b::name, height, a::gpa;");
        buildPlan("e = order a by name, age, gpa desc;");
        buildPlan("f = order a by $0 asc, age, gpa desc;");
        buildPlan("g = order a by * asc;");
        buildPlan("h = cogroup a by name, b by name;");
        buildPlan("i = foreach h {i1 = order a by *; generate i1;};");
    }

    @Test
    public void testQueryFail62() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
        String query = "c = cross a,b;";
        buildPlan(query);
        try {
        	buildPlan("d = order c by name, b::name, height, a::gpa;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    @Test
    public void testQuery63() {
        buildPlan("a = load 'a' as (name, details: tuple(age, gpa));");
        buildPlan("b = group a by details;");
        String query = "d = foreach b generate group.age;";
        buildPlan(query);
        buildPlan("e = foreach a generate name, details;");
    }

    @Test
    public void testQueryFail63() {
        String query = "foreach (load 'myfile' as (col1, col2 : (sub1, sub2), col3 : (bag1))) generate col1 ;";
        try {
        	buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQuery64() {
        buildPlan("a = load 'a' as (name: chararray, details: tuple(age, gpa), mymap: map[]);");
        buildPlan("c = load 'a' as (name, details: bag{mytuple: tuple(age: int, gpa)});");
        buildPlan("b = group a by details;");
        String query = "d = foreach b generate group.age;";
        buildPlan(query);
		buildPlan("e = foreach a generate name, details;");
		buildPlan("f = LOAD 'myfile' AS (garage: bag{tuple1: tuple(num_tools: int)}, links: bag{tuple2: tuple(websites: chararray)}, page: bag{something_stupid: tuple(yeah_double: double)}, coordinates: bag{another_tuple: tuple(ok_float: float, bite_the_array: bytearray, bag_of_unknown: bag{})});");
    }

    @Test
    public void testQueryFail64() {
        String query = "foreach (load 'myfile' as (col1, col2 : bag{age: int})) generate col1 ;";
        try {
        	buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQuery65() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
		buildPlan("c = cogroup a by (name, age), b by (name, height);");
		buildPlan("d = foreach c generate group.name, a.name as aName, b.name as b::name;");
	}

    @Test
    public void testQueryFail65() {
        buildPlan("a = load 'a' as (name, age, gpa);");
        buildPlan("b = load 'b' as (name, height);");
		buildPlan("c = cogroup a by (name, age), b by (name, height);");
        try {
			buildPlan("d = foreach c generate group.name, a.name, b.height as age, a.age;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
	}

    @Test
    public void testQuery67() {
        buildPlan(" a = load 'input1' as (name, age, gpa);");
        buildPlan(" b = foreach a generate age, age * 10L, gpa/0.2f, {(16, 4.0e-2, 'hello')};");
    }

    @Test
    public void testQuery68() {
        buildPlan(" a = load 'input1';");
        buildPlan(" b = foreach a generate 10, {(16, 4.0e-2, 'hello'), (0.5f, 12l, 'another tuple')};");
    }

    @Test
    public void testQuery69() {
        buildPlan(" a = load 'input1';");
        buildPlan(" b = foreach a generate {(16, 4.0e-2, 'hello'), (0.5f, 'another tuple', 12L, (1))};");
    }

    @Test
    public void testQuery70() {
        buildPlan(" a = load 'input1';");
        buildPlan(" b = foreach a generate [10L#'hello', 4.0e-2#10L, 0.5f#(1), 'world'#42, 42#{('guide')}] as mymap:map[];");
        buildPlan(" c = foreach b generate mymap#10L;");
    }

    @Test
    public void testQueryFail67() {
        buildPlan(" a = load 'input1' as (name, age, gpa);");
        try {
            buildPlan(" b = foreach a generate age, age * 10L, gpa/0.2f, {16, 4.0e-2, 'hello'};");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQueryFail68() {
        buildPlan(" a = load 'input1' as (name, age, gpa);");
        try {
            buildPlan(" b = foreach a generate {(16 L, 4.0e-2, 'hello'), (0.5f, 'another tuple', 12L, {()})};");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQuery71() {
        buildPlan("split (load 'a') into x if $0 > '7', y if $0 < '7';");
        buildPlan("b = foreach x generate $0;");
        buildPlan("c = foreach y generate $1;");
    }

    @Test
    public void testQuery72() {
        buildPlan("split (load 'a') into x if $0 > '7', y if $0 < '7';");
        buildPlan("b = foreach x generate (int)$0;");
        buildPlan("c = foreach y generate (bag{})$1;");
        buildPlan("d = foreach y generate (int)($1/2);");
        buildPlan("e = foreach y generate (bag{tuple(int, float)})($1/2);");
        buildPlan("f = foreach x generate (tuple(int, float))($1/2);");
        buildPlan("g = foreach x generate (tuple())($1/2);");
        buildPlan("h = foreach x generate (chararray)($1/2);");
    }

    @Test
    public void testQueryFail72() {
        buildPlan("split (load 'a') into x if $0 > '7', y if $0 < '7';");
        try {
            buildPlan("c = foreach y generate (bag)$1;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
        try {
            buildPlan("c = foreach y generate (bag{int, float})$1;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
        try {
            buildPlan("c = foreach y generate (tuple)$1;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    @Test
    public void testQuery73() {
        buildPlan("split (load 'a') into x if $0 > '7', y if $0 < '7';");
        buildPlan("b = filter x by $0 matches '^fred.*';");
        buildPlan("c = foreach y generate $0, ($0 matches 'yuri.*' ? $1 - 10 : $1);");
    }

    @Test
    public void testQuery74() {
        buildPlan("a = load 'a' as (field1: int, field2: long);");
        buildPlan("b = load 'a' as (field1: bytearray, field2: double);");
        buildPlan("c = group a by field1, b by field1;");
        buildPlan("d = cogroup a by ((field1+field2)*field1), b by field1;");
    }

    @Test
    public void testQuery77() {
        buildPlan("limit (load 'a') 100;");
    }
    
    @Test
    public void testQuery75() {
        buildPlan("a = union (load 'a'), (load 'b'), (load 'c');");
        buildPlan("b = foreach a {generate $0;} parallel 10;");
    }
    
    @Test
    public void testQuery76() {
        buildPlan("split (load 'a') into x if $0 > '7', y if $0 < '7';");
        buildPlan("b = filter x by $0 IS NULL;");
        buildPlan("c = filter y by $0 IS NOT NULL;");
        buildPlan("d = foreach b generate $0, ($1 IS NULL ? 0 : $1 - 7);");
        buildPlan("e = foreach c generate $0, ($1 IS NOT NULL ? $1 - 5 : 0);");
    }

    @Test 
    public void testQuery80() {
        buildPlan("a = load 'input1' as (name, age, gpa);");
        buildPlan("b = filter a by age < '20';");
        buildPlan("c = group b by age;");
        String query = "d = foreach c {" 
            + "cf = filter b by gpa < '3.0';"
            + "cp = cf.gpa;"
            + "cd = distinct cp;"
            + "co = order cd by gpa;"
            + "generate group, flatten(co);"
            //+ "generate group, flatten(cd);"
            + "};";
        buildPlan(query);
    }

    @Test
    public void testQuery81() {
        buildPlan("a = load 'input1' using PigStorage() as (name, age, gpa);");
        buildPlan("split a into b if name lt 'f', c if (name gte 'f' and name lte 'h'), d if name gt 'h';");
    }

    @Test
    public void testQueryFail81() {
        buildPlan("a = load 'input1' using PigStorage() as (name, age, gpa);");
        try {
            buildPlan("split a into b if name lt 'f', c if (name ge 'f' and name le 'h'), d if name gt 'h';");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }
    
    @Test
    public void testQuery82() {
        buildPlan("a = load 'myfile';");
        buildPlan("b = group a by $0;"); 
        String query = "c = foreach b {"
            + "c1 = order $1 by *;" 
            + "c2 = $1.$0;" 
            + "generate flatten(c1), c2;"
            + "};";
        buildPlan(query);
    }

    @Test
    public void testQueryFail82() {
        buildPlan("a = load 'myfile';");
        buildPlan("b = group a by $0;"); 
        String query = "c = foreach b {"
            + "c1 = order $1 by *;" 
            + "c2 = $1;" 
            + "generate flatten(c1), c2;"
            + "};";
        try {
        buildPlan(query);
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Exception"));
        }
    }

    @Test
    public void testQuery83() {
        buildPlan("a = load 'input1' as (name, age, gpa);");
        buildPlan("b = filter a by age < '20';");
        buildPlan("c = group b by (name,age);");
        String query = "d = foreach c {" 
            + "cf = filter b by gpa < '3.0';"
            + "cp = cf.gpa;"
            + "cd = distinct cp;"
            + "co = order cd by gpa;"
            + "generate group, flatten(co);"
            + "};";
        buildPlan(query);
    }

    @Test
    public void testQuery84() {
        buildPlan("a = load 'input1' as (name, age, gpa);");
        buildPlan("b = filter a by age < '20';");
        buildPlan("c = group b by (name,age);");
        String query = "d = foreach c {"
            + "cf = filter b by gpa < '3.0';"
            + "cp = cf.$2;"
            + "cd = distinct cp;"
            + "co = order cd by gpa;"
            + "generate group, flatten(co);"
            + "};";
        buildPlan(query);
    }
    
    @Test
    public void testQuery85() throws FrontendException {
        LogicalPlan lp;
        buildPlan("a = load 'myfile' as (name, age, gpa);");
        lp = buildPlan("b = group a by (name, age);");
        LOCogroup cogroup = (LOCogroup) lp.getLeaves().get(0);

        Schema.FieldSchema nameFs = new Schema.FieldSchema("name", DataType.BYTEARRAY);
        Schema.FieldSchema ageFs = new Schema.FieldSchema("age", DataType.BYTEARRAY);
        Schema.FieldSchema gpaFs = new Schema.FieldSchema("gpa", DataType.BYTEARRAY);
        
        Schema groupSchema = new Schema(nameFs);
        groupSchema.add(ageFs);
        Schema.FieldSchema groupFs = new Schema.FieldSchema("group", groupSchema, DataType.TUPLE);
        
        Schema loadSchema = new Schema(nameFs);
        loadSchema.add(ageFs);
        loadSchema.add(gpaFs);

        Schema.FieldSchema bagFs = new Schema.FieldSchema("a", loadSchema, DataType.BAG);
        
        Schema cogroupExpectedSchema = new Schema(groupFs);
        cogroupExpectedSchema.add(bagFs);

        assertTrue(cogroup.getSchema().equals(cogroupExpectedSchema));

        lp = buildPlan("c = foreach b generate group.name, group.age, COUNT(a.gpa);");
        LOForEach foreach  = (LOForEach) lp.getLeaves().get(0);

        Schema foreachExpectedSchema = new Schema(nameFs);
        foreachExpectedSchema.add(ageFs);
        foreachExpectedSchema.add(new Schema.FieldSchema(null, DataType.LONG));

        assertTrue(foreach.getSchema().equals(foreachExpectedSchema));
    }

    @Test
    public void testQuery86() throws FrontendException {
        LogicalPlan lp;
        buildPlan("a = load 'myfile' as (name:Chararray, age:Int, gpa:Float);");
        lp = buildPlan("b = group a by (name, age);");
        LOCogroup cogroup = (LOCogroup) lp.getLeaves().get(0);

        Schema.FieldSchema nameFs = new Schema.FieldSchema("name", DataType.CHARARRAY);
        Schema.FieldSchema ageFs = new Schema.FieldSchema("age", DataType.INTEGER);
        Schema.FieldSchema gpaFs = new Schema.FieldSchema("gpa", DataType.FLOAT);

        Schema groupSchema = new Schema(nameFs);
        groupSchema.add(ageFs);
        Schema.FieldSchema groupFs = new Schema.FieldSchema("group", groupSchema, DataType.TUPLE);

        Schema loadSchema = new Schema(nameFs);
        loadSchema.add(ageFs);
        loadSchema.add(gpaFs);

        Schema.FieldSchema bagFs = new Schema.FieldSchema("a", loadSchema, DataType.BAG);

        Schema cogroupExpectedSchema = new Schema(groupFs);
        cogroupExpectedSchema.add(bagFs);

        assertTrue(cogroup.getSchema().equals(cogroupExpectedSchema));

    }

    @Test
    public void testQuery87() {
        buildPlan("a = load 'myfile';");
        buildPlan("b = group a by $0;");
        LogicalPlan lp = buildPlan("c = foreach b {c1 = order $1 by $1; generate flatten(c1); };");
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        LogicalPlan nestedPlan = foreach.getForEachPlans().get(0);
        LOProject sortInput = (LOProject)nestedPlan.getRoots().get(0);
        LOSort nestedSort = (LOSort)nestedPlan.getSuccessors(sortInput).get(0);
        LogicalPlan sortPlan = nestedSort.getSortColPlans().get(0);
        assertTrue(sortPlan.getLeaves().size() == 1);
    }

    @Test
    public void testQuery88() {
        buildPlan("a = load 'myfile';");
        buildPlan("b = group a by $0;");
        LogicalPlan lp = buildPlan("c = order b by $1 ;");
        LOSort sort = (LOSort)lp.getLeaves().get(0);
        LOProject project1 = (LOProject) sort.getSortColPlans().get(0).getLeaves().get(0) ;
        LOCogroup cogroup = (LOCogroup) lp.getPredecessors(sort).get(0) ;
        assertEquals(project1.getExpression(), cogroup) ;
    }

    @Test
    public void testQuery89() {
        buildPlan("a = load 'myfile';");
        buildPlan("b = foreach a generate $0, $100;");
        buildPlan("c = load 'myfile' as (i: int);");
        buildPlan("d = foreach c generate $0 as zero, i;");
    }

    @Test
    public void testQueryFail89() {
        buildPlan("c = load 'myfile' as (i: int);");
        try {
            buildPlan("d = foreach c generate $0, $5;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Out of bound access"));
        }
    }

    @Test
    public void testQuery90() throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'myfile' as (name:Chararray, age:Int, gpa:Float);");
        buildPlan("b = group a by (name, age);");

        //the first element in group, i.e., name is renamed as myname
        lp = buildPlan("c = foreach b generate flatten(group) as (myname), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("myname: chararray, age: int, mycount: long")));

        //the first and second elements in group, i.e., name and age are renamed as myname and myage
        lp = buildPlan("c = foreach b generate flatten(group) as (myname, myage), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("myname: chararray, myage: int, mycount: long")));

        //the schema of group is unchanged
        lp = buildPlan("c = foreach b generate flatten(group) as (), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("group::name: chararray, group::age: int, mycount: long")));

        //the first element in group, i.e., name is renamed as myname 
        lp = buildPlan("c = foreach b generate flatten(group) as myname, COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("myname: chararray, age: int, mycount: long")));

        //group is renamed as mygroup
        lp = buildPlan("c = foreach b generate group as mygroup, COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("mygroup:(name: chararray, age: int), mycount: long")));

        //group is renamed as mygroup and the first element is renamed as myname
        lp = buildPlan("c = foreach b generate group as mygroup:(myname), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("mygroup:(myname: chararray, age: int), mycount: long")));

        //group is renamed as mygroup and the elements are renamed as myname and myage
        lp = buildPlan("c = foreach b generate group as mygroup:(myname, myage), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("mygroup:(myname: chararray, myage: int), mycount: long")));

        //group is renamed to mygroup as the tuple schema is empty
        lp = buildPlan("c = foreach b generate group as mygroup:(), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("mygroup:(name: chararray, age: int), mycount: long")));

        //setting the schema of flattened bag that has no schema with the user defined schema
        buildPlan("c = load 'another_file';");
        buildPlan("d = cogroup a by $0, c by $0;");
        lp = buildPlan("e = foreach d generate flatten(DIFF(a, c)) as (x, y, z), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("x: bytearray, y: bytearray, z: bytearray, mycount: long")));

        //setting the schema of flattened bag that has no schema with the user defined schema
        buildPlan("c = load 'another_file';");
        buildPlan("d = cogroup a by $0, c by $0;");
        lp = buildPlan("e = foreach d generate flatten(DIFF(a, c)) as (x: int, y: float, z), COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("x: int, y: float, z: bytearray, mycount: long")));

        //setting the schema of flattened bag that has no schema with the user defined schema
        buildPlan("c = load 'another_file';");
        buildPlan("d = cogroup a by $0, c by $0;");
        lp = buildPlan("e = foreach d generate flatten(DIFF(a, c)) as x, COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("x: bytearray, mycount: long")));

        //setting the schema of flattened bag that has no schema with the user defined schema
        buildPlan("c = load 'another_file';");
        buildPlan("d = cogroup a by $0, c by $0;");
        lp = buildPlan("e = foreach d generate flatten(DIFF(a, c)) as x: int, COUNT(a) as mycount;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(foreach.getSchema().equals(Util.getSchemaFromString("x: int, mycount: long")));

    }

    @Test
    public void testQueryFail90() throws FrontendException, ParseException {
        buildPlan("a = load 'myfile' as (name:Chararray, age:Int, gpa:Float);");
        buildPlan("b = group a by (name, age);");

        try {
            buildPlan("c = foreach b generate group as mygroup:(myname, myage, mygpa), COUNT(a) as mycount;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Schema size mismatch"));
        }

        try {
            buildPlan("c = foreach b generate group as mygroup:(myname: int, myage), COUNT(a) as mycount;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Type mismatch"));
        }

        try {
            buildPlan("c = foreach b generate group as mygroup:(myname, myage: chararray), COUNT(a) as mycount;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Type mismatch"));
        }

        try {
            buildPlan("c = foreach b generate group as mygroup:{t: (myname, myage)}, COUNT(a) as mycount;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Type mismatch"));
        }

        try {
            buildPlan("c = foreach b generate flatten(group) as (myname, myage, mygpa), COUNT(a) as mycount;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Schema size mismatch"));
        }
    }
        
    @Test
    public void testQuery91() {
        buildPlan("a = load 'myfile' as (name:Chararray, age:Int, gpa:Float);");
        buildPlan("b = group a by name;");
        buildPlan("c = foreach b generate SUM(a.age) + SUM(a.gpa);");
    }

    @Test
    public void testQuery92() {
        buildPlan("a = load 'myfile' as (name, age, gpa);");
        buildPlan("b = group a by name;");
        String query = "c = foreach b { "
        + " alias = name#'alias'; "
        + " af = alias#'first'; "
        + " al = alias#'last'; "
        + " generate SUM(a.age) + SUM(a.gpa); "
        + "};";
    }

    @Test
    public void testQuery93() throws FrontendException, ParseException {
        buildPlan("a = load 'one' as (name, age, gpa);");
        buildPlan("b = group a by name;");
        buildPlan("c = foreach b generate flatten(a);");
        buildPlan("d = foreach c generate name;");
        // test that we can refer to "name" field and not a::name
        buildPlan("e = foreach d generate name;");
    }
    
    @Test
    public void testQueryFail93() throws FrontendException, ParseException {
        buildPlan("a = load 'one' as (name, age, gpa);");
        buildPlan("b = group a by name;");
        buildPlan("c = foreach b generate flatten(a);");
        buildPlan("d = foreach c generate name;");
        // test that we can refer to "name" field and a::name
        buildPlan("e = foreach d generate a::name;");
    }
    
    @Test
    public void testQuery94() throws FrontendException, ParseException {
        buildPlan("a = load 'one' as (name, age, gpa);");
        buildPlan("b = load 'two' as (name, age, somethingelse);");
        buildPlan("c = cogroup a by name, b by name;");
        buildPlan("d = foreach c generate flatten(a), flatten(b);");
        // test that we can refer to "a::name" field and not name
        // test that we can refer to "b::name" field and not name
        buildPlan("e = foreach d generate a::name, b::name;");
        // test that we can refer to gpa and somethingelse
        buildPlan("f = foreach d generate gpa, somethingelse, a::gpa, b::somethingelse;");
        
    }
    
    @Test
    public void testQueryFail94() throws FrontendException, ParseException {
        buildPlan("a = load 'one' as (name, age, gpa);");
        buildPlan("b = load 'two' as (name, age, somethingelse);");
        buildPlan("c = cogroup a by name, b by name;");
        buildPlan("d = foreach c generate flatten(a), flatten(b);");
        // test that we can refer to "a::name" field and not name
        try {
            buildPlan("e = foreach d generate name;");
        } catch (AssertionFailedError e) {
            assertTrue(e.getMessage().contains("Found more than one match:"));
        }
    }

    @Test
    public void testQuery95() throws FrontendException, ParseException {
        buildPlan("a = load 'myfile' as (name, age, gpa);");
        buildPlan("b = group a by name;");
        LogicalPlan lp = buildPlan("c = foreach b {d = order a by $1; generate flatten(d), MAX(a.age) as max_age;};");
        LOForEach foreach = (LOForEach) lp.getLeaves().get(0);
        LOCogroup cogroup = (LOCogroup) lp.getPredecessors(foreach).get(0);
        Schema.FieldSchema bagFs = new Schema.FieldSchema("a", Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray"), DataType.BAG);
        Schema.FieldSchema groupFs = new Schema.FieldSchema("group", DataType.BYTEARRAY);
        Schema cogroupExpectedSchema = new Schema();
        cogroupExpectedSchema.add(groupFs);
        cogroupExpectedSchema.add(bagFs);
        assertTrue(Schema.equals(cogroup.getSchema(), cogroupExpectedSchema, false, false));
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray, max_age: double"), false, true));
    }

    @Test
    public void testQuery96() throws FrontendException, ParseException {
        buildPlan("a = load 'input' as (name, age, gpa);");
        buildPlan("b = filter a by age < 20;");
        buildPlan("c = group b by age;");
        String query = "d = foreach c {"
        + "cf = filter b by gpa < 3.0;"
        + "cd = distinct cf.gpa;"
        + "co = order cd by $0;"
        + "generate group, flatten(co);"
        + "};";
        LogicalPlan lp = buildPlan(query);

        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ArrayList<LogicalPlan> foreachPlans = foreach.getForEachPlans();
        LogicalPlan flattenPlan = foreachPlans.get(1);
        LogicalOperator project = flattenPlan.getLeaves().get(0);
        assertTrue(project instanceof LOProject);
        LogicalOperator sort = flattenPlan.getPredecessors(project).get(0);
        assertTrue(sort instanceof LOSort);
        LogicalOperator distinct = flattenPlan.getPredecessors(sort).get(0);
        assertTrue(distinct instanceof LODistinct);

        //testing the presence of the nested foreach
        LogicalOperator nestedForeach = flattenPlan.getPredecessors(distinct).get(0);
        assertTrue(nestedForeach instanceof LOForEach);
        LogicalPlan nestedForeachPlan = ((LOForEach)nestedForeach).getForEachPlans().get(0);
        LogicalOperator nestedProject = nestedForeachPlan.getRoots().get(0);
        assertTrue(nestedProject instanceof LOProject);
        assertTrue(((LOProject)nestedProject).getCol() == 2);

        //testing the filter inner plan for the absence of the project connected to project
        LogicalOperator filter = flattenPlan.getPredecessors(nestedForeach).get(0);
        assertTrue(filter instanceof LOFilter);
        LogicalPlan comparisonPlan = ((LOFilter)filter).getComparisonPlan();
        LOLesserThan lessThan = (LOLesserThan)comparisonPlan.getLeaves().get(0);
        LOProject filterProject = (LOProject)lessThan.getLhsOperand();
        assertTrue(null == comparisonPlan.getPredecessors(filterProject));
    }

    @Test
    public void testQuery97() throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a generate 1;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("x: int"), false, true));

        lp = buildPlan("b = foreach a generate 1L;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("x: long"), false, true));

        lp = buildPlan("b = foreach a generate 1.0;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("x: double"), false, true));

        lp = buildPlan("b = foreach a generate 1.0f;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("x: float"), false, true));

        lp = buildPlan("b = foreach a generate 'hello';");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("x: chararray"), false, true));
    }

    @Test
    public void testQuery98() throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a generate (1);");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: int)"), false, true));

        lp = buildPlan("b = foreach a generate (1L);");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: long)"), false, true));

        lp = buildPlan("b = foreach a generate (1.0);");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: double)"), false, true));

        lp = buildPlan("b = foreach a generate (1.0f);");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: float)"), false, true));

        lp = buildPlan("b = foreach a generate ('hello');");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: chararray)"), false, true));

        lp = buildPlan("b = foreach a generate ('hello', 1, 1L, 1.0f, 1.0);");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: chararray, y: int, z: long, a: float, b: double)"), false, true));

        lp = buildPlan("b = foreach a generate ('hello', {(1), (1.0)});");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("t:(x: chararray, ib:{it:(d: double)})"), false, true));

    }

    @Test
    public void testQuery99() throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a generate {(1, 'hello'), (2, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: int, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1, 'hello'), (1L, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: long, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1, 'hello'), (1.0f, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: float, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1, 'hello'), (1.0, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: double, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1L, 'hello'), (1.0f, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: float, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1L, 'hello'), (1.0, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: double, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1.0f, 'hello'), (1.0, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: double, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1.0, 'hello'), (1.0f, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:(x: double, y: chararray)}"), false, true));

        lp = buildPlan("b = foreach a generate {(1.0, 'hello', 3.14), (1.0f, 'world')};");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("b:{t:()}"), false, true));

    }

    @Test
    public void testQuery101() {
        // test usage of an alias from define
        String query = "define FUNC ARITY();";
        buildPlan(query);

        query = "foreach (load 'data') generate FUNC($0);";
        buildPlan(query);
    }

    @Test
    public void testQuery102() {
        // test basic store
        buildPlan("a = load 'a';");
        buildPlan("store a into 'out';");
    }

    @Test
    public void testQuery103() {
        // test store with store function
        buildPlan("a = load 'a';");
        buildPlan("store a into 'out' using PigStorage();");
    }

    @Test
    public void testQuery104() {
        // check that a field alias can be referenced
        // by unambiguous free form alias, fully qualified alias
        // and partially qualified unambiguous alias
        buildPlan( "a = load 'st10k' as (name, age, gpa);" );
        buildPlan( "b = group a by name;" );
        buildPlan("c = foreach b generate flatten(a);" );
        buildPlan("d = filter c by name != 'fred';" );
        buildPlan("e = group d by name;" );
        buildPlan("f = foreach e generate flatten(d);" );
        buildPlan("g = foreach f generate name, d::a::name, a::name;");

    }

    @Test
    public void testQuery105() {
        // test that the alias "group" can be used
        // after a flatten(group)
        buildPlan( "a = load 'st10k' as (name, age, gpa);" );
        buildPlan("b = group a by name;" );
        buildPlan("c = foreach b generate flatten(group), COUNT(a) as cnt;" );
        buildPlan("d = foreach c generate group;");

    }

    @Test
    public void testQuery106()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a generate *;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        assertTrue(Schema.equals(foreach.getSchema(), Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray"), false, true));

    }

    @Test
    public void testQuery107()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'one';");

        lp = buildPlan("b = foreach a generate *;");
        foreach = (LOForEach) lp.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        assertTrue(checkPlanForProjectStar(foreachPlan));

    }

    @Test
    public void testQuery108()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOCogroup cogroup;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = group a by *;");
        cogroup = (LOCogroup) lp.getLeaves().get(0);
        Schema groupSchema = Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray");
        Schema bagASchema = Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray");
        Schema.FieldSchema groupFs = new Schema.FieldSchema("group", groupSchema, DataType.TUPLE);
        Schema.FieldSchema bagAFs = new Schema.FieldSchema("a", bagASchema, DataType.BAG);
        Schema expectedSchema = new Schema(groupFs);
        expectedSchema.add(bagAFs);
        assertTrue(Schema.equals(cogroup.getSchema(), expectedSchema, false, true));

    }

    @Test
    public void testQuery109()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOCogroup cogroup;

        buildPlan("a = load 'one' as (name, age, gpa);");
        buildPlan("b = load 'two' as (first_name, enrol_age, high_school_gpa);");

        lp = buildPlan("c = group a by *, b by *;");
        cogroup = (LOCogroup) lp.getLeaves().get(0);
        Schema groupSchema = Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray");
        Schema bagASchema = Util.getSchemaFromString("name: bytearray, age: bytearray, gpa: bytearray");
        Schema bagBSchema = Util.getSchemaFromString("first_name: bytearray, enrol_age: bytearray, high_school_gpa: bytearray");
        Schema.FieldSchema groupFs = new Schema.FieldSchema("group", groupSchema, DataType.TUPLE);
        Schema.FieldSchema bagAFs = new Schema.FieldSchema("a", bagASchema, DataType.BAG);
        Schema.FieldSchema bagBFs = new Schema.FieldSchema("b", bagBSchema, DataType.BAG);
        Schema expectedSchema = new Schema(groupFs);
        expectedSchema.add(bagAFs);
        expectedSchema.add(bagBFs);
        assertTrue(Schema.equals(cogroup.getSchema(), expectedSchema, false, true));

    }

    @Test
    public void testQuery110()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOLoad load;
        LOCogroup cogroup;

        buildPlan("a = load 'one' as (name, age, gpa);");
        lp = buildPlan("b = load 'two';");

        load = (LOLoad) lp.getLeaves().get(0);

        lp = buildPlan("c = cogroup a by $0, b by *;");
        cogroup = (LOCogroup) lp.getLeaves().get(0);

        MultiMap<LogicalOperator, LogicalPlan> mapGByPlans = cogroup.getGroupByPlans();
        LogicalPlan cogroupPlan = (LogicalPlan)(mapGByPlans.get(load).toArray())[0];
        assertTrue(checkPlanForProjectStar(cogroupPlan) == true);

    }

    @Test
    public void testQuery111()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOSort sort;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = order a by *;");
        sort = (LOSort) lp.getLeaves().get(0);

        for(LogicalPlan sortPlan: sort.getSortColPlans()) {
            assertTrue(checkPlanForProjectStar(sortPlan) == false);
        }

    }

    @Test
    public void testQuery112()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;
        LOSort sort;

        buildPlan("a = load 'one' as (name, age, gpa);");

        buildPlan("b = group a by *;");
        lp = buildPlan("c = foreach b {a1 = order a by *; generate a1;};");
        foreach = (LOForEach) lp.getLeaves().get(0);

        for(LogicalPlan foreachPlan: foreach.getForEachPlans()) {
            assertTrue(checkPlanForProjectStar(foreachPlan) == true);
        }

        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        sort = (LOSort)foreachPlan.getPredecessors(foreachPlan.getLeaves().get(0)).get(0);

        for(LogicalPlan sortPlan: sort.getSortColPlans()) {
            assertTrue(checkPlanForProjectStar(sortPlan) == true);
        }

    }

    @Test
    public void testQuery113()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;
        LOSort sort;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a {exp1 = age + gpa; exp2 = exp1 + age; generate exp1, exp2;};");
        foreach = (LOForEach) lp.getLeaves().get(0);

        for(LogicalPlan foreachPlan: foreach.getForEachPlans()) {
            printPlan(foreachPlan);
            assertTrue(checkPlanForProjectStar(foreachPlan) == false);
        }

    }

    @Test
    public void testQuery114()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;
        LOSort sort;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a generate " + Identity.class.getName() + "(name, age);");
        foreach = (LOForEach) lp.getLeaves().get(0);

        Schema s = new Schema();
        s.add(new Schema.FieldSchema("name", DataType.BYTEARRAY));
        s.add(new Schema.FieldSchema("age", DataType.BYTEARRAY));
        Schema.FieldSchema tupleFs = new Schema.FieldSchema(null, s, DataType.TUPLE);
        Schema expectedSchema = new Schema(tupleFs);
        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));

    }

    @Test
    public void testQuery115()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;
        LOSort sort;

        buildPlan("a = load 'one' as (name, age, gpa);");

        lp = buildPlan("b = foreach a generate " + Identity.class.getName() + "(*);");
        foreach = (LOForEach) lp.getLeaves().get(0);

        Schema s = new Schema();
        s.add(new Schema.FieldSchema("name", DataType.BYTEARRAY));
        s.add(new Schema.FieldSchema("age", DataType.BYTEARRAY));
        s.add(new Schema.FieldSchema("gpa", DataType.BYTEARRAY));
        Schema.FieldSchema tupleFs = new Schema.FieldSchema(null, s, DataType.TUPLE);
        Schema expectedSchema = new Schema(tupleFs);
        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));

    }

    @Test
    public void testQuery116()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;
        LOSort sort;

        buildPlan("a = load 'one';");

        lp = buildPlan("b = foreach a generate " + Identity.class.getName() + "($0, $1);");
        foreach = (LOForEach) lp.getLeaves().get(0);

        Schema s = new Schema();
        s.add(new Schema.FieldSchema(null, DataType.BYTEARRAY));
        s.add(new Schema.FieldSchema(null, DataType.BYTEARRAY));
        Schema.FieldSchema tupleFs = new Schema.FieldSchema(null, s, DataType.TUPLE);
        Schema expectedSchema = new Schema(tupleFs);
        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));

    }

    @Test
    public void testQuery117()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;
        LOSort sort;

        buildPlan("a = load 'one';");

        lp = buildPlan("b = foreach a generate " + Identity.class.getName() + "(*);");
        foreach = (LOForEach) lp.getLeaves().get(0);

        Schema.FieldSchema tupleFs = new Schema.FieldSchema(null, null, DataType.TUPLE);
        Schema expectedSchema = new Schema(tupleFs);
        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));

    }

    @Test
    public void testNullConsArithExprs() {
        buildPlan("a = load 'a' as (x:int, y:double);" );
        buildPlan("b = foreach a generate x + null, x * null, x / null, x - null, null % x, " +
                "y + null, y * null, y / null, y - null;"
        );
    }

    @Test
    public void testNullConsBincond1() {
        buildPlan("a = load 'a' as (x:int, y:double);" );
        buildPlan("b = foreach a generate (2 > 1? null : 1), ( 2 < 1 ? null : 1), " +
                "(2 > 1 ? 1 : null), ( 2 < 1 ? 1 : null);"
        );
    }

    @Test
    public void testNullConsBincond2() {
        buildPlan("a = load 'a' as (x:int, y:double);" );
        buildPlan("b = foreach a generate (null is null ? 1 : 2), ( null is not null ? 2 : 1);");
    }

    @Test
    public void testNullConsForEachGenerate() {
        buildPlan("a = load 'a' as (x:int, y:double);" );
        buildPlan("b = foreach a generate x, null, y, null;");

    }

    @Test
    public void testNullConsOuterJoin() {
        buildPlan("a = load 'a' as (x:int, y:chararray);" );
        buildPlan("b = load 'b' as (u:int, v:chararray);" );
        buildPlan("c = cogroup a by x, b by u;" );
        buildPlan("d = foreach c generate flatten((SIZE(a) == 0 ? null : a)), " +
                "flatten((SIZE(b) == 0 ? null : b));"
        );
    }

    @Test
    public void testNullConsConcatSize() {
        buildPlan("a = load 'a' as (x:int, y:double, str:chararray);" );
        buildPlan("b = foreach a generate SIZE(null), CONCAT(str, null), " + 
                "CONCAT(null, str);"
        );
    }

    @Test
    public void testFilterUdfDefine() {
        buildPlan("define isempty IsEmpty();");
        buildPlan("a = load 'a' as (x:int, y:double, str:chararray);");
        buildPlan("b = filter a by isempty(*);");
    }

    @Test
    public void testLoadUdfDefine() {
        buildPlan("define PS PigStorage();");
        buildPlan("a = load 'a' using PS as (x:int, y:double, str:chararray);" );
        buildPlan("b = filter a by IsEmpty(*);");
    }

    @Test
    public void testLoadUdfConstructorArgDefine() {
        buildPlan("define PS PigStorage(':');");
        buildPlan("a = load 'a' using PS as (x:int, y:double, str:chararray);" );
        buildPlan("b = filter a by IsEmpty(*);");
    }

    @Test
    public void testStoreUdfDefine() {
        buildPlan( "define PS PigStorage();");
        buildPlan("a = load 'a' using PS as (x:int, y:double, str:chararray);" );
        buildPlan("b = filter a by IsEmpty(*);" );
        buildPlan("store b into 'x' using PS;");
    }

    @Test
    public void testStoreUdfConstructorArgDefine() {
        buildPlan( "define PS PigStorage(':');");
        buildPlan(" a = load 'a' using PS as (x:int, y:double, str:chararray);" );
        buildPlan(" b = filter a by IsEmpty(*);" );
        buildPlan(" store b into 'x' using PS;") ;

    }

    @Test
    public void testCastAlias() {
        buildPlan("a = load 'one.txt' as (x,y); ");
        buildPlan("b =  foreach a generate (int)x, (double)y;");
        buildPlan("c = group b by x;");
    }

    @Test
    public void testCast() {
        buildPlan("a = load 'one.txt' as (x,y); " );
        buildPlan("b = foreach a generate (int)$0, (double)$1;" ); 
        buildPlan("c = group b by $0;");
    }

    @Test
    public void testReservedWordsInFunctionNames() {
        // test that define can contain reserved words are later parts of
        // fully qualified function name
        String query = "define FUNC org.apache.iterators.foreach();";
        LogicalOperator lo = buildPlan(query).getRoots().get(0);
        assertTrue(lo instanceof LODefine);
    }


    @Test
    public void testTokenizeSchema()  throws FrontendException, ParseException {
        LogicalPlan lp;
        LOForEach foreach;

        buildPlan("a = load 'one' as (f1: chararray);");
        lp = buildPlan("b = foreach a generate TOKENIZE(f1);");
        foreach = (LOForEach) lp.getLeaves().get(0);

        Schema.FieldSchema tokenFs = new Schema.FieldSchema("token", 
                DataType.CHARARRAY); 
        Schema tupleSchema = new Schema(tokenFs);

        Schema.FieldSchema tupleFs;
        tupleFs = new Schema.FieldSchema("tuple_of_tokens", tupleSchema,
                DataType.TUPLE);

        Schema bagSchema = new Schema(tupleFs);
        bagSchema.setTwoLevelAccessRequired(true);
        Schema.FieldSchema bagFs = new Schema.FieldSchema(
                    "bag_of_tokenTuples",bagSchema, DataType.BAG);
        
        assertTrue(Schema.equals(foreach.getSchema(), new Schema(bagFs), false, true));
    }
    
    private void printPlan(LogicalPlan lp) {
        LOPrinter graphPrinter = new LOPrinter(System.err, lp);
        System.err.println("Printing the logical plan");
        try {
            graphPrinter.visit();
        } catch (Exception e) {
            System.err.println(e.getMessage());
        }
        System.err.println();
    }
    
    private boolean checkPlanForProjectStar(LogicalPlan lp) {
        List<LogicalOperator> leaves = lp.getLeaves();

        for(LogicalOperator op: leaves) {
            if(op instanceof LOProject) {
                if(((LOProject) op).isStar()) {
                    return true;
                }
            }
        }

        return false;
    }

    // Helper Functions
    
    // Helper Functions
    // =================
    public LogicalPlan buildPlan(String query) {
        return buildPlan(query, LogicalPlanBuilder.class.getClassLoader());
    }

    public LogicalPlan buildPlan(String query, ClassLoader cldr) {
        LogicalPlanBuilder.classloader = cldr;

        try {
            pigContext.connect();
            LogicalPlanBuilder builder = new LogicalPlanBuilder(pigContext); //

            LogicalPlan lp = builder.parse("Test-Plan-Builder",
                                           query,
                                           aliases,
                                           logicalOpTable,
                                           aliasOp,
                                           fileNameMap);
            List<LogicalOperator> roots = lp.getRoots();
            
            if(roots.size() > 0) {
                for(LogicalOperator op: roots) {
                    if (!(op instanceof LOLoad) && !(op instanceof LODefine)){
                        throw new Exception("Cannot have a root that is not the load or define operator. Found " + op.getClass().getName());
                    }
                }
            }
            
            //System.err.println("Query: " + query);
            
            assertNotNull(lp != null);
            return lp;
        } catch (IOException e) {
            // log.error(e);
            //System.err.println("IOException Stack trace for query: " + query);
            //e.printStackTrace();
            PigException pe = LogUtils.getPigException(e);
            fail("IOException: " + (pe == null? e.getMessage(): pe.getMessage()));
        } catch (Exception e) {
            log.error(e);
            //System.err.println("Exception Stack trace for query: " + query);
            //e.printStackTrace();
            PigException pe = LogUtils.getPigException(e);
            fail(e.getClass().getName() + ": " + (pe == null? e.getMessage(): pe.getMessage()) + " -- " + query);
        }
        return null;
    }
    
    Map<LogicalOperator, LogicalPlan> aliases = new HashMap<LogicalOperator, LogicalPlan>();
    Map<OperatorKey, LogicalOperator> logicalOpTable = new HashMap<OperatorKey, LogicalOperator>();
    Map<String, LogicalOperator> aliasOp = new HashMap<String, LogicalOperator>();
    Map<String, String> fileNameMap = new HashMap<String, String>();
    PigContext pigContext = new PigContext(ExecType.LOCAL, new Properties());
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.backend.hadoop.executionengine;

import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.FileOutputStream;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.PrintStream;
import java.net.InetAddress;
import java.net.Socket;
import java.net.SocketException;
import java.net.SocketImplFactory;
import java.net.URI;
import java.net.URISyntaxException;
import java.net.UnknownHostException;
import java.util.Collection;
import java.util.List;
import java.util.ArrayList;
import java.util.LinkedList;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Properties;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.ipc.RPC;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.JobTracker;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.executionengine.ExecJob;
import org.apache.pig.backend.executionengine.ExecPhysicalOperator;
import org.apache.pig.backend.executionengine.ExecutionEngine;
import org.apache.pig.backend.executionengine.util.ExecTools;
import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
import org.apache.pig.backend.hadoop.datastorage.HDataStorage;
import org.apache.pig.builtin.BinStorage;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PlanPrinter;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.shock.SSHSocketImplFactory;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.tools.pigstats.PigStats;

public class HExecutionEngine implements ExecutionEngine {
    
    private static final String HOD_SERVER = "hod.server";
    public static final String JOB_TRACKER_LOCATION = "mapred.job.tracker";
    private static final String FILE_SYSTEM_LOCATION = "fs.default.name";
    
    private final Log log = LogFactory.getLog(getClass());
    private static final String LOCAL = "local";
    
    private StringBuilder hodParams = null;
    
    protected PigContext pigContext;
    
    protected DataStorage ds;
    
    protected JobClient jobClient;

    // key: the operator key from the logical plan that originated the physical plan
    // val: the operator key for the root of the phyisical plan
    protected Map<OperatorKey, OperatorKey> logicalToPhysicalKeys;
    
    protected Map<OperatorKey, ExecPhysicalOperator> physicalOpTable;
    
    // map from LOGICAL key to into about the execution
    protected Map<OperatorKey, MapRedResult> materializedResults;
    
    public HExecutionEngine(PigContext pigContext) {
        this.pigContext = pigContext;
        this.logicalToPhysicalKeys = new HashMap<OperatorKey, OperatorKey>();
        this.physicalOpTable = new HashMap<OperatorKey, ExecPhysicalOperator>();
        this.materializedResults = new HashMap<OperatorKey, MapRedResult>();
        
        this.ds = null;
        
        // to be set in the init method
        this.jobClient = null;
    }
    
    public JobClient getJobClient() {
        return this.jobClient;
    }
    
    public Map<OperatorKey, MapRedResult> getMaterializedResults() {
        return this.materializedResults;
    }
    
    public Map<OperatorKey, ExecPhysicalOperator> getPhysicalOpTable() {
        return this.physicalOpTable;
    }
    
    
    public DataStorage getDataStorage() {
        return this.ds;
    }

    public void init() throws ExecException {
        init(this.pigContext.getProperties());
    }
    
    public void init(Properties properties) throws ExecException {
        //First set the ssh socket factory
        setSSHFactory();
        
        String hodServer = properties.getProperty(HOD_SERVER);
        String cluster = null;
        String nameNode = null;
        Configuration configuration = null;
    
        if (hodServer != null && hodServer.length() > 0) {
            String hdfsAndMapred[] = doHod(hodServer, properties);
            properties.setProperty(FILE_SYSTEM_LOCATION, hdfsAndMapred[0]);
            properties.setProperty(JOB_TRACKER_LOCATION, hdfsAndMapred[1]);
        }
        else {
            
            // We need to build a configuration object first in the manner described below
            // and then get back a properties object to inspect the JOB_TRACKER_LOCATION
            // and FILE_SYSTEM_LOCATION. The reason to do this is if we looked only at
            // the existing properties object, we may not get the right settings. So we want
            // to read the configurations in the order specified below and only then look
            // for JOB_TRACKER_LOCATION and FILE_SYSTEM_LOCATION.
            
            // Hadoop by default specifies two resources, loaded in-order from the classpath:
            // 1. hadoop-default.xml : Read-only defaults for hadoop.
            // 2. hadoop-site.xml: Site-specific configuration for a given hadoop installation.
            // Now add the settings from "properties" object to override any existing properties
            // All of the above is accomplished in the method call below
           
            JobConf jobConf = new JobConf();
            jobConf.addResource("pig-cluster-hadoop-site.xml");
            
            //the method below alters the properties object by overriding the
            //hadoop properties with the values from properties and recomputing
            //the properties
            recomputeProperties(jobConf, properties);
            
            configuration = ConfigurationUtil.toConfiguration(properties);            
            properties = ConfigurationUtil.toProperties(configuration);
            cluster = properties.getProperty(JOB_TRACKER_LOCATION);
            nameNode = properties.getProperty(FILE_SYSTEM_LOCATION);
            
            if (cluster != null && cluster.length() > 0) {
                if(!cluster.contains(":") && !cluster.equalsIgnoreCase(LOCAL)) {
                    cluster = cluster + ":50020";
                }
                properties.setProperty(JOB_TRACKER_LOCATION, cluster);
            }

            if (nameNode!=null && nameNode.length() > 0) {
                if(!nameNode.contains(":")  && !nameNode.equalsIgnoreCase(LOCAL)) {
                    nameNode = nameNode + ":8020";
                }
                properties.setProperty(FILE_SYSTEM_LOCATION, nameNode);
            }
        }
     
        log.info("Connecting to hadoop file system at: "  + (nameNode==null? LOCAL: nameNode) )  ;
        ds = new HDataStorage(properties);
                
        // The above HDataStorage constructor sets DEFAULT_REPLICATION_FACTOR_KEY in properties.
        // So we need to reconstruct the configuration object for the non HOD case
        // In the HOD case, this is the first time the configuration object will be created
        configuration = ConfigurationUtil.toConfiguration(properties);
        
            
        if(cluster != null && !cluster.equalsIgnoreCase(LOCAL)){
                log.info("Connecting to map-reduce job tracker at: " + properties.get(JOB_TRACKER_LOCATION));
        }

        try {
            // Set job-specific configuration knobs
            jobClient = new JobClient(new JobConf(configuration));
        }
        catch (IOException e) {
            int errCode = 6009;
            String msg = "Failed to create job client:" + e.getMessage();
            throw new ExecException(msg, errCode, PigException.BUG, e);
        }
    }

    public void close() throws ExecException {
        closeHod(pigContext.getProperties().getProperty("hod.server"));
    }
        
    public Properties getConfiguration() throws ExecException {
        return this.pigContext.getProperties();
    }
        
    public void updateConfiguration(Properties newConfiguration) 
            throws ExecException {
        init(newConfiguration);
    }
        
    public Map<String, Object> getStatistics() throws ExecException {
        throw new UnsupportedOperationException();
    }

    public PhysicalPlan compile(LogicalPlan plan,
                                Properties properties) throws ExecException {
        if (plan == null) {
            int errCode = 2041;
            String msg = "No Plan to compile";
            throw new ExecException(msg, errCode, PigException.BUG);
        }

        try {
            LogToPhyTranslationVisitor translator = 
                new LogToPhyTranslationVisitor(plan);
            translator.setPigContext(pigContext);
            translator.visit();
            return translator.getPhysicalPlan();
        } catch (VisitorException ve) {
            int errCode = 2042;
            String msg = "Internal error. Unable to translate logical plan to physical plan.";
            throw new ExecException(msg, errCode, PigException.BUG, ve);
        }
    }

    public List<ExecJob> execute(PhysicalPlan plan,
                                 String jobName) throws ExecException {
        MapReduceLauncher launcher = new MapReduceLauncher();
        List<ExecJob> jobs = new ArrayList<ExecJob>();

        try {
            PigStats stats = launcher.launchPig(plan, jobName, pigContext);

            for (FileSpec spec: launcher.getSucceededFiles()) {
                jobs.add(new HJob(ExecJob.JOB_STATUS.COMPLETED, pigContext, spec, stats));
            }

            for (FileSpec spec: launcher.getFailedFiles()) {
                HJob j = new HJob(ExecJob.JOB_STATUS.FAILED, pigContext, spec, stats);
                j.setException(launcher.getError(spec));
                jobs.add(j);
            }

            return jobs;
        } catch (Exception e) {
            // There are a lot of exceptions thrown by the launcher.  If this
            // is an ExecException, just let it through.  Else wrap it.
            if (e instanceof ExecException) throw (ExecException)e;
            else {
                int errCode = 2043;
                String msg = "Unexpected error during execution.";
                throw new ExecException(msg, errCode, PigException.BUG, e);
            }
        } finally {
            launcher.reset();
        }

    }

    public List<ExecJob> submit(PhysicalPlan plan,
                          String jobName) throws ExecException {
        throw new UnsupportedOperationException();
    }

    public void explain(PhysicalPlan plan, PrintStream stream, String format, boolean verbose) {
        try {
            ExecTools.checkLeafIsStore(plan, pigContext);

            MapReduceLauncher launcher = new MapReduceLauncher();
            launcher.explain(plan, pigContext, stream, format, verbose);

        } catch (Exception ve) {
            throw new RuntimeException(ve);
        }
    }

    public Collection<ExecJob> runningJobs(Properties properties) throws ExecException {
        throw new UnsupportedOperationException();
    }
    
    public Collection<String> activeScopes() throws ExecException {
        throw new UnsupportedOperationException();
    }
    
    public void reclaimScope(String scope) throws ExecException {
        throw new UnsupportedOperationException();
    }
    
    private void setSSHFactory(){
        Properties properties = this.pigContext.getProperties();
        String g = properties.getProperty("ssh.gateway");
        if (g == null || g.length() == 0) return;
        try {
            Class clazz = Class.forName("org.apache.pig.shock.SSHSocketImplFactory");
            SocketImplFactory f = (SocketImplFactory)clazz.getMethod("getFactory", new Class[0]).invoke(0, new Object[0]);
            Socket.setSocketImplFactory(f);
        } 
        catch (SocketException e) {}
        catch (Exception e){
            throw new RuntimeException(e);
        }
    }

    //To prevent doing hod if the pig server is constructed multiple times
    private static String hodMapRed;
    private static String hodHDFS;
    private String hodConfDir = null; 
    private String remoteHodConfDir = null; 
    private Process hodProcess = null;

    class ShutdownThread extends Thread{
        public synchronized void run() {
            closeHod(pigContext.getProperties().getProperty("hod.server"));
        }
    }
    
    private String[] doHod(String server, Properties properties) throws ExecException {
        if (hodMapRed != null) {
            return new String[] {hodHDFS, hodMapRed};
        }
        
        try {
            // first, create temp director to store the configuration
            hodConfDir = createTempDir(server);
			
            //jz: fallback to systemproperty cause this not handled in Main
            hodParams = new StringBuilder(properties.getProperty(
                    "hod.param", System.getProperty("hod.param", "")));
            // get the number of nodes out of the command or use default
            int nodes = getNumNodes(hodParams);

            // command format: hod allocate - d <cluster_dir> -n <number_of_nodes> <other params>
            String[] fixedCmdArray = new String[] { "hod", "allocate", "-d",
                                       hodConfDir, "-n", Integer.toString(nodes) };
            String[] extraParams = hodParams.toString().split(" ");
    
            String[] cmdarray = new String[fixedCmdArray.length + extraParams.length];
            System.arraycopy(fixedCmdArray, 0, cmdarray, 0, fixedCmdArray.length);
            System.arraycopy(extraParams, 0, cmdarray, fixedCmdArray.length, extraParams.length);

            log.info("Connecting to HOD...");
            log.debug("sending HOD command " + cmdToString(cmdarray));

            // setup shutdown hook to make sure we tear down hod connection
            Runtime.getRuntime().addShutdownHook(new ShutdownThread());

            runCommand(server, cmdarray, true);

            // print all the information provided by HOD
            try {
                BufferedReader br = new BufferedReader(new InputStreamReader(hodProcess.getErrorStream()));
                String msg;
                while ((msg = br.readLine()) != null)
                    log.info(msg);
                br.close();
            } catch(IOException ioe) {}

            // for remote connection we need to bring the file locally  
            if (!server.equals(LOCAL))
                hodConfDir = copyHadoopConfLocally(server);

            String hdfs = null;
            String mapred = null;
            String hadoopConf = hodConfDir + "/hadoop-site.xml";

            log.info ("Hadoop configuration file: " + hadoopConf);

            JobConf jobConf = new JobConf(hadoopConf);
            jobConf.addResource("pig-cluster-hadoop-site.xml");

            //the method below alters the properties object by overriding the
            //hod properties with the values from properties and recomputing
            //the properties
            recomputeProperties(jobConf, properties);
            
            hdfs = properties.getProperty(FILE_SYSTEM_LOCATION);
            if (hdfs == null) {
                int errCode = 4007;
                String msg = "Missing fs.default.name from hadoop configuration.";
                throw new ExecException(msg, errCode, PigException.USER_ENVIRONMENT);
            }
            log.info("HDFS: " + hdfs);

            mapred = properties.getProperty(JOB_TRACKER_LOCATION);
            if (mapred == null) {
                int errCode = 4007;
                String msg = "Missing mapred.job.tracker from hadoop configuration";
                throw new ExecException(msg, errCode, PigException.USER_ENVIRONMENT);
            }
            log.info("JobTracker: " + mapred);

            // this is not longer needed as hadoop-site.xml given to us by HOD
            // contains data in the correct format
            // hdfs = fixUpDomain(hdfs, properties);
            // mapred = fixUpDomain(mapred, properties);
            hodHDFS = hdfs;
            hodMapRed = mapred;

            return new String[] {hdfs, mapred};
        } 
        catch (Exception e) {
            int errCode = 6010;
            String msg = "Could not connect to HOD";
            throw new ExecException(msg, errCode, PigException.REMOTE_ENVIRONMENT, e);
        }
    }

    private synchronized void closeHod(String server){
            if (hodProcess == null){
                // just cleanup the dir if it exists and return
                if (hodConfDir != null)
                    deleteDir(server, hodConfDir);
                return;
            }

            // hod deallocate format: hod deallocate -d <conf dir>
            String[] cmdarray = new String[4];
			cmdarray[0] = "hod";
            cmdarray[1] = "deallocate";
            cmdarray[2] = "-d";
            if (remoteHodConfDir != null)
                cmdarray[3] = remoteHodConfDir;
            else
                cmdarray[3] = hodConfDir;
            
            log.info("Disconnecting from HOD...");
            log.debug("Disconnect command: " + cmdToString(cmdarray));

            try {
                runCommand(server, cmdarray, false);
           } catch (Exception e) {
                log.warn("Failed to disconnect from HOD; error: " + e.getMessage());
                hodProcess.destroy();
           } finally {
               if (remoteHodConfDir != null){
                   deleteDir(server, remoteHodConfDir);
                   if (hodConfDir != null)
                       deleteDir(LOCAL, hodConfDir);
               }else
                   deleteDir(server, hodConfDir);
           }

           hodProcess = null;
    }

    private String copyHadoopConfLocally(String server) throws ExecException {
        String localDir = createTempDir(LOCAL);
        String remoteFile = new String(hodConfDir + "/hadoop-site.xml");
        String localFile = new String(localDir + "/hadoop-site.xml");

        remoteHodConfDir = hodConfDir;

        String[] cmdarray = new String[2];
        cmdarray[0] = "cat";
        cmdarray[1] = remoteFile;

        Process p = runCommand(server, cmdarray, false);

        BufferedWriter bw;
        try {
            bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(localFile)));
        } catch (Exception e){
            int errCode = 4008;
            String msg = "Failed to create local hadoop file " + localFile;
            throw new ExecException(msg, errCode, PigException.USER_ENVIRONMENT, e);
        }

        try {
            BufferedReader br = new BufferedReader(new InputStreamReader(p.getInputStream()));
            String line;
            while ((line = br.readLine()) != null){
                bw.write(line, 0, line.length());
                bw.newLine();
            }
            br.close();
            bw.close();
        } catch (Exception e){
            int errCode = 4009;
            String msg = "Failed to copy data to local hadoop file " + localFile;
            throw new ExecException(msg, errCode, PigException.USER_ENVIRONMENT, e);
        }

        return localDir;
    }

    private String cmdToString(String[] cmdarray) {
        StringBuilder cmd = new StringBuilder();

        for (int i = 0; i < cmdarray.length; i++) {
            cmd.append(cmdarray[i]);
            cmd.append(' ');
        }

        return cmd.toString();
    }
    private Process runCommand(String server, String[] cmdarray, boolean connect) throws ExecException {
        Process p;
        try {
            if (server.equals(LOCAL)) {
                p = Runtime.getRuntime().exec(cmdarray);
            } 
            else {
                SSHSocketImplFactory fac = SSHSocketImplFactory.getFactory(server);
                p = fac.ssh(cmdToString(cmdarray));
            }

            if (connect)
                hodProcess = p;

            //this should return as soon as connection is shutdown
            int rc = p.waitFor();
            if (rc != 0) {
                StringBuilder errMsg = new StringBuilder();
                try {
                    BufferedReader br = new BufferedReader(new InputStreamReader(p.getInputStream()));
                    String line = null;
                    while((line = br.readLine()) != null) {
                        errMsg.append(line);
                    }
                    br.close();
                    br = new BufferedReader(new InputStreamReader(p.getErrorStream()));
                    line = null;
                    while((line = br.readLine()) != null) {
                        errMsg.append(line);
                    }
                    br.close();
                } catch (IOException ioe) {}
                int errCode = 6011;
                StringBuilder msg = new StringBuilder("Failed to run command ");
                msg.append(cmdToString(cmdarray));
                msg.append(" on server ");
                msg.append(server);
                msg.append("; return code: ");
                msg.append(rc);
                msg.append("; error: ");
                msg.append(errMsg.toString());
                throw new ExecException(msg.toString(), errCode, PigException.REMOTE_ENVIRONMENT);
            }
        } catch (Exception e){
            if(e instanceof ExecException) throw (ExecException)e;
            int errCode = 6012;
            String msg = "Unable to run command: " + cmdToString(cmdarray) + " on server " + server;
            throw new ExecException(msg, errCode, PigException.REMOTE_ENVIRONMENT, e);
        }

        return p;
    }

    /*
    private FileSpec checkLeafIsStore(PhysicalPlan plan) throws ExecException {
        try {
            PhysicalOperator leaf = (PhysicalOperator)plan.getLeaves().get(0);
            FileSpec spec = null;
            if(!(leaf instanceof POStore)){
                String scope = leaf.getOperatorKey().getScope();
                POStore str = new POStore(new OperatorKey(scope,
                    NodeIdGenerator.getGenerator().getNextNodeId(scope)));
                str.setPc(pigContext);
                spec = new FileSpec(FileLocalizer.getTemporaryPath(null,
                    pigContext).toString(),
                    new FuncSpec(BinStorage.class.getName()));
                str.setSFile(spec);
                plan.addAsLeaf(str);
            } else{
                spec = ((POStore)leaf).getSFile();
            }
            return spec;
        } catch (Exception e) {
            throw new ExecException(e);
        }
    }
    */

    private void deleteDir(String server, String dir) {
        if (server.equals(LOCAL)){
            File path = new File(dir);
            deleteLocalDir(path);
        }
        else { 
            // send rm command over ssh
            String[] cmdarray = new String[3];
			cmdarray[0] = "rm";
            cmdarray[1] = "-rf";
            cmdarray[2] = dir;

            try{
                runCommand(server, cmdarray, false);
            }catch(Exception e){
                    log.warn("Failed to remove HOD configuration directory - " + dir);
            }
        }
    }

    private void deleteLocalDir(File path){
        File[] files = path.listFiles();
        int i;
        for (i = 0; i < files.length; i++){
            if (files[i].isHidden())
                continue;
            if (files[i].isFile())
                files[i].delete();
            else if (files[i].isDirectory())
                deleteLocalDir(files[i]);
        }

        path.delete();
    }

    private String fixUpDomain(String hostPort,Properties properties) throws UnknownHostException {
        URI uri = null;
        try {
            uri = new URI(hostPort);
        } catch (URISyntaxException use) {
            throw new RuntimeException("Illegal hostPort: " + hostPort);
        }
        
        String hostname = uri.getHost();
        int port = uri.getPort();
        
        // Parse manually if hostPort wasn't non-opaque URI
        // e.g. hostPort is "myhost:myport"
        if (hostname == null || port == -1) {
            String parts[] = hostPort.split(":");
            hostname = parts[0];
            port = Integer.valueOf(parts[1]);
        }
        
        if (hostname.indexOf('.') == -1) {
          //jz: fallback to systemproperty cause this not handled in Main 
            String domain = properties.getProperty("cluster.domain",System.getProperty("cluster.domain"));
            if (domain == null) 
                throw new RuntimeException("Missing cluster.domain property!");
            hostname = hostname + "." + domain;
        }
        InetAddress.getByName(hostname);
        return hostname + ":" + Integer.toString(port);
    }

    // create temp dir to store hod output; removed on exit
    // format: <tempdir>/PigHod.<host name>.<user name>.<nanosecondts>
    private String createTempDir(String server) throws ExecException {
        StringBuilder tempDirPrefix  = new StringBuilder ();
        
        if (server.equals(LOCAL))
            tempDirPrefix.append(System.getProperty("java.io.tmpdir"));
        else
            // for remote access we assume /tmp as temp dir
            tempDirPrefix.append("/tmp");

        tempDirPrefix.append("/PigHod.");
        try {
            tempDirPrefix.append(InetAddress.getLocalHost().getHostName());
            tempDirPrefix.append(".");
        } catch (UnknownHostException e) {}
            
        tempDirPrefix.append(System.getProperty("user.name"));
        tempDirPrefix.append(".");
        String path;
        do {
            path = tempDirPrefix.toString() + System.nanoTime();
        } while (!createDir(server, path));

        return path;
    }

    private boolean createDir(String server, String dir) throws ExecException{
        if (server.equals(LOCAL)){ 
            // create local directory
            File tempDir = new File(dir);
            boolean success = tempDir.mkdir();
            if (!success)
                log.warn("Failed to create HOD configuration directory - " + dir + ". Retrying ...");

            return success;
        }
        else {
            String[] cmdarray = new String[2];
			cmdarray[0] = "mkdir ";
            cmdarray[1] = dir;

            try{
                runCommand(server, cmdarray, false);
            }
            catch(ExecException e){
                    log.warn("Failed to create HOD configuration directory - " + dir + "Retrying...");
                    return false;
            }

            return true;
        }
    }

    // returns number of nodes based on -m option in hodParams if present;
    // otherwise, default is used; -m is removed from the params
    int getNumNodes(StringBuilder hodParams) {
        String val = hodParams.toString();
        int startPos = val.indexOf("-m ");
        if (startPos == -1)
            startPos = val.indexOf("-m\t");
        if (startPos != -1) {
            int curPos = startPos + 3;
            int len = val.length();
            while (curPos < len && Character.isWhitespace(val.charAt(curPos))) curPos ++;
            int numStartPos = curPos;
            while (curPos < len && Character.isDigit(val.charAt(curPos))) curPos ++;
            int nodes = Integer.parseInt(val.substring(numStartPos, curPos));
            hodParams.delete(startPos, curPos);
            return nodes;
        } else {
            return Integer.getInteger("hod.nodes", 15);
        }
    }
    
    /**
     * Method to recompute pig properties by overriding hadoop properties
     * with pig properties
     * @param conf JobConf with appropriate hadoop resource files
     * @param properties Pig properties that will override hadoop properties; properties might be modified
     */
    private void recomputeProperties(JobConf jobConf, Properties properties) {
        // We need to load the properties from the hadoop configuration
        // We want to override these with any existing properties we have.
        if (jobConf != null && properties != null) {
            Properties hadoopProperties = new Properties();
            Iterator<Map.Entry<String, String>> iter = jobConf.iterator();
            while (iter.hasNext()) {
                Map.Entry<String, String> entry = iter.next();
                hadoopProperties.put(entry.getKey(), entry.getValue());
            }

            //override hadoop properties with user defined properties
            Enumeration<Object> propertiesIter = properties.keys();
            while (propertiesIter.hasMoreElements()) {
                String key = (String) propertiesIter.nextElement();
                String val = properties.getProperty(key);
                hadoopProperties.put(key, val);
            }
            
            //clear user defined properties and re-populate
            properties.clear();
            Enumeration<Object> hodPropertiesIter = hadoopProperties.keys();
            while (hodPropertiesIter.hasMoreElements()) {
                String key = (String) hodPropertiesIter.nextElement();
                String val = hadoopProperties.getProperty(key);
                properties.put(key, val);
            }

        }
    }
    
}





/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;

import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.impl.plan.DependencyOrderWalker;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.pen.util.ExampleTuple;

public class POForEach extends PhysicalOperator {

    /**
     * 
     */
    private static final long serialVersionUID = 1L;
    
    protected List<PhysicalPlan> inputPlans;
    protected List<PhysicalOperator> opsToBeReset;
    protected Log log = LogFactory.getLog(getClass());
    protected static TupleFactory mTupleFactory = TupleFactory.getInstance();
    //Since the plan has a generate, this needs to be maintained
    //as the generate can potentially return multiple tuples for
    //same call.
    protected boolean processingPlan = false;
    
    //its holds the iterators of the databags given by the input expressions which need flattening.
    protected Iterator<Tuple> [] its = null;
    
    //This holds the outputs given out by the input expressions of any datatype
    protected Object [] bags = null;
    
    //This is the template whcih contains tuples and is flattened out in CreateTuple() to generate the final output
    protected Object[] data = null;
    
    // store result types of the plan leaves
    protected byte[] resultTypes = null;
    
    // array version of isToBeFlattened - this is purely
    // for optimization - instead of calling isToBeFlattened.get(i)
    // we can do the quicker array access - isToBeFlattenedArray[i].
    // Also we can store "boolean" values rather than "Boolean" objects
    // so we can also save on the Boolean.booleanValue() calls
    protected boolean[] isToBeFlattenedArray;
    
    ExampleTuple tIn = null;
    protected int noItems;

    protected PhysicalOperator[] planLeafOps = null;
    
    public POForEach(OperatorKey k) {
        this(k,-1,null,null);
    }

    public POForEach(OperatorKey k, int rp, List inp) {
        this(k,rp,inp,null);
    }

    public POForEach(OperatorKey k, int rp) {
        this(k,rp,null,null);
    }

    public POForEach(OperatorKey k, List inp) {
        this(k,-1,inp,null);
    }
    
    public POForEach(OperatorKey k, int rp, List<PhysicalPlan> inp, List<Boolean>  isToBeFlattened){
        super(k, rp);
        setUpFlattens(isToBeFlattened);
        this.inputPlans = inp;
        opsToBeReset = new ArrayList<PhysicalOperator>();
        getLeaves();
    }

    @Override
    public void visit(PhyPlanVisitor v) throws VisitorException {
        v.visitPOForEach(this);
    }

    @Override
    public String name() {
        String fString = getFlatStr();
        return "New For Each" + "(" + fString + ")" + "[" + DataType.findTypeName(resultType) + "]" +" - " + mKey.toString();
    }
    
    String getFlatStr() {
        if(isToBeFlattenedArray ==null)
            return "";
        StringBuilder sb = new StringBuilder();
        for (Boolean b : isToBeFlattenedArray) {
            sb.append(b);
            sb.append(',');
        }
        if(sb.length()>0){
            sb.deleteCharAt(sb.length()-1);
        }
        return sb.toString();
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    @Override
    public boolean supportsMultipleOutputs() {
        return false;
    }
    
    /**
     * Calls getNext on the generate operator inside the nested
     * physical plan and returns it maintaining an additional state
     * to denote the begin and end of the nested plan processing.
     */
    @Override
    public Result getNext(Tuple t) throws ExecException {
        Result res = null;
        Result inp = null;
        //The nested plan is under processing
        //So return tuples that the generate oper
        //returns
        if(processingPlan){
            while(true) {
                res = processPlan();
                if(res.returnStatus==POStatus.STATUS_OK) {
                    if(lineageTracer !=  null && res.result != null) {
                	ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
                	tOut.synthetic = tIn.synthetic;
                	lineageTracer.insert(tOut);
                	lineageTracer.union(tOut, tIn);
                	res.result = tOut;
                    }
                    return res;
                }
                if(res.returnStatus==POStatus.STATUS_EOP) {
                    processingPlan = false;
                    break;
                }
                if(res.returnStatus==POStatus.STATUS_ERR) {
                    return res;
                }
                if(res.returnStatus==POStatus.STATUS_NULL) {
                    continue;
                }
            }
        }
        //The nested plan processing is done or is
        //yet to begin. So process the input and start
        //nested plan processing on the input tuple
        //read
        while (true) {
            inp = processInput();
            if (inp.returnStatus == POStatus.STATUS_EOP ||
                    inp.returnStatus == POStatus.STATUS_ERR) {
                return inp;
            }
            if (inp.returnStatus == POStatus.STATUS_NULL) {
                continue;
            }
            
            attachInputToPlans((Tuple) inp.result);
            for (PhysicalOperator po : opsToBeReset) {
                po.reset();
            }
            res = processPlan();
            
            processingPlan = true;

            if(lineageTracer != null && res.result != null) {
        	//we check for res.result since that can also be null in the case of flatten
        	tIn = (ExampleTuple) inp.result;
        	ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
        	tOut.synthetic = tIn.synthetic;
        	lineageTracer.insert(tOut);
        	lineageTracer.union(tOut, tIn);
        	res.result = tOut;
            }
            
            return res;
        }
    }

    protected Result processPlan() throws ExecException{
        Result res = new Result();
        
        //We check if all the databags have exhausted the tuples. If so we enforce the reading of new data by setting data and its to null
        if(its != null) {
            boolean restartIts = true;
            for(int i = 0; i < noItems; ++i) {
                if(its[i] != null && isToBeFlattenedArray[i] == true)
                    restartIts &= !its[i].hasNext();
            }
            //this means that all the databags have reached their last elements. so we need to force reading of fresh databags
            if(restartIts) {
                its = null;
                data = null;
            }
        }
        
        if(its == null) {
            //getNext being called for the first time OR starting with a set of new data from inputs 
            its = new Iterator[noItems];
            bags = new Object[noItems];
            
            for(int i = 0; i < noItems; ++i) {
                //Getting the iterators
                //populate the input data
                Result inputData = null;
                switch(resultTypes[i]) {
                case DataType.BAG:
                    inputData = planLeafOps[i].getNext(dummyBag);
                    break;

                case DataType.TUPLE :
                inputData = planLeafOps[i].getNext(dummyTuple);
                break;
                case DataType.BYTEARRAY :
                inputData = planLeafOps[i].getNext(dummyDBA);
                break; 
                case DataType.MAP :
                inputData = planLeafOps[i].getNext(dummyMap);
                break;
                case DataType.BOOLEAN :
                inputData = planLeafOps[i].getNext(dummyBool);
                break;
                case DataType.INTEGER :
                inputData = planLeafOps[i].getNext(dummyInt);
                break;
                case DataType.DOUBLE :
                inputData = planLeafOps[i].getNext(dummyDouble);
                break;
                case DataType.LONG :
                inputData = planLeafOps[i].getNext(dummyLong);
                break;
                case DataType.FLOAT :
                inputData = planLeafOps[i].getNext(dummyFloat);
                break;
                case DataType.CHARARRAY :
                inputData = planLeafOps[i].getNext(dummyString);
                break;

                default: {
                    int errCode = 2080;
                    String msg = "Foreach currently does not handle type " + DataType.findTypeName(resultTypes[i]);
                    throw new ExecException(msg, errCode, PigException.BUG);
                }
                
                }
                
                if(inputData.returnStatus == POStatus.STATUS_EOP) {
                    //we are done with all the elements. Time to return.
                    its = null;
                    bags = null;
                    return inputData;
                }
                // if we see a error just return it
                if(inputData.returnStatus == POStatus.STATUS_ERR) {
                    return inputData;
                }

//                Object input = null;
                
                bags[i] = inputData.result;
                
                if(inputData.result instanceof DataBag && isToBeFlattenedArray[i]) 
                    its[i] = ((DataBag)bags[i]).iterator();
                else 
                    its[i] = null;
            }
        }

        
        while(true) {
            if(data == null) {
                //getNext being called for the first time or starting on new input data
                //we instantiate the template array and start populating it with data
                data = new Object[noItems];
                for(int i = 0; i < noItems; ++i) {
                    if(isToBeFlattenedArray[i] && bags[i] instanceof DataBag) {
                        if(its[i].hasNext()) {
                            data[i] = its[i].next();
                        } else {
                            //the input set is null, so we return.  This is
                            // caught above and this function recalled with
                            // new inputs.
                            its = null;
                            data = null;
                            res.returnStatus = POStatus.STATUS_NULL;
                            return res;
                        }
                    } else {
                        data[i] = bags[i];
                    }
                    
                }
                if(reporter!=null) reporter.progress();
                //CreateTuple(data);
                res.result = CreateTuple(data);
                res.returnStatus = POStatus.STATUS_OK;
                return res;
            } else {
                //we try to find the last expression which needs flattening and start iterating over it
                //we also try to update the template array
                for(int index = noItems - 1; index >= 0; --index) {
                    if(its[index] != null && isToBeFlattenedArray[index]) {
                        if(its[index].hasNext()) {
                            data[index] =  its[index].next();
                            res.result = CreateTuple(data);
                            res.returnStatus = POStatus.STATUS_OK;
                            return res;
                        }
                        else{
                            // reset this index's iterator so cross product can be achieved
                            // we would be resetting this way only for the indexes from the end
                            // when the first index which needs to be flattened has reached the
                            // last element in its iterator, we won't come here - instead, we reset
                            // all iterators at the beginning of this method.
                            its[index] = ((DataBag)bags[index]).iterator();
                            data[index] = its[index].next();
                        }
                    }
                }
            }
        }
        
        //return null;
    }
    
    /**
     * 
     * @param data array that is the template for the final flattened tuple
     * @return the final flattened tuple
     */
    protected Tuple CreateTuple(Object[] data) throws ExecException {
        Tuple out =  mTupleFactory.newTuple();
        for(int i = 0; i < data.length; ++i) {
            Object in = data[i];
            
            if(isToBeFlattenedArray[i] && in instanceof Tuple) {
                Tuple t = (Tuple)in;
                int size = t.size();
                for(int j = 0; j < size; ++j) {
                    out.append(t.get(j));
                }
            } else
                out.append(in);
        }
        
        if(lineageTracer != null) {
            ExampleTuple tOut = new ExampleTuple();
            tOut.reference(out);
        }
        return out;
    }

    
    protected void attachInputToPlans(Tuple t) {
        //super.attachInput(t);
        for(PhysicalPlan p : inputPlans) {
            p.attachInput(t);
        }
    }
    
    protected void getLeaves() {
        if (inputPlans != null) {
            int i=-1;
            if(isToBeFlattenedArray == null) {
                isToBeFlattenedArray = new boolean[inputPlans.size()];
            }
            planLeafOps = new PhysicalOperator[inputPlans.size()];
            for(PhysicalPlan p : inputPlans) {
                ++i;
                PhysicalOperator leaf = (PhysicalOperator)p.getLeaves().get(0); 
                planLeafOps[i] = leaf;
                if(leaf instanceof POProject &&
                        leaf.getResultType() == DataType.TUPLE &&
                        ((POProject)leaf).isStar())
                    isToBeFlattenedArray[i] = true;
            }
        }
        // we are calculating plan leaves
        // so lets reinitialize
        reInitialize();
    }
    
    private void reInitialize() {
        if(planLeafOps != null) {
            noItems = planLeafOps.length;
            resultTypes = new byte[noItems];
            for (int i = 0; i < resultTypes.length; i++) {
                resultTypes[i] = planLeafOps[i].getResultType();
            }
        } else {
            noItems = 0;
            resultTypes = null;
        }
        
        if(inputPlans != null) {
            for (PhysicalPlan pp : inputPlans) {
                try {
                    ResetFinder lf = new ResetFinder(pp, opsToBeReset);
                    lf.visit();
                } catch (VisitorException ve) {
                    String errMsg = "Internal Error:  Unexpected error looking for nested operators which need to be reset in FOREACH";
                    throw new RuntimeException(errMsg, ve);
                }
            }
        }
    }
    
    public List<PhysicalPlan> getInputPlans() {
        return inputPlans;
    }

    public void setInputPlans(List<PhysicalPlan> plans) {
        inputPlans = plans;
        planLeafOps = null;
        getLeaves();
    }

    public void addInputPlan(PhysicalPlan plan, boolean flatten) {
        inputPlans.add(plan);
        // add to planLeafOps
        // copy existing leaves
        PhysicalOperator[] newPlanLeafOps = new PhysicalOperator[planLeafOps.length + 1];
        for (int i = 0; i < planLeafOps.length; i++) {
            newPlanLeafOps[i] = planLeafOps[i];
        }
        // add to the end
        newPlanLeafOps[planLeafOps.length] = plan.getLeaves().get(0); 
        planLeafOps = newPlanLeafOps;
        
        // add to isToBeFlattenedArray
        // copy existing values
        boolean[] newIsToBeFlattenedArray = new boolean[isToBeFlattenedArray.length + 1];
        for(int i = 0; i < isToBeFlattenedArray.length; i++) {
            newIsToBeFlattenedArray[i] = isToBeFlattenedArray[i];
        }
        // add to end
        newIsToBeFlattenedArray[isToBeFlattenedArray.length] = flatten;
        isToBeFlattenedArray = newIsToBeFlattenedArray;
        
        // we just added a leaf - reinitialize
        reInitialize();
    }

    public void setToBeFlattened(List<Boolean> flattens) {
        setUpFlattens(flattens);
    }

    public List<Boolean> getToBeFlattened() {
        List<Boolean> result = null;
        if(isToBeFlattenedArray != null) {
            result = new ArrayList<Boolean>();
            for (int i = 0; i < isToBeFlattenedArray.length; i++) {
                result.add(isToBeFlattenedArray[i]);
            }
        }
        return result;
    }

    /**
     * Make a deep copy of this operator.  
     * @throws CloneNotSupportedException
     */
    @Override
    public POForEach clone() throws CloneNotSupportedException {
        List<PhysicalPlan> plans = new
            ArrayList<PhysicalPlan>(inputPlans.size());
        for (PhysicalPlan plan : inputPlans) {
            plans.add(plan.clone());
        }
        List<Boolean> flattens = null;
        if(isToBeFlattenedArray != null) {
            flattens = new
                ArrayList<Boolean>(isToBeFlattenedArray.length);
            for (boolean b : isToBeFlattenedArray) {
                flattens.add(b);
            }
        }
        
        List<PhysicalOperator> ops = new ArrayList<PhysicalOperator>(opsToBeReset.size());
        for (PhysicalOperator op : opsToBeReset) {
            ops.add(op);
        }
        POForEach clone = new POForEach(new OperatorKey(mKey.scope, 
                NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
                requestedParallelism, plans, flattens);
        clone.setOpsToBeReset(ops);
        clone.setResultType(getResultType());
        return clone;
    }

    public boolean inProcessing()
    {
        return processingPlan;
    }
    
    protected void setUpFlattens(List<Boolean> isToBeFlattened) {
        if(isToBeFlattened == null) {
            isToBeFlattenedArray = null;
        } else {
            isToBeFlattenedArray = new boolean[isToBeFlattened.size()];
            int i = 0;
            for (Iterator<Boolean> it = isToBeFlattened.iterator(); it.hasNext();) {
                isToBeFlattenedArray[i++] = it.next();
            }
        }
    }

    /**
     * Visits a pipeline and calls reset on all the nodes.  Currently only
     * pays attention to limit nodes, each of which need to be told to reset
     * their limit.
     */
    private class ResetFinder extends PhyPlanVisitor {

        ResetFinder(PhysicalPlan plan, List<PhysicalOperator> toBeReset) {
            super(plan,
                new DependencyOrderWalker<PhysicalOperator, PhysicalPlan>(plan));
        }

        @Override
        public void visitDistinct(PODistinct d) throws VisitorException {
            // FIXME: add only if limit is present
            opsToBeReset.add(d);
        }

        @Override
        public void visitLimit(POLimit limit) throws VisitorException {
            opsToBeReset.add(limit);
        }

        @Override
        public void visitSort(POSort sort) throws VisitorException {
            // FIXME: add only if limit is present
            opsToBeReset.add(sort);
        }
        
        /* (non-Javadoc)
         * @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor#visitProject(org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject)
         */
        @Override
        public void visitProject(POProject proj) throws VisitorException {
            if(proj instanceof PORelationToExprProject) {
                opsToBeReset.add(proj);
            }
        }
    }

    /**
     * @return the opsToBeReset
     */
    public List<PhysicalOperator> getOpsToBeReset() {
        return opsToBeReset;
    }

    /**
     * @param opsToBeReset the opsToBeReset to set
     */
    public void setOpsToBeReset(List<PhysicalOperator> opsToBeReset) {
        this.opsToBeReset = opsToBeReset;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.data;

import java.io.BufferedOutputStream;
import java.io.DataOutputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Collection;
import java.util.Iterator;
import java.util.ArrayList;

import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PigLogger;
import org.apache.pig.impl.util.Spillable;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

/**
 * Default implementation of DataBag.  This is the an abstract class used as a
 * parent for all three of the types of data bags.
 */
public abstract class DefaultAbstractBag implements DataBag {

     private static final Log log = LogFactory.getLog(DataBag.class);
     
     private static PigLogger pigLogger = PhysicalOperator.getPigLogger();

    // Container that holds the tuples. Actual object instantiated by
    // subclasses.
    protected Collection<Tuple> mContents;

    // Spill files we've created.  These need to be removed in finalize.
    protected ArrayList<File> mSpillFiles;

    // Total size, including tuples on disk.  Stored here so we don't have
    // to run through the disk when people ask.
    protected long mSize = 0;

    protected boolean mMemSizeChanged = false;

    protected long mMemSize = 0;

    /**
     * Get the number of elements in the bag, both in memory and on disk.
     */
    public long size() {
        return mSize;
    }

    /**
     * Add a tuple to the bag.
     * @param t tuple to add.
     */
    public void add(Tuple t) {
        synchronized (mContents) {
            mMemSizeChanged = true;
            mSize++;
            mContents.add(t);
        }
    }

    /**
     * Add contents of a bag to the bag.
     * @param b bag to add contents of.
     */
    public void addAll(DataBag b) {
        synchronized (mContents) {
            mMemSizeChanged = true;
            mSize += b.size();
            Iterator<Tuple> i = b.iterator();
            while (i.hasNext()) mContents.add(i.next());
        }
    }

    /**
     * Add contents of a container to the bag.
     * @param c Collection to add contents of.
     */
    public void addAll(Collection<Tuple> c) {
        synchronized (mContents) {
            mMemSizeChanged = true;
            mSize += c.size();
            Iterator<Tuple> i = c.iterator();
            while (i.hasNext()) mContents.add(i.next());
        }
    }

    /**
     * Return the size of memory usage.
     */
    public long getMemorySize() {
        if (!mMemSizeChanged) return mMemSize;

        long used = 0;
        // I can't afford to talk through all the tuples every time the
        // memory manager wants to know if it's time to dump.  Just sample
        // the first 100 and see what we get.  This may not be 100%
        // accurate, but it's just an estimate anyway.
        int j;
        int numInMem = 0;
        synchronized (mContents) {
            numInMem = mContents.size();
            // Measure only what's in memory, not what's on disk.
            Iterator<Tuple> i = mContents.iterator();
            for (j = 0; i.hasNext() && j < 100; j++) { 
                used += i.next().getMemorySize();
            }
        }

        if (numInMem > 100) {
            // Estimate the per tuple size.  Do it in integer arithmetic
            // (even though it will be slightly less accurate) for speed.
            used /= j;
            used *= numInMem;
        }

        mMemSize = used;
        mMemSizeChanged = false;
        return used;
    }

    /**
     * Clear out the contents of the bag, both on disk and in memory.
     * Any attempts to read after this is called will produce undefined
     * results.
     */
    public void clear() {
        synchronized (mContents) {
            mContents.clear();
            if (mSpillFiles != null) {
                for (int i = 0; i < mSpillFiles.size(); i++) {
                    mSpillFiles.get(i).delete();
                }
                mSpillFiles.clear();
            }
            mSize = 0;
        }
    }

    /**
     * This method is potentially very expensive since it may require a
     * sort of the bag; don't call it unless you have to.
     */
    public int compareTo(Object other) {
        if (this == other)
            return 0;
        if (other instanceof DataBag) {
            DataBag bOther = (DataBag) other;
            if (this.size() != bOther.size()) {
                if (this.size() > bOther.size()) return 1;
                else return -1;
            }

            // Ugh, this is bogus.  But I have to know if two bags have the
            // same tuples, regardless of order.  Hopefully most of the
            // time the size check above will prevent this.
            // If either bag isn't already sorted, create a sorted bag out
            // of it so I can guarantee order.
            DataBag thisClone;
            DataBag otherClone;
            if (this instanceof SortedDataBag ||
                    this instanceof DistinctDataBag) {
                thisClone = this;
            } else {
                thisClone = new SortedDataBag(null);
                Iterator<Tuple> i = iterator();
                while (i.hasNext()) thisClone.add(i.next());
            }
            if (other instanceof SortedDataBag ||
                    other instanceof DistinctDataBag) {
                otherClone = bOther;
            } else {
                otherClone = new SortedDataBag(null);
                Iterator<Tuple> i = bOther.iterator();
                while (i.hasNext()) otherClone.add(i.next());
            }
            Iterator<Tuple> thisIt = thisClone.iterator();
            Iterator<Tuple> otherIt = otherClone.iterator();
            while (thisIt.hasNext() && otherIt.hasNext()) {
                Tuple thisT = thisIt.next();
                Tuple otherT = otherIt.next();
                
                int c = thisT.compareTo(otherT);
                if (c != 0) return c;
            }
            
            return 0;   // if we got this far, they must be equal
        } else {
            return DataType.compare(this, other);
        }
    }

    @Override
    public boolean equals(Object other) {
        return compareTo(other) == 0;
    }

    /**
     * Write a bag's contents to disk.
     * @param out DataOutput to write data to.
     * @throws IOException (passes it on from underlying calls).
     */
    public void write(DataOutput out) throws IOException {
        // We don't care whether this bag was sorted or distinct because
        // using the iterator to write it will guarantee those things come
        // correctly.  And on the other end there'll be no reason to waste
        // time re-sorting or re-applying distinct.
        out.writeLong(size());
        Iterator<Tuple> it = iterator();
        while (it.hasNext()) {
            Tuple item = it.next();
            item.write(out);
        }    
    }
 
    /**
     * Read a bag from disk.
     * @param in DataInput to read data from.
     * @throws IOException (passes it on from underlying calls).
     */
    public void readFields(DataInput in) throws IOException {
        long size = in.readLong();
        
        for (long i = 0; i < size; i++) {
            try {
                Object o = DataReaderWriter.readDatum(in);
                add((Tuple)o);
            } catch (ExecException ee) {
                throw ee;
            }
        }
    }

    /**
     * This is used by FuncEvalSpec.FakeDataBag.
     * @param stale Set stale state.
     */
    public void markStale(boolean stale)
    {
    }

    /**
     * Write the bag into a string. */
    @Override
    public String toString() {
        StringBuffer sb = new StringBuffer();
        sb.append('{');
        Iterator<Tuple> it = iterator();
        while ( it.hasNext() ) {
            Tuple t = it.next();
            String s = t.toString();
            sb.append(s);
            if (it.hasNext()) sb.append(",");
        }
        sb.append('}');
        return sb.toString();
    }

    @Override
    public int hashCode() {
        int hash = 1;
        Iterator<Tuple> i = iterator();
        while (i.hasNext()) {
            // Use 37 because we want a prime, and tuple uses 31.
            hash = 37 * hash + i.next().hashCode();
        }
        return hash;
    }

    /**
     * Need to override finalize to clean out the mSpillFiles array.
     */
    @Override
    protected void finalize() {
        if (mSpillFiles != null) {
            for (int i = 0; i < mSpillFiles.size(); i++) {
                mSpillFiles.get(i).delete();
            }
        }
    }

    /**
     * Get a file to spill contents to.  The file will be registered in the
     * mSpillFiles array.
     * @return stream to write tuples to.
     */
    protected DataOutputStream getSpillFile() throws IOException {
        if (mSpillFiles == null) {
            // We want to keep the list as small as possible.
            mSpillFiles = new ArrayList<File>(1);
        }

        String tmpDirName= System.getProperties().getProperty("java.io.tmpdir") ;                
        File tmpDir = new File(tmpDirName);
  
        // if the directory does not exist, create it.
        if (!tmpDir.exists()){
            log.info("Temporary directory doesn't exists. Trying to create: " + tmpDir.getAbsolutePath());
          // Create the directory and see if it was successful
          if (tmpDir.mkdir()){
            log.info("Successfully created temporary directory: " + tmpDir.getAbsolutePath());
          } else {
              // If execution reaches here, it means that we needed to create the directory but
              // were not successful in doing so.
              // 
              // If this directory is created recently then we can simply 
              // skip creation. This is to address a rare issue occuring in a cluster despite the
              // the fact that spill() makes call to getSpillFile() in a synchronized 
              // block. 
              if (tmpDir.exists()) {
                log.info("Temporary directory already exists: " + tmpDir.getAbsolutePath());
              } else {
                int errCode = 2111;
                String msg = "Unable to create temporary directory: " + tmpDir.getAbsolutePath();
                throw new ExecException(msg, errCode, PigException.BUG);                  
              }
          }
        }
        
        File f = File.createTempFile("pigbag", null);
        f.deleteOnExit();
        mSpillFiles.add(f);
        return new DataOutputStream(new BufferedOutputStream(
            new FileOutputStream(f)));
    }

    /**
     * Report progress to HDFS.
     */
    protected void reportProgress() {
        if (PhysicalOperator.reporter != null) {
            PhysicalOperator.reporter.progress();
        }
    }

    protected void warn(String msg, Enum warningEnum, Exception e) {
    	pigLogger = PhysicalOperator.getPigLogger();
    	if(pigLogger != null) {
    		pigLogger.warn(this, msg, warningEnum);
    	} else {
    		log.warn(msg, e);
    	}    	
    }

    public static abstract class BagDelimiterTuple extends DefaultTuple{}
    public static class StartBag extends BagDelimiterTuple{}
    
    public static class EndBag extends BagDelimiterTuple{}
    
    public static final Tuple startBag = new StartBag();
    public static final Tuple endBag = new EndBag();

    protected static final int MAX_SPILL_FILES = 100;
 
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners;


import java.io.InputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.hadoop.io.RawComparator;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.Partitioner;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.HDataType;
import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
import org.apache.pig.builtin.BinStorage;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.builtin.FindQuantiles;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.NullableBytesWritable;
import org.apache.pig.impl.io.NullableDoubleWritable;
import org.apache.pig.impl.io.NullableFloatWritable;
import org.apache.pig.impl.io.NullableIntWritable;
import org.apache.pig.impl.io.NullableLongWritable;
import org.apache.pig.impl.io.NullableText;
import org.apache.pig.impl.io.NullableTuple;
import org.apache.pig.impl.io.PigNullableWritable;

public class WeightedRangePartitioner implements Partitioner<PigNullableWritable, Writable> {
    PigNullableWritable[] quantiles;
    RawComparator<PigNullableWritable> comparator;
    Integer numQuantiles;
    DataBag samples;
    public static Map<PigNullableWritable,DiscreteProbabilitySampleGenerator> weightedParts = new HashMap<PigNullableWritable, DiscreteProbabilitySampleGenerator>();
    JobConf job;

    public int getPartition(PigNullableWritable key, Writable value,
            int numPartitions){
        if(!weightedParts.containsKey(key)){
            int index = Arrays.binarySearch(quantiles, key, comparator);
            if (index < 0)
                index = -index-1;
            else
                index = index + 1;
            return Math.min(index, numPartitions - 1);
        }
        DiscreteProbabilitySampleGenerator gen = weightedParts.get(key);
        return gen.getNext();
    }

    @SuppressWarnings("unchecked")
    public void configure(JobConf job) {
        this.job = job;
        String quantilesFile = job.get("pig.quantilesFile", "");
        comparator = job.getOutputKeyComparator();
        if (quantilesFile.length() == 0)
            throw new RuntimeException(this.getClass().getSimpleName() + " used but no quantiles found");
        
        try{
            InputStream is = FileLocalizer.openDFSFile(quantilesFile,ConfigurationUtil.toProperties(job));
            BinStorage loader = new BinStorage();
            DataBag quantilesList;
            loader.bindTo(quantilesFile, new BufferedPositionedInputStream(is), 0, Long.MAX_VALUE);
            Tuple t = loader.getNext();
            if(t==null) throw new RuntimeException("Empty samples file");
            // the Quantiles file has a tuple as under:
            // (numQuantiles, bag of samples) 
            // numQuantiles here is the reduce parallelism
            Map<String, Object> quantileMap = (Map<String, Object>) t.get(0);
            quantilesList = (DataBag) quantileMap.get(FindQuantiles.QUANTILES_LIST);
            Map<Tuple, Tuple> weightedPartsData = (Map<Tuple, Tuple>) quantileMap.get(FindQuantiles.WEIGHTED_PARTS);
            convertToArray(quantilesList);
            for(Entry<Tuple, Tuple> ent : weightedPartsData.entrySet()){
                Tuple key = ent.getKey(); // sample item which repeats
                float[] probVec = getProbVec(ent.getValue());
                weightedParts.put(getPigNullableWritable(key), 
                        new DiscreteProbabilitySampleGenerator(probVec));
            }
        }catch (Exception e){
            throw new RuntimeException(e);
        }
    }

    /**
     * @param value
     * @return
     * @throws ExecException 
     */
    private float[] getProbVec(Tuple values) throws ExecException {
        float[] probVec = new float[values.size()];
        for(int i = 0; i < values.size(); i++) {
            probVec[i] = (Float)values.get(i);
        }
        return probVec;
    }

    private PigNullableWritable getPigNullableWritable(Tuple t) {
        try {
            // user comparators work with tuples - so if user comparator
            // is being used OR if there are more than 1 sort cols, use
            // NullableTuple
            if ("true".equals(job.get("pig.usercomparator")) || t.size() > 1) {
                return new NullableTuple(t);
            } else {
                Object o = t.get(0);
                String kts = job.get("pig.reduce.key.type");
                if (kts == null) {
                    throw new RuntimeException("Didn't get reduce key type "
                        + "from config file.");
                }
                return HDataType.getWritableComparableTypes(o,
                    Byte.valueOf(kts));
            }
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    private boolean areEqual(PigNullableWritable sample, PigNullableWritable writable) {
        return comparator.compare(sample, writable)==0;
    }

    private void convertToArray(
            DataBag quantilesListAsBag) {
        ArrayList<PigNullableWritable> quantilesList = getList(quantilesListAsBag);
        if ("true".equals(job.get("pig.usercomparator")) ||
                quantilesList.get(0).getClass().equals(NullableTuple.class)) {
            quantiles = quantilesList.toArray(new NullableTuple[0]);
        } else if (quantilesList.get(0).getClass().equals(NullableBytesWritable.class)) {
            quantiles = quantilesList.toArray(new NullableBytesWritable[0]);
        } else if (quantilesList.get(0).getClass().equals(NullableDoubleWritable.class)) {
            quantiles = quantilesList.toArray(new NullableDoubleWritable[0]);
        } else if (quantilesList.get(0).getClass().equals(NullableFloatWritable.class)) {
            quantiles = quantilesList.toArray(new NullableFloatWritable[0]);
        } else if (quantilesList.get(0).getClass().equals(NullableIntWritable.class)) {
            quantiles = quantilesList.toArray(new NullableIntWritable[0]);
        } else if (quantilesList.get(0).getClass().equals(NullableLongWritable.class)) {
            quantiles = quantilesList.toArray(new NullableLongWritable[0]);
        } else if (quantilesList.get(0).getClass().equals(NullableText.class)) {
            quantiles = quantilesList.toArray(new NullableText[0]);
        } else {
            throw new RuntimeException("Unexpected class in " + this.getClass().getSimpleName());
        }
    }

    /**
     * @param quantilesListAsBag
     * @return
     */
    private ArrayList<PigNullableWritable> getList(DataBag quantilesListAsBag) {
        
        ArrayList<PigNullableWritable> list = new ArrayList<PigNullableWritable>();
        for (Tuple tuple : quantilesListAsBag) {
            list.add(getPigNullableWritable(tuple));
        }
        return list;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.tools.pigstats;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.hadoop.mapred.Counters;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.jobcontrol.Job;
import org.apache.hadoop.mapred.jobcontrol.JobControl;
import org.apache.pig.ExecType;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
import org.apache.pig.backend.local.executionengine.physicalLayer.counters.POCounter;
import org.apache.pig.impl.util.ObjectSerializer;

public class PigStats {
    MROperPlan mrp;
    PhysicalPlan php;
    JobControl jc;
    JobClient jobClient;
    Map<String, Map<String, String>> stats = new HashMap<String, Map<String,String>>();
    // String lastJobID;
    ArrayList<String> rootJobIDs = new ArrayList<String>();
    ExecType mode;
    
    public void setMROperatorPlan(MROperPlan mrp) {
        this.mrp = mrp;
    }
    
    public void setJobControl(JobControl jc) {
        this.jc = jc;
    }
    
    public void setJobClient(JobClient jobClient) {
        this.jobClient = jobClient;
    }
    
    public String getMRPlan() {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        mrp.dump(new PrintStream(baos));
        return baos.toString();
    }
    
    public void setExecType(ExecType mode) {
        this.mode = mode;
    }
    
    public void setPhysicalPlan(PhysicalPlan php) {
        this.php = php;
    }
    
    public String getPhysicalPlan() {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        php.explain(baos);
        return baos.toString();
    }
    
    public Map<String, Map<String, String>> accumulateStats() throws ExecException {
        if(mode == ExecType.MAPREDUCE)
            return accumulateMRStats();
        else if(mode == ExecType.LOCAL)
            return accumulateLocalStats();
        else
            throw new RuntimeException("Unrecognized mode. Either MapReduce or Local mode expected.");
    }
    
    private Map<String, Map<String, String>> accumulateLocalStats() {
        //The counter placed before a store in the local plan should be able to get the number of records
        for(PhysicalOperator op : php.getLeaves()) {
            Map<String, String> jobStats = new HashMap<String, String>();
            stats.put(op.toString(), jobStats);
            POCounter counter = (POCounter) php.getPredecessors(op).get(0);
            jobStats.put("PIG_STATS_LOCAL_OUTPUT_RECORDS", (Long.valueOf(counter.getCount())).toString());
            jobStats.put("PIG_STATS_LOCAL_BYTES_WRITTEN", (Long.valueOf((new File(((POStore)op).getSFile().getFileName())).length())).toString());
        }
        return stats;
    }
    
    private Map<String, Map<String, String>> accumulateMRStats() throws ExecException {
        
        for(Job job : jc.getSuccessfulJobs()) {
            
            
            JobConf jobConf = job.getJobConf();
            
            
                RunningJob rj = null;
                try {
                    rj = jobClient.getJob(job.getAssignedJobID());
                } catch (IOException e1) {
                    String error = "Unable to get the job statistics from JobClient.";
                    throw new ExecException(error, e1);
                }
                if(rj == null)
                    continue;
                
                Map<String, String> jobStats = new HashMap<String, String>();
                stats.put(job.getAssignedJobID().toString(), jobStats);
                
                try {
                    PhysicalPlan plan = (PhysicalPlan) ObjectSerializer.deserialize(jobConf.get("pig.mapPlan"));
                    jobStats.put("PIG_STATS_MAP_PLAN", plan.toString());
                    plan = (PhysicalPlan) ObjectSerializer.deserialize(jobConf.get("pig.combinePlan"));
                    if(plan != null) {
                        jobStats.put("PIG_STATS_COMBINE_PLAN", plan.toString());
                    }
                    plan = (PhysicalPlan) ObjectSerializer.deserialize(jobConf.get("pig.reducePlan"));
                    if(plan != null) {
                        jobStats.put("PIG_STATS_REDUCE_PLAN", plan.toString());
                    }
                } catch (IOException e2) {
                    String error = "Error deserializing plans from the JobConf.";
                    throw new RuntimeException(error, e2);
                }
                
                Counters counters = null;
                try {
                    counters = rj.getCounters();
                    Counters.Group taskgroup = counters.getGroup("org.apache.hadoop.mapred.Task$Counter");
                    Counters.Group hdfsgroup = counters.getGroup("org.apache.hadoop.mapred.Task$FileSystemCounter");

                    jobStats.put("PIG_STATS_MAP_INPUT_RECORDS", (Long.valueOf(taskgroup.getCounterForName("MAP_INPUT_RECORDS").getCounter())).toString());
                    jobStats.put("PIG_STATS_MAP_OUTPUT_RECORDS", (Long.valueOf(taskgroup.getCounterForName("MAP_OUTPUT_RECORDS").getCounter())).toString());
                    jobStats.put("PIG_STATS_REDUCE_INPUT_RECORDS", (Long.valueOf(taskgroup.getCounterForName("REDUCE_INPUT_RECORDS").getCounter())).toString());
                    jobStats.put("PIG_STATS_REDUCE_OUTPUT_RECORDS", (Long.valueOf(taskgroup.getCounterForName("REDUCE_OUTPUT_RECORDS").getCounter())).toString());
                    jobStats.put("PIG_STATS_BYTES_WRITTEN", (Long.valueOf(hdfsgroup.getCounterForName("HDFS_WRITE").getCounter())).toString());
                } catch (IOException e) {
                    // TODO Auto-generated catch block
                    String error = "Unable to get the counters.";
                    throw new ExecException(error, e);
                }
                
            
            
        }
        
        getLastJobIDs(jc.getSuccessfulJobs());
        
        return stats;
    }
    

    private void getLastJobIDs(List<Job> jobs) {
        rootJobIDs.clear();
         Set<Job> temp = new HashSet<Job>();
         for(Job job : jobs) {
             if(job.getDependingJobs() != null && job.getDependingJobs().size() > 0)
                 temp.addAll(job.getDependingJobs());
         }
         
         //difference between temp and jobs would be the set of leaves
         //we can safely assume there would be only one leaf
         for(Job job : jobs) {
             if(temp.contains(job)) continue;
             else rootJobIDs.add(job.getAssignedJobID().toString());
         }
    }
    
    public List<String> getRootJobIDs() {
        return rootJobIDs;
    }
    
    public Map<String, Map<String, String>> getPigStats() {
        return stats;
    }
    
    public long getRecordsWritten() {
        if(mode == ExecType.LOCAL)
            return getRecordsCountLocal();
        else if(mode == ExecType.MAPREDUCE)
            return getRecordsCountMR();
        else
            throw new RuntimeException("Unrecognized mode. Either MapReduce or Local mode expected.");
    }
    
    private long getRecordsCountLocal() {
        //System.out.println(getPhysicalPlan());
        //because of the nature of the parser, there will always be only one store

        for(PhysicalOperator op : php.getLeaves()) {
            return Long.parseLong(stats.get(op.toString()).get("PIG_STATS_LOCAL_OUTPUT_RECORDS"));
        }
        return 0;
    }
    
    /**
     * Returns the no. of records written by the pig script in MR mode
     * @return
     */
    private long getRecordsCountMR() {
        long records = 0;
        for (String jid : rootJobIDs) {
            Map<String, String> jobStats = stats.get(jid);
            if (jobStats == null) continue;
            String reducePlan = jobStats.get("PIG_STATS_REDUCE_PLAN");
        	if(reducePlan == null) {
            	records += Long.parseLong(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        	} else {
            	records += Long.parseLong(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS"));
        	}
        }
    	return records;
    }
    
    public long getBytesWritten() {
    	if(mode == ExecType.LOCAL) {
    		return getLocalBytesWritten();
    	} else if(mode == ExecType.MAPREDUCE) {
    		return getMapReduceBytesWritten();
    	} else {
    		throw new RuntimeException("Unrecognized mode. Either MapReduce or Local mode expected.");
    	}
    	
    }
    
    private long getLocalBytesWritten() {
    	for(PhysicalOperator op : php.getLeaves())
    		return Long.parseLong(stats.get(op.toString()).get("PIG_STATS_LOCAL_BYTES_WRITTEN"));
    	return 0;
    }
    
    private long getMapReduceBytesWritten() {
        long bytesWritten = 0;
        for (String jid : rootJobIDs) {
            Map<String, String> jobStats = stats.get(jid);
            if (jobStats == null) continue;
            bytesWritten += Long.parseLong(jobStats.get("PIG_STATS_BYTES_WRITTEN"));
        }
        return bytesWritten;
    }
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.Collection;
import java.util.List;
import java.util.Iterator;
import java.util.Set;
import java.util.Map;
import java.util.ArrayList;

import org.apache.commons.el.RelationalOperator;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.PlanWalker;
import org.apache.pig.impl.plan.DepthFirstWalker;
import org.apache.pig.impl.plan.DependencyOrderWalker;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.plan.PlanException;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

/**
 * A visitor to remove redundant operators in a plan
 */
public class RemoveRedundantOperators extends
        LOVisitor {

    public RemoveRedundantOperators(LogicalPlan plan) {
        super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(plan));
    }

    /**
     * 
     * @param project
     *            the logical project operator that has to be visited
     * @throws VisitorException
     */
    protected void visit(LOProject project) throws VisitorException {
        LogicalPlan currentPlan = (LogicalPlan)mCurrentWalker.getPlan();
        
        //if the project is a project(*) and if there are predecessors 
        // and successors that are
        //1. both relational operators OR
        //2. both expression operators
        //then the project(*) can be removed and the input and outputs
        // short circuited, i.e. directly connected
        if(project.isStar()) {

            List<LogicalOperator> prSuccessors = 
                    currentPlan.getSuccessors(project);
            
            List<LogicalOperator> prPredecessors = 
                    currentPlan.getPredecessors(project);
            
            if( ((prSuccessors != null) && (prSuccessors.size() > 0)) 
                    /* prPredecessors.size() == 1 for project(*) */
                    && ((prPredecessors != null) && (prPredecessors.size() == 1)) ){
                
                LogicalOperator pred =  prPredecessors.get(0);
                
                
                //check if either all pred and succ oper are ExpressionOperator
                // or if all of them are relationalOperators (ie != ExpressionOperator)
                boolean allExpressionOp = true;
                boolean allRelationalOp = true;
                if(pred instanceof ExpressionOperator)
                    allRelationalOp = false;
                else 
                    allExpressionOp = false;
                
                for(LogicalOperator op: prSuccessors){
                    if (op instanceof ExpressionOperator) 
                        allRelationalOp = false;
                    else 
                        allExpressionOp = false;
                    
                    if(allExpressionOp == false && allRelationalOp == false)
                        break;
                }
                
                // remove project if either condition is met
                if(allExpressionOp == true || allRelationalOp == true){
                    try{
                        currentPlan.removeAndReconnectMultiSucc(project);
                        patchInputReference(pred, project, prSuccessors);
                    }catch (PlanException pe){
                        String msg = new String("Error while removing redundant project in plan");
                        throw new VisitorException(msg,pe);
                    }
                }
                
            }
        }       
    }
    
    
    private void patchInputReference(LogicalOperator pred, LogicalOperator current, List<LogicalOperator> succs) {
        for(LogicalOperator n : succs){
            // special handling of LOProject because its getExpression() does
            // need not be same as getPredecessors(LOProject)
            if(n instanceof LOProject){
                LOProject lop = (LOProject)n;
                if(current == lop.getExpression()){
                    lop.setExpression((LogicalOperator)pred);
                }
            }
        }
    }
    
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.HashSet;
import java.util.HashMap;
import java.util.Iterator;

import org.apache.pig.PigException;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.optimizer.SchemaRemover;
import org.apache.pig.impl.logicalLayer.schema.SchemaMergeException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;

public class LOCogroup extends LogicalOperator {
    private static final long serialVersionUID = 2L;

    /**
     * Cogroup contains a list of logical operators corresponding to the
     * relational operators and a list of generates for each relational
     * operator. Each generate operator in turn contains a list of expressions
     * for the columns that are projected
     */
    private boolean[] mIsInner;
    private static Log log = LogFactory.getLog(LOCogroup.class);
    private MultiMap<LogicalOperator, LogicalPlan> mGroupByPlans;

    /**
     * 
     * @param plan
     *            LogicalPlan this operator is a part of.
     * @param k
     *            OperatorKey for this operator
     * @param groupByPlans
     *            the group by columns
     * @param isInner
     *            indicates whether the cogroup is inner for each relation
     */
    public LOCogroup(
            LogicalPlan plan,
            OperatorKey k,
            MultiMap<LogicalOperator, LogicalPlan> groupByPlans,
            boolean[] isInner) {
        super(plan, k);
        mGroupByPlans = groupByPlans;
        mIsInner = isInner;
    }

    public List<LogicalOperator> getInputs() {
        return mPlan.getPredecessors(this);
    }

    public MultiMap<LogicalOperator, LogicalPlan> getGroupByPlans() {
        return mGroupByPlans;
    }    

    public void setGroupByPlans(MultiMap<LogicalOperator, LogicalPlan> groupByPlans) {
        mGroupByPlans = groupByPlans;
    }    

    public boolean[] getInner() {
        return mIsInner;
    }

    public void setInner(boolean[] inner) {
        mIsInner = inner;
    }

    @Override
    public String name() {
        return "CoGroup " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    @Override
    public Schema getSchema() throws FrontendException {
        List<LogicalOperator> inputs = mPlan.getPredecessors(this);
        /*
         * Dumping my understanding of how the schema of a Group/CoGroup will
         * look. The first field of the resulting tuple will have the alias
         * 'group'. The schema for this field is a union of the group by columns
         * for each input. The subsequent fields in the output tuple will have
         * the alias of the input as the alias for a bag that contains the
         * tuples from the input that match the grouping criterion
         */
        if (!mIsSchemaComputed) {
            List<Schema.FieldSchema> fss = new ArrayList<Schema.FieldSchema>(
                    inputs.size() + 1);
            // one more to account for the "group"
            // the alias of the first field is group and hence the
            // string "group"

            /*
             * Here goes an attempt to describe how the schema for the first
             * column - 'group' should look like. If the number of group by
             * columns = 1 then the schema for 'group' is the
             * schema(fieldschema(col)) If the number of group by columns > 1
             * then find the set union of the group by columns and form the
             * schema as schema(list<fieldschema of the cols>)
             * The parser will ensure that the number of group by columns are
             * the same across all inputs. The computation of the schema for group
             * is as follows:
             * For each input of cogroup, for each operator (projection ,udf, constant), etc.
             * compute the multimaps <group_column_number, alias> and <group_column_number, operator>
             * and <alias, expression_operator>
             * Also set the lookup table for each alias to false
             */

            Schema groupBySchema = null;
            List<Schema.FieldSchema> groupByFss = new ArrayList<Schema.FieldSchema>();
            Map<String, Boolean> aliasLookup = new HashMap<String, Boolean>();
            MultiMap<String, ExpressionOperator> aliasExop = new MultiMap<String, ExpressionOperator>();
            MultiMap<Integer, String> positionAlias = new MultiMap<Integer, String>();
            MultiMap<Integer, ExpressionOperator> positionOperators = new MultiMap<Integer, ExpressionOperator>();
            
            for (LogicalOperator op : inputs) {
                int position = 0;
                for(LogicalPlan plan: mGroupByPlans.get(op)) {
                    for(LogicalOperator eOp: plan.getLeaves()) {
                        Schema.FieldSchema fs = ((ExpressionOperator)eOp).getFieldSchema();
                        if (null != fs) {
                            String alias = fs.alias;
                            if(null != alias) {
                                aliasLookup.put(alias, false);
                                aliasExop.put(alias, (ExpressionOperator)eOp);                            
                                positionAlias.put(position, alias);
                            }
                            //store the operators for each position in the group
                        } else {
                            log.warn("Field Schema of an expression operator cannot be null"); 
                        }
                        positionOperators.put(position, (ExpressionOperator)eOp);
                    }
                    ++position;
                }
            }
            
            /*
             * Now that the multi maps and the look up table are computed, do the following:
             * for each column in the group, in order check if the alias is alaready used or not
             * If the alias is already used, check for the next unused alias.
             * IF none of the aliases can be used then the alias of that column is null
             * If an alias is found usable, then use that alias and the schema of the expression operator
             * corresponding to that position. Note that the first operator for that position is
             * picked. The type checker will ensure that the correct schema is merged
             */
            int arity = mGroupByPlans.get(inputs.get(0)).size();
            for (int i = 0; i < arity; ++i) {
                Schema.FieldSchema groupByFs;
                Collection<String> cAliases = positionAlias.get(i);
                if(null != cAliases) {
                    Object[] aliases = cAliases.toArray();
                    for(int j = 0; j < aliases.length; ++j) {
                        String alias = (String) aliases[j];
                        if(null != alias) {
                            //Collection<ExpressionOperator> cEops = aliasExop.get(alias);
                            Collection<ExpressionOperator> cEops = positionOperators.get(i);
                            if(null != cEops) {
                                ExpressionOperator eOp = (ExpressionOperator) (cEops.toArray())[0];
                                if(null != eOp) {
                                    if(!aliasLookup.get(alias)) {
                                        Schema.FieldSchema fs = eOp.getFieldSchema();
                                        if(null != fs) {
                                            groupByFs = new Schema.FieldSchema(alias, fs.schema, fs.type);
                                            groupByFss.add(groupByFs);
                                            aliasLookup.put(alias, true);
                                        } else {
                                            groupByFs = new Schema.FieldSchema(alias, null, DataType.BYTEARRAY);
                                            groupByFss.add(groupByFs);
                                        }
                                        setFieldSchemaParent(groupByFs, positionOperators, i);
                                        break;
                                    } else {
                                        if((j + 1) < aliases.length) {
                                            continue;
                                        } else {
                                            //we have seen this alias before
                                            //just add the schema of the expression operator with the null alias
                                            Schema.FieldSchema fs = eOp.getFieldSchema();
                                            if(null != fs) {
                                                groupByFs = new Schema.FieldSchema(null, fs.schema, fs.type);
                                                groupByFss.add(groupByFs);
                                                for(ExpressionOperator op: cEops) {
                                                    Schema.FieldSchema opFs = op.getFieldSchema();
                                                    if(null != opFs) {
                                                        groupByFs.setParent(opFs.canonicalName, eOp);
                                                    } else {
                                                        groupByFs.setParent(null, eOp);
                                                    }
                                                }
                                            } else {
                                                groupByFs = new Schema.FieldSchema(null, null, DataType.BYTEARRAY);
                                                groupByFss.add(groupByFs);
                                            }
                                            setFieldSchemaParent(groupByFs, positionOperators, i);
                                            break;
                                        }
                                    }
                                } else {
                                    //should not be here
                                    log.debug("Cannot be here: we cannot have a collection of null expression operators");
                                }
                            } else {
                                //should not be here
                                log.debug("Cannot be here: we should have an expression operator at each position");
                            }
                        } else {
                            //should not be here
                            log.debug("Cannot be here: we cannot have a collection of null aliases ");
                        }
                    }
                } else {
                    //We do not have any alias for this position in the group by columns
                    //We have positions $1, $2, etc.
                    Collection<ExpressionOperator> cEops = positionOperators.get(i);
                    if(null != cEops) {
                        ExpressionOperator eOp = (ExpressionOperator) (cEops.toArray())[0];
                        if(null != eOp) {
                            Schema.FieldSchema fs = eOp.getFieldSchema();
                            if(null != fs) {
                                groupByFs = new Schema.FieldSchema(null, fs.schema, fs.type);
                                groupByFss.add(groupByFs);
                            } else {
                                groupByFs = new Schema.FieldSchema(null, null, DataType.BYTEARRAY);
                                groupByFss.add(groupByFs);
                            }
                        } else {
                            groupByFs = new Schema.FieldSchema(null, DataType.BYTEARRAY);
                            groupByFss.add(groupByFs);
                        }
                    } else {
                        groupByFs = new Schema.FieldSchema(null, DataType.BYTEARRAY);
                        groupByFss.add(groupByFs);
                    }
                    setFieldSchemaParent(groupByFs, positionOperators, i);
                }
            }            

            groupBySchema = new Schema(groupByFss);

            if(1 == arity) {
                byte groupByType = getAtomicGroupByType();
                Schema groupSchema = groupByFss.get(0).schema;
                Schema.FieldSchema groupByFs = new Schema.FieldSchema("group", groupSchema, groupByType);
                setFieldSchemaParent(groupByFs, positionOperators, 0);
                fss.add(groupByFs);
            } else {
                Schema mergedGroupSchema = getTupleGroupBySchema();
                if(mergedGroupSchema.size() != groupBySchema.size()) {
                    mSchema = null;
                    mIsSchemaComputed = false;
                    int errCode = 2000;
                    String msg = "Internal error. Mismatch in group by arities. Expected: " + mergedGroupSchema + ". Found: " + groupBySchema;
                    throw new FrontendException(msg, errCode, PigException.BUG, false, null);
                } else {
                    for(int i = 0; i < mergedGroupSchema.size(); ++i) {
                        Schema.FieldSchema mergedFs = mergedGroupSchema.getField(i);
                        Schema.FieldSchema groupFs = groupBySchema.getField(i);
                        mergedFs.alias = groupFs.alias;
                        mergedGroupSchema.addAlias(mergedFs.alias, mergedFs);
                    }
                }
                
                Schema.FieldSchema groupByFs = new Schema.FieldSchema("group", mergedGroupSchema);
                fss.add(groupByFs);
                for(int i = 0; i < arity; ++i) {
                    setFieldSchemaParent(groupByFs, positionOperators, i);
                }
            }
            for (LogicalOperator op : inputs) {
                try {
                    Schema.FieldSchema bagFs = new Schema.FieldSchema(op.getAlias(),
                            op.getSchema(), DataType.BAG);
                    fss.add(bagFs);
                    setFieldSchemaParent(bagFs, op);
                } catch (FrontendException ioe) {
                    mIsSchemaComputed = false;
                    mSchema = null;
                    throw ioe;
                }
            }
            mIsSchemaComputed = true;
            mSchema = new Schema(fss);
            mType = DataType.BAG;//mType is from the super class
        }
        return mSchema;
    }

    public boolean isTupleGroupCol() {
        List<LogicalOperator> inputs = mPlan.getPredecessors(this);
        if (inputs == null || inputs.size() == 0) {
            throw new AssertionError("COGroup.isTupleGroupCol() can be called "
                                     + "after it has an input only") ;
        }
        return mGroupByPlans.get(inputs.get(0)).size() > 1 ;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    /***
     *
     * This does switch the mapping
     *
     * oldOp -> List of inner plans
     *         to
     * newOp -> List of inner plans
     *
     * which is useful when there is a structural change in LogicalPlan
     *
     * @param oldOp the old operator
     * @param newOp the new operator
     */
    public void switchGroupByPlanOp(LogicalOperator oldOp,
                                    LogicalOperator newOp) {
        Collection<LogicalPlan> innerPlans = mGroupByPlans.removeKey(oldOp) ;
        mGroupByPlans.put(newOp, innerPlans);
    }

    public void unsetSchema() throws VisitorException{
        for(LogicalOperator input: getInputs()) {
            for(LogicalPlan plan: mGroupByPlans.get(input)) {
                SchemaRemover sr = new SchemaRemover(plan);
                sr.visit();
            }
        }
        super.unsetSchema();
    }

    /**
     * This can be used to get the merged type of output group col
     * only when the group col is of atomic type
     * TODO: This doesn't work with group by complex type
     * @return The type of the group by
     */
    public byte getAtomicGroupByType() throws FrontendException {
        if (isTupleGroupCol()) {
            int errCode = 1010;
            String msg = "getAtomicGroupByType is used only when"
                + " dealing with atomic group col";
            throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
        }

        byte groupType = DataType.BYTEARRAY ;
        // merge all the inner plan outputs so we know what type
        // our group column should be
        for(int i=0;i < getInputs().size(); i++) {
            LogicalOperator input = getInputs().get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(getGroupByPlans().get(input)) ;
            if (innerPlans.size() != 1) {
                int errCode = 1012;
                String msg = "Each COGroup input has to have "
                + "the same number of inner plans";
                throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
            }
            byte innerType = innerPlans.get(0).getSingleLeafPlanOutputType() ;
            groupType = DataType.mergeType(groupType, innerType) ;
        }

        return groupType ;
    }

    /*
        This implementation is based on the assumption that all the
        inputs have the same group col tuple arity.
        TODO: This doesn't work with group by complex type
     */
    public Schema getTupleGroupBySchema() throws FrontendException {
        if (!isTupleGroupCol()) {
            int errCode = 1011;
            String msg = "getTupleGroupBySchema is used only when"
                + " dealing with tuple group col";
            throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
        }

        // this fsList represents all the columns in group tuple
        List<Schema.FieldSchema> fsList = new ArrayList<Schema.FieldSchema>() ;

        int outputSchemaSize = getGroupByPlans().get(getInputs().get(0)).size() ;

        // by default, they are all bytearray
        // for type checking, we don't care about aliases
        for(int i=0; i<outputSchemaSize; i++) {
            fsList.add(new Schema.FieldSchema(null, DataType.BYTEARRAY)) ;
        }

        // merge all the inner plan outputs so we know what type
        // our group column should be
        for(int i=0;i < getInputs().size(); i++) {
            LogicalOperator input = getInputs().get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(getGroupByPlans().get(input)) ;

            boolean seenProjectStar = false;
            for(int j=0;j < innerPlans.size(); j++) {
                byte innerType = innerPlans.get(j).getSingleLeafPlanOutputType() ;
                ExpressionOperator eOp = (ExpressionOperator)innerPlans.get(j).getSingleLeafPlanOutputOp();

                if(eOp instanceof LOProject) {
                    if(((LOProject)eOp).isStar()) {
                        seenProjectStar = true;
                    }
                }
                        
                Schema.FieldSchema groupFs = fsList.get(j);
                groupFs.type = DataType.mergeType(groupFs.type, innerType) ;
                Schema.FieldSchema fs = eOp.getFieldSchema();
                if(null != fs) {
                    groupFs.setParent(eOp.getFieldSchema().canonicalName, eOp);
                } else {
                    groupFs.setParent(null, eOp);
                }
            }

            if(seenProjectStar) {
                int errCode = 1013;
                String msg = "Grouping attributes can either be star (*) or a list of expressions, but not both.";
                throw new FrontendException(msg, errCode, PigException.INPUT, false, null);                
            }

        }

        return new Schema(fsList) ;
    }

    private void setFieldSchemaParent(Schema.FieldSchema fs, MultiMap<Integer, ExpressionOperator> positionOperators, int position) throws FrontendException {
        for(ExpressionOperator op: positionOperators.get(position)) {
            Schema.FieldSchema opFs = op.getFieldSchema();
            if(null != opFs) {
                fs.setParent(opFs.canonicalName, op);
            } else {
                fs.setParent(null, op);
            }
        }
    }

    private void setFieldSchemaParent(Schema.FieldSchema fs, LogicalOperator op) throws FrontendException {
        Schema s = op.getSchema();
        if(null != s) {
            for(Schema.FieldSchema inputFs: s.getFields()) {
                if(null != inputFs) {
                    fs.setParent(inputFs.canonicalName, op);
                } else {
                    fs.setParent(null, op);
                }
            }
        } else {
            fs.setParent(null, op);
        }
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LogicalOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        
        // first start with LogicalOperator clone
        LOCogroup  cogroupClone = (LOCogroup)super.clone();
        
        // create deep copy of other cogroup specific members
        cogroupClone.mIsInner = new boolean[mIsInner.length];
        for (int i = 0; i < mIsInner.length; i++) {
            cogroupClone.mIsInner[i] = mIsInner[i];
        }
        
        cogroupClone.mGroupByPlans = new MultiMap<LogicalOperator, LogicalPlan>();
        for (Iterator<LogicalOperator> it = mGroupByPlans.keySet().iterator(); it.hasNext();) {
            LogicalOperator relOp = it.next();
            Collection<LogicalPlan> values = mGroupByPlans.get(relOp);
            for (Iterator<LogicalPlan> planIterator = values.iterator(); planIterator.hasNext();) {
                LogicalPlanCloneHelper lpCloneHelper = new LogicalPlanCloneHelper(planIterator.next());
                cogroupClone.mGroupByPlans.put(relOp, lpCloneHelper.getClonedPlan());
            }
        }
        
        return cogroupClone;
    }
    
    @Override
    public ProjectionMap getProjectionMap() {
        Schema outputSchema;
        
        try {
            outputSchema = getSchema();
        } catch (FrontendException fee) {
            return null;
        }
        
        if(outputSchema == null) {
            return null;
        }
        
        List<LogicalOperator> predecessors = (ArrayList<LogicalOperator>)mPlan.getPredecessors(this);
        if(predecessors == null) {
            return null;
        }
        
        //the column with the alias 'group' can be mapped in several ways
        //1. group A by $0;
        //Here the mapping is 0 -> (0, 0)
        //2. group A by ($0, $1);
        //Here there is no direct mapping and 'group' is an added column
        //3. cogroup A by $0, B by $0;
        //Here the mapping is 0 -> ((0, 0), (1, 0))
        //4. cogroup A by ($0, $1), B by ($0, $1);
        //Here there is no direct mapping and 'group' is an added column
        //For anything other than a simple project 'group' is an added column
        
        MultiMap<LogicalOperator, LogicalPlan> groupByPlans = getGroupByPlans();
        
        boolean groupByAdded = false;
        MultiMap<Integer, Pair<Integer, Integer>> mapFields = new MultiMap<Integer, Pair<Integer, Integer>>();
        List<Pair<Integer, Integer>> removedFields = new ArrayList<Pair<Integer, Integer>>();
        
        for(int inputNum = 0; (inputNum < predecessors.size()) && (!groupByAdded); ++inputNum) {
            LogicalOperator predecessor = predecessors.get(inputNum);

            List<LogicalPlan> predecessorPlans = (ArrayList<LogicalPlan>) groupByPlans.get(predecessor);

            int inputColumn = -1;
            for(LogicalPlan predecessorPlan: predecessorPlans) {                
                List<LogicalOperator> leaves = predecessorPlan.getLeaves();
                if(leaves == null || leaves.size() > 1) {
                    groupByAdded = true;
                    break;
                }
                
                if(leaves.get(0) instanceof LOProject) {
                    //find out if this project is a chain of projects
                    if(LogicalPlan.chainOfProjects(predecessorPlan)) {
                        LOProject rootProject = (LOProject)predecessorPlan.getRoots().get(0);
                        inputColumn = rootProject.getCol();
                        mapFields.put(0, new Pair<Integer, Integer>(inputNum, inputColumn));
                    }
                } else {
                    groupByAdded = true;
                }                
            }
            
            Schema inputSchema;            
            try {
                inputSchema = predecessor.getSchema();
            } catch (FrontendException fee) {
                return null;
            }
            
            if(inputSchema != null) {
                for(int column = 0; column < inputSchema.size(); ++column) {
                    if(!groupByAdded && inputColumn != column) {
                        removedFields.add(new Pair<Integer, Integer>(inputNum, column));
                    }
                }
            }

        }

        List<Integer> addedFields = new ArrayList<Integer>();

        if(groupByAdded) {
            addedFields.add(0); //for the column 'group'
            mapFields = null; //since 'group' is an added column there is no mapping            
        }
        
        //the columns 1 through n - 1 are generated by cogroup
        for(int i = 0; i < groupByPlans.keySet().size(); ++i) {
            addedFields.add(i+ 1);
        }
        
        if(removedFields.size() == 0) {
            removedFields = null;
        }

        return new ProjectionMap(mapFields, removedFields, addedFields);
    }

    @Override
    public List<RequiredFields> getRequiredFields() {
        List<LogicalOperator> predecessors = mPlan.getPredecessors(this);
        
        if(predecessors == null) {
            return null;
        }

        List<RequiredFields> requiredFields = new ArrayList<RequiredFields>();
        
        for(int inputNum = 0; inputNum < predecessors.size(); ++inputNum) {
            Set<Pair<Integer, Integer>> fields = new HashSet<Pair<Integer, Integer>>();
            Set<LOProject> projectSet = new HashSet<LOProject>();
            boolean groupByStar = false;

            for (LogicalPlan plan : getGroupByPlans().get(predecessors.get(inputNum))) {
                TopLevelProjectFinder projectFinder = new TopLevelProjectFinder(plan);
                try {
                    projectFinder.visit();
                } catch (VisitorException ve) {
                    requiredFields.clear();
                    requiredFields.add(null);
                    return requiredFields;
                }
                projectSet.addAll(projectFinder.getProjectSet());
                if(projectFinder.getProjectStarSet() != null) {
                    groupByStar = true;
                }
            }

            if(groupByStar) {
                requiredFields.add(new RequiredFields(true));
            } else {                
                for (LOProject project : projectSet) {
                    for (int inputColumn : project.getProjection()) {
                        fields.add(new Pair<Integer, Integer>(inputNum, inputColumn));
                    }
                }
        
                if(fields.size() == 0) {
                    requiredFields.add(new RequiredFields(false, true));
                } else {                
                    requiredFields.add(new RequiredFields(new ArrayList<Pair<Integer, Integer>>(fields)));
                }
            }
        }
        
        return (requiredFields.size() == 0? null: requiredFields);
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import java.util.List;
import java.util.ArrayList;
import java.util.Set;

import org.junit.After;
import org.junit.Test;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.logicalLayer.*;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.test.utils.LogicalPlanTester;


public class TestRequiredFields extends junit.framework.TestCase {

    private final Log log = LogFactory.getLog(getClass());
    LogicalPlanTester planTester = new LogicalPlanTester();
    
    @After
    @Override
    public void tearDown() throws Exception{
        planTester.reset(); 
    }

    private static final String simpleEchoStreamingCommand;
    static {
        if (System.getProperty("os.name").toUpperCase().startsWith("WINDOWS"))
            simpleEchoStreamingCommand = "perl -ne 'print \\\"$_\\\"'";
        else
            simpleEchoStreamingCommand = "perl -ne 'print \"$_\"'";
    }

    @Test
    public void testQueryForeach1() {
        String query = "foreach (load 'a') generate $1,$2;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load required fields is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadRequiredFields = load.getRequiredFields();
        assertTrue(loadRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);
        
    }

    @Test
    public void testQueryForeach2() {
        String query = "foreach (load 'a' using " + PigStorage.class.getName() + "(':')) generate $1, 'aoeuaoeu' ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load required fields is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadRequiredFields = load.getRequiredFields();
        assertTrue(loadRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryCogroup1() {
        String query = "foreach (cogroup (load 'a') by $1, (load 'b') by $1) generate org.apache.pig.builtin.AVG($1) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 1);
        
        //check that the foreach required fields contain [<0, 1>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryGroupAll() throws Exception {
        String query = "foreach (group (load 'a') ALL) generate $1 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 1);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryGroup2() {
        String query = "foreach (group (load 'a') by $1) generate group, '1' ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 1);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        
        //check that the foreach required fields contain [<0, 0>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
    }

    @Test
    public void testQueryCogroup2() {
        String query = "foreach (cogroup (load 'a') by ($1), (load 'b') by ($1)) generate $1.$1, $2.$1 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);


        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 1);
        
        //check that the foreach required fields contain [<0, 1>, <0,2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);
    }

    @Test
    public void testQueryGroup3() {
        String query = "foreach (group (load 'a') by ($6, $7)) generate flatten(group) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 1);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 6);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 7);
        
        //check that the foreach required fields contain [<0, 0>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
    }

    @Test
    public void testQueryFilterNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = filter a by $1 == '3';");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check filter required fields
        LOFilter filter = (LOFilter)lp.getSuccessors(loada).get(0);
        List<RequiredFields> filterRequiredFields = filter.getRequiredFields();
        assertTrue(filterRequiredFields.size() == 1);
        
        requiredField = filterRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }
    
    @Test
    public void testQuerySplitNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("split a into b if $0 == '3', c if $1 == '3';");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOSplit split = (LOSplit)lp.getSuccessors(loada).get(0);
        List<RequiredFields> splitRequiredFields = split.getRequiredFields();
        assertTrue(splitRequiredFields.size() == 1);
        
        requiredField = splitRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.getFields() == null);
        
        //check split outputs' required fields
        LOSplitOutput splitb = (LOSplitOutput)lp.getSuccessors(split).get(0);
        List<RequiredFields> splitbRequiredFields = splitb.getRequiredFields();
        assertTrue(splitbRequiredFields.size() == 1);
        
        requiredField = splitbRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        LOSplitOutput splitc = (LOSplitOutput)lp.getSuccessors(split).get(1);
        List<RequiredFields> splitcRequiredFields = splitc.getRequiredFields();
        assertTrue(splitcRequiredFields.size() == 1);
        
        requiredField = splitcRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryOrderByNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = order a by $1;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check order by required fields
        LOSort sort = (LOSort)lp.getSuccessors(loada).get(0);
        List<RequiredFields> sortRequiredFields = sort.getRequiredFields();
        assertTrue(sortRequiredFields.size() == 1);
        
        requiredField = sortRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }
    
    @Test
    public void testQueryLimitNoSchema() {
        planTester.buildPlan("a = load 'a';");
        planTester.buildPlan("b = order a by $1;");
        LogicalPlan lp = planTester.buildPlan("c = limit b 10;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check order by required fields
        LOSort sort = (LOSort)lp.getSuccessors(loada).get(0);
        List<RequiredFields> sortRequiredFields = sort.getRequiredFields();
        assertTrue(sortRequiredFields.size() == 1);
        
        requiredField = sortRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);

        //check limit required fields
        LOLimit limit = (LOLimit)lp.getLeaves().get(0);
        List<RequiredFields> limitRequiredFields = limit.getRequiredFields();
        assertTrue(limitRequiredFields.size() == 1);
        
        requiredField = limitRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.getFields() == null);

    }

    @Test
    public void testQueryDistinctNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = distinct a;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check distinct required fields
        LODistinct distinct = (LODistinct)lp.getSuccessors(loada).get(0);
        List<RequiredFields> distinctRequiredFields = distinct.getRequiredFields();
        assertTrue(distinctRequiredFields.size() == 1);
        
        requiredField = distinctRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

    }

    @Test
    public void testQueryStreamingNoSchema() {
        String query = "stream (load 'a') through `" + simpleEchoStreamingCommand + "`;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check streaming required fields
        LOStream stream = (LOStream)lp.getSuccessors(loada).get(0);
        List<RequiredFields> streamRequiredFields = stream.getRequiredFields();
        assertTrue(streamRequiredFields.size() == 1);
        
        requiredField = streamRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

    }
    
    @Test
    public void testQueryStreamingNoSchema1() {
        String query = "stream (load 'a' as (url, hitCount)) through `" + simpleEchoStreamingCommand + "` ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check streaming required fields
        LOStream stream = (LOStream)lp.getSuccessors(loada).get(0);
        List<RequiredFields> streamRequiredFields = stream.getRequiredFields();
        assertTrue(streamRequiredFields.size() == 1);
        
        requiredField = streamRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

    }
    
    @Test
    public void testQueryForeach3() {
        String query = "foreach (load 'a') generate ($1 == '3'? $2 : $3) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>, <0,3>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 3);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);
        assertTrue(fields.get(2).first == 0);
        assertTrue(fields.get(2).second == 3);
    }

    @Test
    public void testQueryForeach4() {
        planTester.buildPlan("A = load 'a';");
        planTester.buildPlan("B = load 'b';");
        LogicalPlan lp = planTester.buildPlan("foreach (cogroup A by ($1), B by ($1)) generate A, flatten(B.($1, $2, $3));");
        
        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 1);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);
    }

    @Test
    public void testForeach5() {
        planTester.buildPlan("A = load 'a';");
        planTester.buildPlan("B = load 'b';");
        planTester.buildPlan("C = cogroup A by ($1), B by ($1);");
        String query = "foreach C { " +
                "B = order B by $0; " +
                "generate FLATTEN(A), B.($1, $2, $3) ;" +
                "};" ;
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 1);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);

    }

    @Test
    public void testQueryCrossNoSchema(){
        String query = "c = cross (load 'a'), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cross required fields
        LOCross cross = (LOCross)lp.getSuccessors(loada).get(0);
        List<RequiredFields> crossRequiredFields = cross.getRequiredFields();
        assertTrue(crossRequiredFields.size() == 2);
        
        requiredField = crossRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = crossRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
        
    }

    @Test
    public void testQueryUnionNoSchema(){
        String query = "c = union (load 'a'), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check union required fields
        LOUnion union = (LOUnion)lp.getSuccessors(loada).get(0);
        List<RequiredFields> unionRequiredFields = union.getRequiredFields();
        assertTrue(unionRequiredFields.size() == 2);
        
        requiredField = unionRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = unionRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
        
    }

    @Test
    public void testQueryFRJoinNoSchema(){
        String query = "c = join (load 'a') by $0, (load 'b') by $0 using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check frjoin required fields
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        List<RequiredFields> frjoinRequiredFields = frjoin.getRequiredFields();
        assertTrue(frjoinRequiredFields.size() == 2);
        
        requiredField = frjoinRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);


        requiredField = frjoinRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);
        
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
    }

    @Test
    public void testQueryJoinNoSchema(){
        String query = "c = join (load 'a') by $0, (load 'b') by $0;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);

    }

    @Test
    public void testQueryFilterWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = filter a by $1 == '3';");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check filter required fields
        LOFilter filter = (LOFilter)lp.getSuccessors(loada).get(0);
        List<RequiredFields> filterRequiredFields = filter.getRequiredFields();
        assertTrue(filterRequiredFields.size() == 1);
        
        requiredField = filterRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }
    
    
    @Test
    public void testQuerySplitWithSchema() {
        planTester.buildPlan("a = load 'a' as (url, hitCount);");
        LogicalPlan lp = planTester.buildPlan("split a into b if url == '3', c if hitCount == '3';");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOSplit split = (LOSplit)lp.getSuccessors(loada).get(0);
        List<RequiredFields> splitRequiredFields = split.getRequiredFields();
        assertTrue(splitRequiredFields.size() == 1);
        
        requiredField = splitRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.getFields() == null);
        
        //check split outputs' required fields
        LOSplitOutput splitb = (LOSplitOutput)lp.getSuccessors(split).get(0);
        List<RequiredFields> splitbRequiredFields = splitb.getRequiredFields();
        assertTrue(splitbRequiredFields.size() == 1);
        
        requiredField = splitbRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        LOSplitOutput splitc = (LOSplitOutput)lp.getSuccessors(split).get(1);
        List<RequiredFields> splitcRequiredFields = splitc.getRequiredFields();
        assertTrue(splitcRequiredFields.size() == 1);
        
        requiredField = splitcRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }


    @Test
    public void testQueryOrderByWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = order a by $1;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check order by required fields
        LOSort sort = (LOSort)lp.getSuccessors(loada).get(0);
        List<RequiredFields> sortRequiredFields = sort.getRequiredFields();
        assertTrue(sortRequiredFields.size() == 1);
        
        requiredField = sortRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryLimitWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        planTester.buildPlan("b = order a by $1;");
        LogicalPlan lp = planTester.buildPlan("c = limit b 10;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check order by required fields
        LOSort sort = (LOSort)lp.getSuccessors(loada).get(0);
        List<RequiredFields> sortRequiredFields = sort.getRequiredFields();
        assertTrue(sortRequiredFields.size() == 1);
        
        requiredField = sortRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);

        //check limit required fields
        LOLimit limit = (LOLimit)lp.getLeaves().get(0);
        List<RequiredFields> limitRequiredFields = limit.getRequiredFields();
        assertTrue(limitRequiredFields.size() == 1);
        
        requiredField = limitRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.getFields() == null);

    }
    
    @Test
    public void testQueryDistinctWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = distinct a;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check distinct required fields
        LODistinct distinct = (LODistinct)lp.getSuccessors(loada).get(0);
        List<RequiredFields> distinctRequiredFields = distinct.getRequiredFields();
        assertTrue(distinctRequiredFields.size() == 1);
        
        requiredField = distinctRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

    }

    @Test
    public void testQueryStreamingWithSchema() {
        String query = "stream (load 'a') through `" + simpleEchoStreamingCommand + "` as (x, y);";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check streaming required fields
        LOStream stream = (LOStream)lp.getSuccessors(loada).get(0);
        List<RequiredFields> streamRequiredFields = stream.getRequiredFields();
        assertTrue(streamRequiredFields.size() == 1);
        
        requiredField = streamRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

    }

    @Test
    public void testQueryStreamingWithSchema1() {
        String query = "stream (load 'a' as (url, hitCount)) through `" + simpleEchoStreamingCommand + "` as (x, y);";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check streaming required fields
        LOStream stream = (LOStream)lp.getSuccessors(loada).get(0);
        List<RequiredFields> streamRequiredFields = stream.getRequiredFields();
        assertTrue(streamRequiredFields.size() == 1);
        
        requiredField = streamRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }
    
    @Test
    public void testQueryImplicitJoinWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        planTester.buildPlan("b = load 'b' as (url,rank);");
        planTester.buildPlan("c = cogroup a by url, b by url;");
        LogicalPlan lp = planTester.buildPlan("d = foreach c generate group,flatten(a),flatten(b);");

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        
        //check that the foreach required fields contain [<0, 0>, <0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 3);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);
        assertTrue(fields.get(2).first == 0);
        assertTrue(fields.get(2).second == 2);

    }
    
    @Test
    public void testQueryCrossWithSchema(){
        String query = "c = cross (load 'a' as (url, hitcount)), (load 'b' as (url, rank));";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cross required fields
        LOCross cross = (LOCross)lp.getSuccessors(loada).get(0);
        List<RequiredFields> crossRequiredFields = cross.getRequiredFields();
        assertTrue(crossRequiredFields.size() == 2);
        
        requiredField = crossRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = crossRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }

    @Test
    public void testQueryUnionWithSchema(){
        String query = "c = union (load 'a' as (url, hitcount)), (load 'b' as (url, rank));";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check union required fields
        LOUnion union = (LOUnion)lp.getSuccessors(loada).get(0);
        List<RequiredFields> unionRequiredFields = union.getRequiredFields();
        assertTrue(unionRequiredFields.size() == 2);
        
        requiredField = unionRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = unionRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
        
    }

    @Test
    public void testQueryFRJoinWithSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b' as (url, rank)) by $0 using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check frjoin required fields
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        List<RequiredFields> frjoinRequiredFields = frjoin.getRequiredFields();
        assertTrue(frjoinRequiredFields.size() == 2);
        
        requiredField = frjoinRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);


        requiredField = frjoinRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);
        
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);

    }

    @Test
    public void testQueryJoinWithSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b' as (url, rank)) by $0;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);

    }

    @Test
    public void testQueryCrossWithMixedSchema(){
        String query = "c = cross (load 'a' as (url, hitcount)), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cross required fields
        LOCross cross = (LOCross)lp.getSuccessors(loada).get(0);
        List<RequiredFields> crossRequiredFields = cross.getRequiredFields();
        assertTrue(crossRequiredFields.size() == 2);
        
        requiredField = crossRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = crossRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }

    @Test
    public void testQueryUnionWithMixedSchema(){
        String query = "c = union (load 'a' as (url, hitcount)), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check union required fields
        LOUnion union = (LOUnion)lp.getSuccessors(loada).get(0);
        List<RequiredFields> unionRequiredFields = union.getRequiredFields();
        assertTrue(unionRequiredFields.size() == 2);
        
        requiredField = unionRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = unionRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
        
    }

    @Test
    public void testQueryFRJoinWithMixedSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b') by $0 using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check frjoin required fields
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        List<RequiredFields> frjoinRequiredFields = frjoin.getRequiredFields();
        assertTrue(frjoinRequiredFields.size() == 2);
        
        requiredField = frjoinRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);


        requiredField = frjoinRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);
        
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        
    }
    
    @Test
    public void testQueryJoinWithMixedSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b') by $0;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);
    }

    @Test
    public void testQueryFilterWithStarNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = filter a by COUNT(*) == 3;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check filter required fields
        LOFilter filter = (LOFilter)lp.getSuccessors(loada).get(0);
        List<RequiredFields> filterRequiredFields = filter.getRequiredFields();
        assertTrue(filterRequiredFields.size() == 1);
        
        requiredField = filterRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.getFields() == null);
        
    }

    @Test
    public void testQueryOrderByStarNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = order a by *;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check order by required fields
        LOSort sort = (LOSort)lp.getSuccessors(loada).get(0);
        List<RequiredFields> sortRequiredFields = sort.getRequiredFields();
        assertTrue(sortRequiredFields.size() == 1);
        
        requiredField = sortRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.getFields() == null);
        
    }
    
    @Test
    public void testQueryGroupByStarNoSchema() throws Exception {
        String query = "foreach (group (load 'a') by *) generate $1 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 1);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryFRJoinOnStarNoSchema(){
        String query = "c = join (load 'a') by *, (load 'b') by * using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check frjoin required fields
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        List<RequiredFields> frjoinRequiredFields = frjoin.getRequiredFields();
        assertTrue(frjoinRequiredFields.size() == 2);
        
        requiredField = frjoinRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = frjoinRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
        
    }
    
    @Test
    public void testQueryJoinOnStarNoSchema(){
        String query = "c = join (load 'a') by *, (load 'b') by *;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);
    }

    @Test
    public void testQueryFilterStarWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = filter a by COUNT(*) == 3;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check filter required fields
        LOFilter filter = (LOFilter)lp.getSuccessors(loada).get(0);
        List<RequiredFields> filterRequiredFields = filter.getRequiredFields();
        assertTrue(filterRequiredFields.size() == 1);
        
        requiredField = filterRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.getFields() == null);
        
    }
    
    @Test
    public void testQuerySplitWithStarSchema() {
        planTester.buildPlan("a = load 'a' as (url, hitCount);");
        LogicalPlan lp = planTester.buildPlan("split a into b if url == '3', c if COUNT(*) == '3';");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOSplit split = (LOSplit)lp.getSuccessors(loada).get(0);
        List<RequiredFields> splitRequiredFields = split.getRequiredFields();
        assertTrue(splitRequiredFields.size() == 1);
        
        requiredField = splitRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.getFields() == null);
        
        //check split outputs' required fields
        LOSplitOutput splitb = (LOSplitOutput)lp.getSuccessors(split).get(0);
        List<RequiredFields> splitbRequiredFields = splitb.getRequiredFields();
        assertTrue(splitbRequiredFields.size() == 1);
        
        requiredField = splitbRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        
        LOSplitOutput splitc = (LOSplitOutput)lp.getSuccessors(split).get(1);
        List<RequiredFields> splitcRequiredFields = splitc.getRequiredFields();
        assertTrue(splitcRequiredFields.size() == 1);
        
        requiredField = splitcRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.getFields() == null);
        
    }
    
    @Test
    public void testQueryOrderByStarWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = order a by *;");
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check order by required fields
        LOSort sort = (LOSort)lp.getSuccessors(loada).get(0);
        List<RequiredFields> sortRequiredFields = sort.getRequiredFields();
        assertTrue(sortRequiredFields.size() == 1);
        
        requiredField = sortRequiredFields.get(0);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.needNoFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields();
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);
    }
    
    @Test
    public void testQueryGroupByStarWithSchema() throws Exception {
        String query = "foreach (group (load 'a' as (url, hitCount)) by *) generate $1 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        
        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 1);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);
        
        //check that the foreach required fields contain [<0, 1>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 1);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
    }

    @Test
    public void testQueryFRJoinOnStarWithSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by *, (load 'b' as (url, rank)) by * using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check frjoin required fields
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        List<RequiredFields> frjoinRequiredFields = frjoin.getRequiredFields();
        assertTrue(frjoinRequiredFields.size() == 2);
        
        requiredField = frjoinRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);

        requiredField = frjoinRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);
        
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 1);
        assertTrue(fields.get(1).second == 1);

    }

    @Test
    public void testQueryJoinOnStarWithSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by *, (load 'b' as (url, rank)) by *;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        List<RequiredFields> loadbRequiredFields = loadb.getRequiredFields();
        assertTrue(loadbRequiredFields.size() == 1);
        
        requiredField = loadbRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);

        //check cogroup required fields
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        List<RequiredFields> cogroupRequiredFields = cogroup.getRequiredFields();
        assertTrue(cogroupRequiredFields.size() == 2);
        
        requiredField = cogroupRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);
        
        requiredField = cogroupRequiredFields.get(1);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 1);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 1);
        assertTrue(fields.get(1).second == 1);
        
        //check that the foreach required fields contain [<0, 1>, <0, 2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 1);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 2);

    }
    
    @Test
    public void testQueryForeachGenerateStarNoSchema() {
        String query = "foreach (load 'a') generate * ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 3>, <0,2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

    }

    @Test
    public void testQueryForeachGenerateCountStarNoSchema() {
        String query = "foreach (load 'a') generate COUNT(*) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 3>, <0,2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);

    }
    
    @Test
    public void testQueryForeachGenerateStarNoSchema1() {
        String query = "foreach (load 'a') generate *, COUNT(*) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 3>, <0,2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }

    @Test
    public void testQueryForeachGenerateStarNoSchema2() {
        String query = "foreach (load 'a') generate *, $0 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 3>, <0,2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }
    
    @Test
    public void testQueryForeachGenerateStarWithSchema() {
        String query = "foreach (load 'a' as (url, hitCount)) generate * ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 1>, <0, 3>, <0,2>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);

        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);

    }

    @Test
    public void testQueryForeachGenerateCountStarWithSchema() {
        String query = "foreach (load 'a' as (url, hitCount)) generate COUNT(*) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check the foreach required fields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);
        
        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }

    @Test
    public void testQueryForeachGenerateStarWithSchema1() {
        String query = "foreach (load 'a' as (url, hitCount)) generate *, COUNT(*) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check the foreach required fields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == true);
        assertTrue(requiredField.getFields() == null);
    }

    @Test
    public void testQueryForeachGenerateStarWithSchema2() {
        String query = "foreach (load 'a' as (url, hitCount)) generate *, url ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load's required fields is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        List<RequiredFields> loadaRequiredFields = loada.getRequiredFields();
        assertTrue(loadaRequiredFields.size() == 1);
        
        RequiredFields requiredField = loadaRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == true);
        assertTrue(requiredField.needAllFields() == false);
        assertTrue(requiredField.getFields() == null);
        
        //check that the foreach required fields contain [<0, 0>, <0, 1>]
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        List<RequiredFields> foreachRequiredFields = foreach.getRequiredFields();
        assertTrue(foreachRequiredFields.size() == 1);

        requiredField = foreachRequiredFields.get(0);
        assertTrue(requiredField.needNoFields() == false);
        assertTrue(requiredField.needAllFields() == false);
        
        List<Pair<Integer, Integer>> fields = requiredField.getFields(); 
        assertTrue(fields.size() == 2);
        assertTrue(fields.get(0).first == 0);
        assertTrue(fields.get(0).second == 0);
        assertTrue(fields.get(1).first == 0);
        assertTrue(fields.get(1).second == 1);
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.Iterator;
import java.util.List;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ExpressionOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.VisitorException;

/**
 * This implementation is applicable for both the physical plan and for the
 * local backend, as the conversion of physical to mapreduce would see the SORT
 * operator and take necessary steps to convert it to a quantile and a sort job.
 * 
 * This is a blocking operator. The sortedDataBag accumulates Tuples and sorts
 * them only when there an iterator is started. So all the tuples from the input
 * operator should be accumulated and filled into the dataBag. The attachInput
 * method is not applicable here.
 * 
 * 
 */
public class POSort extends PhysicalOperator {

	/**
     * 
     */
    private static final long serialVersionUID = 1L;
    //private List<Integer> mSortCols;
	private List<PhysicalPlan> sortPlans;
	private List<Byte> ExprOutputTypes;
	private List<Boolean> mAscCols;
	private POUserComparisonFunc mSortFunc;
	private final Log log = LogFactory.getLog(getClass());
	private Comparator<Tuple> mComparator;

	private boolean inputsAccumulated = false;
	private long limit;
	public boolean isUDFComparatorUsed = false;
	private DataBag sortedBag;
	transient Iterator<Tuple> it;

	public POSort(
            OperatorKey k,
            int rp,
            List inp,
            List<PhysicalPlan> sortPlans,
			List<Boolean> mAscCols,
            POUserComparisonFunc mSortFunc) {
		super(k, rp, inp);
		//this.mSortCols = mSortCols;
		this.sortPlans = sortPlans;
		this.mAscCols = mAscCols;
        this.limit = -1;
		this.mSortFunc = mSortFunc;
		if (mSortFunc == null) {
            mComparator = new SortComparator();
			/*sortedBag = BagFactory.getInstance().newSortedBag(
					new SortComparator());*/
			ExprOutputTypes = new ArrayList<Byte>(sortPlans.size());

			for(PhysicalPlan plan : sortPlans) {
				ExprOutputTypes.add(plan.getLeaves().get(0).getResultType());
			}
		} else {
			/*sortedBag = BagFactory.getInstance().newSortedBag(
					new UDFSortComparator());*/
            mComparator = new UDFSortComparator();
			isUDFComparatorUsed = true;
		}
	}

	public POSort(OperatorKey k, int rp, List inp) {
		super(k, rp, inp);

	}

	public POSort(OperatorKey k, int rp) {
		super(k, rp);

	}

	public POSort(OperatorKey k, List inp) {
		super(k, inp);

	}

	public POSort(OperatorKey k) {
		super(k);

	}
	
	public class SortComparator implements Comparator<Tuple>,Serializable {
		/**
         * 
         */
        private static final long serialVersionUID = 1L;

        public int compare(Tuple o1, Tuple o2) {
			int count = 0;
			int ret = 0;
			if(sortPlans == null || sortPlans.size() == 0) 
				return 0;
			for(PhysicalPlan plan : sortPlans) {
				try {
					plan.attachInput(o1);
					Result res1 = getResult(plan, ExprOutputTypes.get(count));
					plan.attachInput(o2);
					Result res2 = getResult(plan, ExprOutputTypes.get(count));
					if(res1.returnStatus != POStatus.STATUS_OK || res2.returnStatus != POStatus.STATUS_OK) {
						log.error("Error processing the input in the expression plan : " + plan.toString());
					} else {
						if(mAscCols.get(count++)) {
							ret = DataType.compare(res1.result, res2.result);
                            // If they are not equal, return
                            // Otherwise, keep comparing the next one
                            if (ret != 0) {
                                return ret ;
                            }
                        }
                        else {
                            ret = DataType.compare(res2.result, res1.result);
                            if (ret != 0) {
                                return ret ;
                            }

                        }

					}
						
				} catch (ExecException e) {
					log.error("Invalid result while executing the expression plan : " + plan.toString() + "\n" + e.getMessage());
				}
			}
			return ret;
		} 
		
		private Result getResult(PhysicalPlan plan, byte resultType) throws ExecException {
			ExpressionOperator Op = (ExpressionOperator) plan.getLeaves().get(0);
			Result res = null;
			
			switch (resultType) {
            case DataType.BYTEARRAY:
                res = Op.getNext(dummyDBA);
                break;
            case DataType.CHARARRAY:
                res = Op.getNext(dummyString);
                break;
            case DataType.DOUBLE:
                res = Op.getNext(dummyDouble);
                break;
            case DataType.FLOAT:
                res = Op.getNext(dummyFloat);
                break;
            case DataType.INTEGER:
                res = Op.getNext(dummyInt);
                break;
            case DataType.LONG:
                res = Op.getNext(dummyLong);
                break;
            case DataType.TUPLE:
                res = Op.getNext(dummyTuple);
                break;

            default: {
                int errCode = 2082;
                String msg = "Did not expect result of type: " +
                        DataType.findTypeName(resultType);
                    throw new ExecException(msg, errCode, PigException.BUG);                
            }
            
            }
			return res;
		}
	}

	public class UDFSortComparator implements Comparator<Tuple>,Serializable {

		/**
         * 
         */
        private static final long serialVersionUID = 1L;

        public int compare(Tuple t1, Tuple t2) {

			mSortFunc.attachInput(t1, t2);
			Integer i = null;
			Result res = null;
			try {
				res = mSortFunc.getNext(i);
			} catch (ExecException e) {

				log.error("Input not ready. Error on reading from input. "
						+ e.getMessage());
			}
			if (res != null)
				return (Integer) res.result;
			else
				return 0;
		}

	}

	@Override
	public String name() {

		return "POSort" + "[" + DataType.findTypeName(resultType) + "]" + "(" + (mSortFunc!=null?mSortFunc.getFuncSpec():"") + ")" +" - " + mKey.toString();
	}

	@Override
	public boolean isBlocking() {

		return true;
	}

	@Override
	public Result getNext(Tuple t) throws ExecException {
		Result res = new Result();
		if (!inputsAccumulated) {
			res = processInput();
            sortedBag = BagFactory.getInstance().newSortedBag(mComparator);
			while (res.returnStatus != POStatus.STATUS_EOP) {
				if (res.returnStatus == POStatus.STATUS_ERR) {
					log.error("Error in reading from the inputs");
					return res;
					//continue;
				} else if (res.returnStatus == POStatus.STATUS_NULL) {
                    // ignore the null, read the next tuple.
                    res = processInput();
					continue;
				}
				sortedBag.add((Tuple) res.result);
				res = processInput();

			}

			inputsAccumulated = true;

		}
		if (it == null) {
            it = sortedBag.iterator();
        }
        if (it.hasNext()) {
            res.result = it.next();
            if(lineageTracer != null) {
                lineageTracer.insert((Tuple) res.result);
                lineageTracer.union((Tuple)res.result, (Tuple)res.result);
            }
            res.returnStatus = POStatus.STATUS_OK;
        } else {
            res.returnStatus = POStatus.STATUS_EOP;
            reset();
        }
		return res;
	}

	@Override
	public boolean supportsMultipleInputs() {

		return false;
	}

	@Override
	public boolean supportsMultipleOutputs() {

		return false;
	}

	@Override
	public void visit(PhyPlanVisitor v) throws VisitorException {

		v.visitSort(this);
	}

    @Override
    public void reset() {
        inputsAccumulated = false;
        sortedBag = null;
        it = null;
    }

    public List<PhysicalPlan> getSortPlans() {
        return sortPlans;
    }

    public void setSortPlans(List<PhysicalPlan> sortPlans) {
        this.sortPlans = sortPlans;
    }

    public POUserComparisonFunc getMSortFunc() {
        return mSortFunc;
    }

    public void setMSortFunc(POUserComparisonFunc sortFunc) {
        mSortFunc = sortFunc;
    }

    public List<Boolean> getMAscCols() {
        return mAscCols;
    }
    
    public void setLimit(long l)
    {
    	limit = l;
    }
    
    public long getLimit()
    {
    	return limit;
    }
    
    public boolean isLimited()
    {
    	return (limit!=-1);
    }

    @Override
    public POSort clone() throws CloneNotSupportedException {
        List<PhysicalPlan> clonePlans = new
            ArrayList<PhysicalPlan>(sortPlans.size());
        for (PhysicalPlan plan : sortPlans) {
            clonePlans.add(plan.clone());
        }
        List<Boolean> cloneAsc = new ArrayList<Boolean>(mAscCols.size());
        for (Boolean b : mAscCols) {
            cloneAsc.add(b);
        }
        POUserComparisonFunc cloneFunc = null;
        if (mSortFunc != null) {
            cloneFunc = mSortFunc.clone();
        }
        // Don't set inputs as PhysicalPlan.clone will take care of that
        return new POSort(new OperatorKey(mKey.scope, 
            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
            requestedParallelism, null, clonePlans, cloneAsc, cloneFunc);
    }



}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashSet;
import java.util.Hashtable;
import java.util.List;
import java.util.Set;
import java.util.Map.Entry;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.PigException;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.optimizer.SchemaRemover;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;

/**
 * This is the logical operator for the Fragment Replicate Join
 * It holds the user specified information and is responsible for 
 * the schema computation. This mimics the LOCogroup operator except
 * the schema computation.
 */
public class LOFRJoin extends LogicalOperator {
    private static final long serialVersionUID = 2L;
    
//    private boolean[] mIsInner;
    private static Log log = LogFactory.getLog(LOFRJoin.class);
    private MultiMap<LogicalOperator, LogicalPlan> mJoinColPlans;
    private LogicalOperator fragOp;
    
    public LOFRJoin(
            LogicalPlan plan,
            OperatorKey k,
            MultiMap<LogicalOperator, LogicalPlan> joinColPlans,
            boolean[] isInner, LogicalOperator fragOp) {
        super(plan, k);
        mJoinColPlans = joinColPlans;
//        mIsInner = isInner;
        this.fragOp = fragOp;
    }

    @Override
    /**
     * Uses the schema from its input operators and dedups
     * those fields that have the same alias and sets the
     * schema for the join
     */
    public Schema getSchema() throws FrontendException {
        List<LogicalOperator> inputs = mPlan.getPredecessors(this);
        mType = DataType.BAG;//mType is from the super class
        Hashtable<String, Integer> nonDuplicates = new Hashtable<String, Integer>();
        if(!mIsSchemaComputed){
            List<Schema.FieldSchema> fss = new ArrayList<Schema.FieldSchema>();
            int i=-1;
            for (LogicalOperator op : inputs) {
                try {
                    Schema cSchema = op.getSchema();
                    if(cSchema!=null){
                        
                        for (FieldSchema schema : cSchema.getFields()) {
                            ++i;
                            if(nonDuplicates.containsKey(schema.alias))
                                {
                                    if(nonDuplicates.get(schema.alias)!=-1) {
                                        nonDuplicates.remove(schema.alias);
                                        nonDuplicates.put(schema.alias, -1);
                                    }
                                }
                            else
                                nonDuplicates.put(schema.alias, i);
                            FieldSchema newFS = new FieldSchema(op.getAlias()+"::"+schema.alias,schema.schema,schema.type);
                            newFS.setParent(schema.canonicalName, op);
                            fss.add(newFS);
                        }
                    }
                    else
                        fss.add(new FieldSchema(null,DataType.BYTEARRAY));
                } catch (FrontendException ioe) {
                    mIsSchemaComputed = false;
                    mSchema = null;
                    throw ioe;
                }
            }
            mIsSchemaComputed = true;
            for (Entry<String, Integer> ent : nonDuplicates.entrySet()) {
                int ind = ent.getValue();
                if(ind==-1) continue;
                FieldSchema prevSch = fss.get(ind);
                fss.set(ind, new FieldSchema(ent.getKey(),prevSch.schema,prevSch.type));
            }
            mSchema = new Schema(fss);
        }
        return mSchema;
    }

    public MultiMap<LogicalOperator, LogicalPlan> getJoinColPlans() {
        return mJoinColPlans;
    }
    
    public void switchJoinColPlanOp(LogicalOperator oldOp,
            LogicalOperator newOp) {
        Collection<LogicalPlan> innerPlans = mJoinColPlans.removeKey(oldOp) ;
        mJoinColPlans.put(newOp, innerPlans);
        if(fragOp.getOperatorKey().equals(oldOp.getOperatorKey()))
            fragOp = newOp;
    }
    
    public void unsetSchema() throws VisitorException{
        for(LogicalOperator input: getInputs()) {
            Collection<LogicalPlan> grpPlans = mJoinColPlans.get(input);
            if(grpPlans!=null)
                for(LogicalPlan plan : grpPlans) {
                    SchemaRemover sr = new SchemaRemover(plan);
                    sr.visit();
                }
        }
        super.unsetSchema();
    }
    
    public List<LogicalOperator> getInputs() {
        return mPlan.getPredecessors(this);
    }
    
    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "FRJoin " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    public LogicalOperator getFragOp() {
        return fragOp;
    }

    public void setFragOp(LogicalOperator fragOp) {
        this.fragOp = fragOp;
    }
    
    public boolean isTupleJoinCol() {
        List<LogicalOperator> inputs = mPlan.getPredecessors(this);
        if (inputs == null || inputs.size() == 0) {
            throw new AssertionError("join.isTuplejoinCol() can be called "
                                     + "after it has an input only") ;
        }
		// NOTE: we depend on the number of inner plans to determine
		// if the join col is a tuple. This could be an issue when there
		// is only one inner plan with Project(*). For that case if the
		// corresponding input to the Project had a schema then the front end 
		// would translate the single Project(*) (through ProjectStarTranslator)
		// to many individual Projects. So the number of inner plans would then 
		// be > 1 BEFORE reaching here. For the Project(*) case when the corresponding
		// input for the Project has no schema, treating it as an atomic col join
		// does not cause any problems since no casts need to be inserted in that case
		// anyway.
        return mJoinColPlans.get(inputs.get(0)).size() > 1 ;
    }
    public byte getAtomicJoinColType() throws FrontendException {
        if (isTupleJoinCol()) {
            int errCode = 1010;
            String msg = "getAtomicGroupByType is used only when"
                + " dealing with atomic join col";
            throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
        }

        byte joinColType = DataType.BYTEARRAY ;
        // merge all the inner plan outputs so we know what type
        // our join column should be
        for(int i=0;i < getInputs().size(); i++) {
            LogicalOperator input = getInputs().get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(getJoinColPlans().get(input)) ;
            if (innerPlans.size() != 1) {
                int errCode = 1012;
                String msg = "Each join input has to have "
                + "the same number of inner plans";
                throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
            }
            byte innerType = innerPlans.get(0).getSingleLeafPlanOutputType() ;
            joinColType = DataType.mergeType(joinColType, innerType) ;
        }

        return joinColType ;
    }

    public Schema getTupleJoinColSchema() throws FrontendException {
        if (!isTupleJoinCol()) {
            int errCode = 1011;
            String msg = "getTupleGroupBySchema is used only when"
                + " dealing with tuple join col";
            throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
        }

        // this fsList represents all the columns in join tuple
        List<Schema.FieldSchema> fsList = new ArrayList<Schema.FieldSchema>() ;

        int outputSchemaSize = getJoinColPlans().get(getInputs().get(0)).size() ;

        // by default, they are all bytearray
        // for type checking, we don't care about aliases
        for(int i=0; i<outputSchemaSize; i++) {
            fsList.add(new Schema.FieldSchema(null, DataType.BYTEARRAY)) ;
        }

        // merge all the inner plan outputs so we know what type
        // our join column should be
        for(int i=0;i < getInputs().size(); i++) {
            LogicalOperator input = getInputs().get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(getJoinColPlans().get(input)) ;

            boolean seenProjectStar = false;
            for(int j=0;j < innerPlans.size(); j++) {
                byte innerType = innerPlans.get(j).getSingleLeafPlanOutputType() ;
                ExpressionOperator eOp = (ExpressionOperator)innerPlans.get(j).getSingleLeafPlanOutputOp();

                if(eOp instanceof LOProject) {
                    if(((LOProject)eOp).isStar()) {
                        seenProjectStar = true;
                    }
                }
                        
                Schema.FieldSchema joinFs = fsList.get(j);
                joinFs.type = DataType.mergeType(joinFs.type, innerType) ;
                Schema.FieldSchema fs = eOp.getFieldSchema();
                if(null != fs) {
                    joinFs.setParent(eOp.getFieldSchema().canonicalName, eOp);
                } else {
                    joinFs.setParent(null, eOp);
                }
            }

            if(seenProjectStar && innerPlans.size() > 1) {                
                int errCode = 1013;
                String msg = "Join attributes can either be star (*) or a list of expressions, but not both.";
                throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
                
            }

        }

        return new Schema(fsList) ;
    }

    @Override
    public ProjectionMap getProjectionMap() {
        Schema outputSchema;
        
        try {
            outputSchema = getSchema();
        } catch (FrontendException fee) {
            return null;
        }
        
        if(outputSchema == null) {
            return null;
        }
        
        List<LogicalOperator> predecessors = (ArrayList<LogicalOperator>)mPlan.getPredecessors(this);
        if(predecessors == null) {
            return null;
        }
        
        MultiMap<Integer, Pair<Integer, Integer>> mapFields = new MultiMap<Integer, Pair<Integer, Integer>>();
        List<Integer> addedFields = new ArrayList<Integer>();
        boolean[] unknownSchema = new boolean[predecessors.size()];
        boolean anyUnknownInputSchema = false;
        int outputColumnNum = 0;
        
        for(int inputNum = 0; inputNum < predecessors.size(); ++inputNum) {
            LogicalOperator predecessor = predecessors.get(inputNum);
            Schema inputSchema = null;        
            
            try {
                inputSchema = predecessor.getSchema();
            } catch (FrontendException fee) {
                return null;
            }
            
            if(inputSchema == null) {
                unknownSchema[inputNum] = true;
                outputColumnNum++;
                addedFields.add(inputNum);
                anyUnknownInputSchema = true;
            } else {
                unknownSchema[inputNum] = false;
                for(int inputColumn = 0; inputColumn < inputSchema.size(); ++inputColumn) {
                    mapFields.put(outputColumnNum++, new Pair<Integer, Integer>(inputNum, inputColumn));
                }
            }
        }
        
        //TODO
        /*
         * For now, if there is any input that has an unknown schema
         * flag it and return a null ProjectionMap.
         * In the future, when unknown schemas are handled
         * mark inputs that have unknown schemas as output columns
         * that have been added.
         */

        if(anyUnknownInputSchema) {
            return null;
        }
        
        if(addedFields.size() == 0) {
            addedFields = null;
        }

        return new ProjectionMap(mapFields, null, addedFields);
    }

    @Override
    public List<RequiredFields> getRequiredFields() {        
        List<LogicalOperator> predecessors = mPlan.getPredecessors(this);
        
        if(predecessors == null) {
            return null;
        }
        
        List<RequiredFields> requiredFields = new ArrayList<RequiredFields>();
        
        for(int inputNum = 0; inputNum < predecessors.size(); ++inputNum) {
            Set<Pair<Integer, Integer>> fields = new HashSet<Pair<Integer, Integer>>();
            Set<LOProject> projectSet = new HashSet<LOProject>();
            boolean groupByStar = false;

            for (LogicalPlan plan : this.getJoinColPlans().get(predecessors.get(inputNum))) {
                TopLevelProjectFinder projectFinder = new TopLevelProjectFinder(plan);
                try {
                    projectFinder.visit();
                } catch (VisitorException ve) {
                    requiredFields.clear();
                    requiredFields.add(null);
                    return requiredFields;
                }
                projectSet.addAll(projectFinder.getProjectSet());
                if(projectFinder.getProjectStarSet() != null) {
                    groupByStar = true;
                }
            }

            if(groupByStar) {
                requiredFields.add(new RequiredFields(true));
            } else {                
                for (LOProject project : projectSet) {
                    for (int inputColumn : project.getProjection()) {
                        fields.add(new Pair<Integer, Integer>(inputNum, inputColumn));
                    }
                }
        
                if(fields.size() == 0) {
                    requiredFields.add(new RequiredFields(false, true));
                } else {                
                    requiredFields.add(new RequiredFields(new ArrayList<Pair<Integer, Integer>>(fields)));
                }
            }
        }
        
        return (requiredFields.size() == 0? null: requiredFields);
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.plan;

import java.lang.StringBuilder;
import java.util.List;

import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;

/**
 * A struct detailing how a projection is altered by an operator.
 */
public class ProjectionMap {
    /**
     * Quick way for an operator to note that its input and output are the same.
     */
    private boolean mChanges = true;

    /**
     * Map of field changes, with keys being the output fields of the operator
     * and values being the input fields. Fields are numbered from 0. So for a
     * foreach operator derived from 'B = foreach A generate $0, $2, $3,
     * udf($1)' would produce a mapping of 0->(0, 0), 2->(0, 1), 3->(0, 2)
     */
    private MultiMap<Integer, Pair<Integer, Integer>> mMappedFields;

    /**
     * List of fields removed from the input. This includes fields that were
     * transformed, and thus are no longer the same fields. Using the example
     * foreach given under mappedFields, this list would contain '(0,1)'.
     */
    private List<Pair<Integer, Integer>> mRemovedFields;

    /**
     * List of fields in the output of this operator that were created by this
     * operator. Using the example foreach given under mappedFields, this list
     * would contain '3'.
     */
    private List<Integer> mAddedFields;

    /**
     * 
     * @param changes
     *            to indicate if this projection map changes its input or not
     */
    public ProjectionMap(boolean changes) {
        this(null, null, null, changes);
    }

    /**
     * 
     * @param mapFields
     *            the mapping of input column to output column
     * @param removedFields
     *            the list of input columns that are removed
     * @param addedFields
     *            the list of columns that are added to the output
     */
    public ProjectionMap(MultiMap<Integer, Pair<Integer, Integer>> mapFields,
            List<Pair<Integer, Integer>> removedFields,
            List<Integer> addedFields) {
        this(mapFields, removedFields, addedFields, true);
    }

    /**
     * 
     * @param mapFields
     *            the mapping of input column to output column
     * @param removedFields
     *            the list of input columns that are removed
     * @param addedFields
     *            the list of columns that are added to the output
     * @param changes
     *            to indicate if this projection map changes its input or not
     */
    private ProjectionMap(MultiMap<Integer, Pair<Integer, Integer>> mapFields,
            List<Pair<Integer, Integer>> removedFields,
            List<Integer> addedFields, boolean changes) {
        mMappedFields = mapFields;
        mAddedFields = addedFields;
        mRemovedFields = removedFields;
        mChanges = changes;
    }

    /**
     * 
     * @return the mapping of input column to output column
     */
    public MultiMap<Integer, Pair<Integer, Integer>> getMappedFields() {
        return mMappedFields;
    }

    /**
     * 
     * @param fields
     *            the mapping of input column to output column
     */
    public void setMappedFields(MultiMap<Integer, Pair<Integer, Integer>> fields) {
        mMappedFields = fields;
    }

    /**
     * 
     * @return the list of input columns that are removed
     */
    public List<Pair<Integer, Integer>> getRemovedFields() {
        return mRemovedFields;
    }

    /**
     * 
     * @param fields
     *            the list of input columns that are removed
     */
    public void setRemovedFields(List<Pair<Integer, Integer>> fields) {
        mRemovedFields = fields;
    }

    /**
     * 
     * @return the list of columns that are added to the output
     */
    public List<Integer> getAddedFields() {
        return mAddedFields;
    }

    /**
     * 
     * @param fields
     *            the list of columns that are added to the output
     */
    public void setAddedFields(List<Integer> fields) {
        mAddedFields = fields;
    }

    /**
     * 
     * @return if this projection map changes its input or not
     */
    public boolean changes() {
        return getChanges();
    }


    /**
     * 
     * @return if this projection map changes its input or not
     */
    public boolean getChanges() {
        return mChanges;
    }

    /**
     * 
     * @param changes
     *            if this projection map changes its input or not
     */
    public void setChanges(boolean changes) {
        mChanges = changes;
    }
    
    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("changes: " + mChanges);
        sb.append(" mapped fields: " + mMappedFields);
        sb.append(" added fields: " + mAddedFields);
        sb.append(" removed fields: " + mRemovedFields);
        return sb.toString();
    }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.plan;

import java.io.PrintStream;
import java.util.LinkedList;
import java.util.Collection;
import org.apache.pig.impl.util.MultiMap;

/**
 * This class dumps a nested plan to a print stream. It does not walk
 * the graph in any particular fashion it merely iterates over all
 * operators and edges and calls a corresponding dump function. If a
 * node of the plan has nested plans this will be dumped when the
 * node is handled.
 */
public class PlanDumper<E extends Operator, 
                        P extends OperatorPlan<E>, 
                        S extends OperatorPlan<? extends Operator>> {
    
    protected PrintStream ps;
    protected P plan;
    protected boolean isVerbose = true;
  
    public PlanDumper(P plan, PrintStream ps) {
        this.plan = plan;
        this.ps = ps;
    }

    public void setVerbose(boolean verbose) {
        this.isVerbose = verbose;
    }

    public boolean isVerbose() {
        return isVerbose;
    }

    /**
     * This is the public interface. Dump writes the plan and nested
     * plans to the stream.
     */
    public void dump() {
        for (E op: plan) {
            MultiMap<E,S> map = getMultiInputNestedPlans(op);
            if (isVerbose && !map.isEmpty()) {
                dumpMultiInputNestedOperator(op, map);
                continue;
            }

            Collection<S> plans = getMultiOutputNestedPlans(op);
            if (plans.size() > 0) {
                dumpMultiOutputNestedOperator(op, plans);
                continue;
            }
            
            plans = getNestedPlans(op);
            if (isVerbose && plans.size() > 0) {
                dumpNestedOperator(op, plans);
                continue;
            }

            dumpOperator(op);
        }

        for(E op: plan) {
            Collection<E> successors = plan.getSuccessors(op);
            if (successors != null) {
                for (E suc: successors) {
                    dumpEdge(op, suc);
                }
            }
        }
    }

    /**
     * makeDumper is a factory method. Used by subclasses to specify
     * what dumper should handle the nested plan.
     * @param plan Plan that the new dumper should handle
     * @return the dumper for plan
     */
    protected PlanDumper makeDumper(S plan, PrintStream ps) {
        return new PlanDumper(plan, ps);
    }

    /**
     * Will be called to dump a simple operator
     * @param op the operator to be dumped
     */
    protected void dumpOperator(E op) {
        ps.println(op.name().replace(" ","_"));
    }

    /**
     * Will be called when an operator has nested plans, which are
     * connected to one of the multiple inputs.
     * @param op the nested operator
     * @param plans a map of input operator to connected nested plan
     */
    protected void dumpMultiInputNestedOperator(E op, MultiMap<E,S> plans) {
        dumpOperator(op);
        for (E aop: plans.keySet()) {
            for (S plan: plans.get(aop)) {
                PlanDumper dumper = makeDumper(plan, ps);
                dumper.dump();
            }
        }
    }

    /**
     * Will be called for nested operators, where the plans represent
     * how the output of the operator is processed. 
     * @param op the nested operator
     * @param plans a collection of sub plans.
     */
    protected void dumpMultiOutputNestedOperator(E op, Collection<S> plans) {
        dumpOperator(op);
        for (S plan: plans) {
            PlanDumper  dumper = makeDumper(plan, ps);
            dumper.dump();
            for (Operator p: plan.getRoots()) {
                dumpEdge(op, p);
            }
        }
    }

    /**
     * Will be called for nested operators. The operators are not
     * specifically connected to any input or output operators of E
     * @param op the nested operator
     * @param plans a collection of sub plans.
     */
    protected void dumpNestedOperator(E op, Collection<S> plans) {
        dumpOperator(op);
        for (S plan: plans) {
            PlanDumper  dumper = makeDumper(plan, ps);
            dumper.dump();
        }
    }

    /**
     * Will be called to dump the edges of the plan. Each edge results
     * in one call.
     * @param op tail of the edge
     * @param suc head of the edge
     */
    protected void dumpEdge(Operator op, Operator suc) {
        ps.println(op.name()+" -> "+suc.name());
    }

    /**
     * Used to determine if an operator has nested plans, which are
     * connected to specific input operators.
     * @param op operator
     * @return Map describing the input to nested plan relationship.
     */
    protected MultiMap<E, S> getMultiInputNestedPlans(E op) {
        return new MultiMap<E, S>();
    }

    /**
     * Used to determine if an operator has nested output plans
     *
     * @param op operator
     * @return Map describing the input to nested plan relationship.
     */
    protected Collection<S> getMultiOutputNestedPlans(E op) {
        return new LinkedList<S>();
    }

    /**
     * Used to determine if an operator has nested plans (without
     * connections to in- or output operators.
     * @param op operator
     * @return Collection of nested plans.
     */
    protected Collection<S> getNestedPlans(E op) {
        return new LinkedList<S>();
    }

    /**
     * Helper function to print a string array.
     * @param sep Separator
     * @param strings Array to print
     */
    protected void join(String sep, String[] strings) {
        if (strings == null) {
            return;
        }
        
        for (int i = 0; i < strings.length; ++i) {
            if (i != 0) {
                ps.print(sep);
            }
            ps.print(strings[i]);
        }
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import static java.util.regex.Matcher.quoteReplacement;

import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import junit.framework.Assert;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.pig.PigServer;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.parser.QueryParser;
import org.apache.pig.impl.logicalLayer.schema.Schema;

public class Util {
    private static BagFactory mBagFactory = BagFactory.getInstance();
    private static TupleFactory mTupleFactory = TupleFactory.getInstance();

    // Helper Functions
    // =================
    static public Tuple loadFlatTuple(Tuple t, int[] input) throws ExecException {
        for (int i = 0; i < input.length; i++) {
            t.set(i, new Integer(input[i]));
        }
        return t;
    }

    static public Tuple loadTuple(Tuple t, String[] input) throws ExecException {
        for (int i = 0; i < input.length; i++) {
            t.set(i, input[i]);
        }
        return t;
    }

    static public Tuple loadTuple(Tuple t, DataByteArray[] input) throws ExecException {
        for (int i = 0; i < input.length; i++) {
            t.set(i, input[i]);
        }
        return t;
    }

    static public Tuple loadNestTuple(Tuple t, int[] input) throws ExecException {
        DataBag bag = BagFactory.getInstance().newDefaultBag();
        for(int i = 0; i < input.length; i++) {
            Tuple f = TupleFactory.getInstance().newTuple(1);
            f.set(0, input[i]);
            bag.add(f);
        }
        t.set(0, bag);
        return t;
    }

    static public Tuple loadNestTuple(Tuple t, long[] input) throws ExecException {
        DataBag bag = BagFactory.getInstance().newDefaultBag();
        for(int i = 0; i < input.length; i++) {
            Tuple f = TupleFactory.getInstance().newTuple(1);
            f.set(0, new Long(input[i]));
            bag.add(f);
        }
        t.set(0, bag);
        return t;
    }

    // this one should handle String, DataByteArray, Long, Integer etc..
    static public <T> Tuple loadNestTuple(Tuple t, T[] input) throws ExecException {
        DataBag bag = BagFactory.getInstance().newDefaultBag();
        for(int i = 0; i < input.length; i++) {
            Tuple f = TupleFactory.getInstance().newTuple(1);
            f.set(0, input[i]);
            bag.add(f);
        }
        t.set(0, bag);
        return t;
    }

    static public <T>void addToTuple(Tuple t, T[] b)
    {
        for(int i = 0; i < b.length; i++)
            t.append(b[i]);
    }
    
    
    
    static public <T>Tuple createTuple(T[] s)
    {
        Tuple t = mTupleFactory.newTuple();
        addToTuple(t, s);
        return t;
    }
    
    static public DataBag createBag(Tuple[] t)
    {
        DataBag b = mBagFactory.newDefaultBag();
        for(int i = 0; i < t.length; i++)b.add(t[i]);
        return b;
    }
    
    static public<T> DataBag createBagOfOneColumn(T[] input) throws ExecException {
        DataBag result = mBagFactory.newDefaultBag();
        for (int i = 0; i < input.length; i++) {
            Tuple t = mTupleFactory.newTuple(1);
            t.set(0, input[i]);
            result.add(t);
        }
        return result;
    }
    
    static public Map<Object, Object> createMap(String[] contents)
    {
        Map<Object, Object> m = new HashMap<Object, Object>();
        for(int i = 0; i < contents.length; ) {
            m.put(contents[i], contents[i+1]);
            i += 2;
        }
        return m;
    }

    static public<T> DataByteArray[] toDataByteArrays(T[] input) {
        DataByteArray[] dbas = new DataByteArray[input.length];
        for (int i = 0; i < input.length; i++) {
            dbas[i] = (input[i] == null)?null:new DataByteArray(input[i].toString().getBytes());
        }        
        return dbas;
    }
    
    static public Tuple loadNestTuple(Tuple t, int[][] input) throws ExecException {
        for (int i = 0; i < input.length; i++) {
            DataBag bag = BagFactory.getInstance().newDefaultBag();
            Tuple f = loadFlatTuple(TupleFactory.getInstance().newTuple(input[i].length), input[i]);
            bag.add(f);
            t.set(i, bag);
        }
        return t;
    }

    static public Tuple loadTuple(Tuple t, String[][] input) throws ExecException {
        for (int i = 0; i < input.length; i++) {
            DataBag bag = BagFactory.getInstance().newDefaultBag();
            Tuple f = loadTuple(TupleFactory.getInstance().newTuple(input[i].length), input[i]);
            bag.add(f);
            t.set(i, bag);
        }
        return t;
    }

    /**
     * Helper to create a temporary file with given input data for use in test cases.
     *  
     * @param tmpFilenamePrefix file-name prefix
     * @param tmpFilenameSuffix file-name suffix
     * @param inputData input for test cases, each string in inputData[] is written
     *                  on one line
     * @return {@link File} handle to the created temporary file
     * @throws IOException
     */
	static public File createInputFile(String tmpFilenamePrefix, 
			                           String tmpFilenameSuffix, 
			                           String[] inputData) 
	throws IOException {
		File f = File.createTempFile(tmpFilenamePrefix, tmpFilenameSuffix);
        f.deleteOnExit();
		PrintWriter pw = new PrintWriter(new OutputStreamWriter(new FileOutputStream(f), "UTF-8"));
		for (int i=0; i<inputData.length; i++){
			pw.println(inputData[i]);
		}
		pw.close();
		return f;
	}
	
	/**
     * Helper to create a dfs file on the Minicluster DFS with given
     * input data for use in test cases.
     * 
     * @param miniCluster reference to the Minicluster where the file should be created
     * @param fileName pathname of the file to be created
     * @param inputData input for test cases, each string in inputData[] is written
     *                  on one line
     * @throws IOException
     */
    static public void createInputFile(MiniCluster miniCluster, String fileName, 
                                       String[] inputData) 
    throws IOException {
        FileSystem fs = miniCluster.getFileSystem();
        if(fs.exists(new Path(fileName))) {
            throw new IOException("File " + fileName + " already exists on the minicluster");
        }
        FSDataOutputStream stream = fs.create(new Path(fileName));
        PrintWriter pw = new PrintWriter(new OutputStreamWriter(stream, "UTF-8"));
        for (int i=0; i<inputData.length; i++){
            pw.println(inputData[i]);
        }
        pw.close();
    }
    
    /**
     * Helper to create a dfs file on the MiniCluster dfs. This returns an
     * outputstream that can be used in test cases to write data.
     * 
     * @param cluster
     *            reference to the MiniCluster where the file should be created
     * @param fileName
     *            pathname of the file to be created
     * @return OutputStream to write any data to the file created on the
     *         MiniCluster.
     * @throws IOException
     */
    static public OutputStream createInputFile(MiniCluster cluster,
            String fileName) throws IOException {
        FileSystem fs = cluster.getFileSystem();
        if (fs.exists(new Path(fileName))) {
            throw new IOException("File " + fileName
                    + " already exists on the minicluster");
        }
        return fs.create(new Path(fileName));
    }
    
    /**
     * Helper to remove a dfs file from the minicluster DFS
     * 
     * @param miniCluster reference to the Minicluster where the file should be deleted
     * @param fileName pathname of the file to be deleted
     * @throws IOException
     */
    static public void deleteFile(MiniCluster miniCluster, String fileName) 
    throws IOException {
        FileSystem fs = miniCluster.getFileSystem();
        fs.delete(new Path(fileName), true);
    }

	/**
	 * Helper function to check if the result of a Pig Query is in line with 
	 * expected results.
	 * 
	 * @param actualResults Result of the executed Pig query
	 * @param expectedResults Expected results to validate against
	 */
	static public void checkQueryOutputs(Iterator<Tuple> actualResults, 
			                        Tuple[] expectedResults) {
	    
		for (Tuple expected : expectedResults) {
			Tuple actual = actualResults.next();
			Assert.assertEquals(expected, actual);
		}
	}

	/**
	 * Utility method to copy a file form local filesystem to the dfs on
	 * the minicluster for testing in mapreduce mode
	 * @param cluster a reference to the minicluster
	 * @param localFileName the pathname of local file
	 * @param fileNameOnCluster the name with which the file should be created on the minicluster
	 * @throws IOException
	 */
	static public void copyFromLocalToCluster(MiniCluster cluster, String localFileName, String fileNameOnCluster) throws IOException {
	    BufferedReader reader = new BufferedReader(new FileReader(localFileName));
	    String line = null;
	    List<String> contents = new ArrayList<String>();
	    while((line = reader.readLine()) != null) {
	        contents.add(line);
	    }
	    Util.createInputFile(cluster, fileNameOnCluster, contents.toArray(new String[0]));
	}
	
	static public void printQueryOutput(Iterator<Tuple> actualResults, 
               Tuple[] expectedResults) {

	    System.out.println("Expected :") ;
        for (Tuple expected : expectedResults) {
            System.out.println(expected.toString()) ;
        }
	    System.out.println("---End----") ;
	    
        System.out.println("Actual :") ;
        while (actualResults.hasNext()) {
            System.out.println(actualResults.next().toString()) ;
        }
        System.out.println("---End----") ;
    }

	/**
     * Helper method to replace all occurrences of "\" with "\\" in a 
     * string. This is useful to fix the file path string on Windows
     * where "\" is used as the path separator.
     * 
     * @param str Any string
     * @return The resulting string
     */
	public static String encodeEscape(String str) {
	    String regex = "\\\\";
	    String replacement = quoteReplacement("\\\\");
	    return str.replaceAll(regex, replacement);
	}
	
	   /**
     * Helper method to construct URI for local file system. For unix, it will
     * put "file:" in the front of the path; For Windows, it will put "file:/" in 
     * front of the path, and also call encodeEscape to replace "\" with "\\"
     * 
     * @param str absolute path (under cygwin, should be a windows style path)
     * @return The resulting string
     */
    public static String generateURI(String path)
    {
        if (System.getProperty("os.name").toUpperCase().startsWith("WINDOWS"))
            return "file:/"+encodeEscape(path);
        return "file:"+path;
    }

    public static Schema getSchemaFromString(String schemaString) throws ParseException {
        return Util.getSchemaFromString(schemaString, DataType.BYTEARRAY);
    }

    static Schema getSchemaFromString(String schemaString, byte defaultType) throws ParseException {
        ByteArrayInputStream stream = new ByteArrayInputStream(schemaString.getBytes()) ;
        QueryParser queryParser = new QueryParser(stream) ;
        Schema schema = queryParser.TupleSchema() ;
        Schema.setSchemaDefaultType(schema, defaultType);
        return schema;
    }
    
    static Object getPigConstant(String pigConstantAsString) throws ParseException {
        ByteArrayInputStream stream = new ByteArrayInputStream(pigConstantAsString.getBytes()) ;
        QueryParser queryParser = new QueryParser(stream) ;
        return queryParser.Datum();
    }

    public static File createFile(String[] data) throws Exception{
        File f = File.createTempFile("tmp", "");
        PrintWriter pw = new PrintWriter(f);
        for (int i=0; i<data.length; i++){
            pw.println(data[i]);
        }
        pw.close();
        return f;
    }
    
    public static PhysicalPlan buildPhysicalPlan(LogicalPlan lp, PigContext pc) throws Exception {
    	LogToPhyTranslationVisitor visitor = new LogToPhyTranslationVisitor(lp);
    	visitor.setPigContext(pc);
    	visitor.visit();
    	return visitor.getPhysicalPlan();
    }
    
    public static MROperPlan buildMRPlan(PhysicalPlan pp, PigContext pc) throws Exception{
        MRCompiler comp = new MRCompiler(pp, pc);
        comp.compile();
        return comp.getMRPlan();	
    }
    
    public static void registerMultiLineQuery(PigServer pigServer, String query) throws IOException {
        File f = File.createTempFile("tmp", "");
        PrintWriter pw = new PrintWriter(f);
        pw.println(query);
        pw.close();
        pigServer.registerScript(f.getCanonicalPath());
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.pen.util;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.pig.impl.logicalLayer.LOLoad;
import org.apache.pig.impl.logicalLayer.LOStream;
import org.apache.pig.impl.logicalLayer.LogicalOperator;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.impl.logicalLayer.optimizer.ImplicitSplitInserter;
import org.apache.pig.impl.logicalLayer.optimizer.OpLimitOptimizer;
import org.apache.pig.impl.logicalLayer.optimizer.StreamOptimizer;
import org.apache.pig.impl.logicalLayer.optimizer.TypeCastInserter;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.optimizer.PlanOptimizer;
import org.apache.pig.impl.plan.optimizer.Rule;
import org.apache.pig.impl.plan.optimizer.RuleOperator;
import org.apache.pig.impl.plan.optimizer.RulePlan;
import org.apache.pig.impl.util.MultiMap;

//This optimiser puts in the bare minimum modifications needed to make sure the plan is functional
public class FunctionalLogicalOptimizer extends
        PlanOptimizer<LogicalOperator, LogicalPlan> {

    private static final String SCOPE = "RULE";
    private static NodeIdGenerator nodeIdGen = NodeIdGenerator.getGenerator();
    
    public FunctionalLogicalOptimizer(LogicalPlan plan) {
        super(plan);

        RulePlan rulePlan;        

        // List of rules for the logical optimizer

        // This one has to be first, as the type cast inserter expects the
        // load to only have one output.
        // Find any places in the plan that have an implicit split and make
        // it explicit. Since the RuleMatcher doesn't handle trees properly,
        // we cheat and say that we match any node. Then we'll do the actual
        // test in the transformers check method.
        
        rulePlan = new RulePlan();
        RuleOperator anyLogicalOperator = new RuleOperator(LogicalOperator.class, RuleOperator.NodeType.ANY_NODE, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(anyLogicalOperator);
        mRules.add(new Rule<LogicalOperator, LogicalPlan>(rulePlan,
                new ImplicitSplitInserter(plan), "ImplicitSplitInserter"));


        // Add type casting to plans where the schema has been declared (by
        // user, data, or data catalog).
        rulePlan = new RulePlan();
        RuleOperator loLoad = new RuleOperator(LOLoad.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(loLoad);
        mRules.add(new Rule<LogicalOperator, LogicalPlan>(rulePlan,
                new TypeCastInserter(plan, LOLoad.class.getName()), "LoadTypeCastInserter"));

        // Add type casting to plans where the schema has been declared by
        // user in a statement with stream operator.
        rulePlan = new RulePlan();
        RuleOperator loStream= new RuleOperator(LOStream.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(loStream);
        mRules.add(new Rule<LogicalOperator, LogicalPlan>(rulePlan, new TypeCastInserter(plan,
                LOStream.class.getName()), "StreamTypeCastInserter"));

    }

}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOLesserThanEqual extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOLesserThanEqual.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOLesserThanEqual(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "LesserThanEqual " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.plan;

import java.io.PrintStream;
import java.util.LinkedList;
import java.util.Collection;
import java.util.Set;
import java.util.HashSet;

import org.apache.pig.impl.util.MultiMap;

/**
 * This class puts everything that is needed to dump a plan in a
 * format readable by graphviz's dot algorithm. Out of the box it does
 * not print any nested plans.
 */
public class DotPlanDumper<E extends Operator, P extends OperatorPlan<E>, 
                           N extends Operator, S extends OperatorPlan<N>> 
    extends PlanDumper<E, P, S> {

    protected Set<Operator> mSubgraphs;
    protected Set<Operator> mMultiInputSubgraphs;    
    protected Set<Operator> mMultiOutputSubgraphs;
    private boolean isSubGraph = false;
  
    public DotPlanDumper(P plan, PrintStream ps) {
        this(plan, ps, false, new HashSet<Operator>(), new HashSet<Operator>(),
             new HashSet<Operator>());
    }

    protected DotPlanDumper(P plan, PrintStream ps, boolean isSubGraph, 
                            Set<Operator> mSubgraphs, 
                            Set<Operator> mMultiInputSubgraphs,
                            Set<Operator> mMultiOutputSubgraphs) {
        super(plan, ps);
        this.isSubGraph = isSubGraph;
        this.mSubgraphs = mSubgraphs;
        this.mMultiInputSubgraphs = mMultiInputSubgraphs;
        this.mMultiOutputSubgraphs = mMultiOutputSubgraphs;
    }

    @Override
    public void dump() {
        if (!isSubGraph) {
            ps.println("digraph plan {");
            ps.println("compound=true;");
            ps.println("node [shape=rect];");
        }
        super.dump();
        if (!isSubGraph) {
            ps.println("}");
        }
    }

    @Override
    protected void dumpMultiInputNestedOperator(E op, MultiMap<E, S> plans) {
        dumpInvisibleOutput(op);

        ps.print("subgraph ");
        ps.print(getClusterID(op));
        ps.println(" {");
        join("; ", getAttributes(op));
        ps.println("labelloc=b;");
        
        mMultiInputSubgraphs.add(op);

        for (E o: plans.keySet()) {
            ps.print("subgraph ");
            ps.print(getClusterID(op, o));
            ps.println(" {");
            ps.println("label=\"\";");
            dumpInvisibleInput(op, o);
            for (S plan : plans.get(o)) {
                PlanDumper dumper = makeDumper(plan, ps);
                dumper.dump();
                connectInvisibleInput(op, o, plan);
            }
            ps.println("};");
        }
        ps.println("};");
        
        for (E o: plans.keySet()) {
            for (S plan: plans.get(o)) {
                connectInvisibleOutput(op, plan);
            }
        }
    }

    @Override 
    protected void dumpMultiOutputNestedOperator(E op, Collection<S> plans) {
        super.dumpMultiOutputNestedOperator(op, plans);

        mMultiOutputSubgraphs.add(op);
        
        dumpInvisibleOutput(op);
        for (S plan: plans) {
            connectInvisibleOutput(op, plan);
        }
    }

    @Override
    protected void dumpNestedOperator(E op, Collection<S> plans) {
        dumpInvisibleOperators(op);
        ps.print("subgraph ");
        ps.print(getClusterID(op));
        ps.println(" {");
        join("; ", getAttributes(op));
        ps.println("labelloc=b;");

        mSubgraphs.add(op);
        
        for (S plan: plans) {
            PlanDumper dumper = makeDumper(plan, ps);
            dumper.dump();
            connectInvisibleInput(op, plan);
        }
        ps.println("};");

        for (S plan: plans) {
            connectInvisibleOutput(op, plan);
        }
    }

    @Override
    protected void dumpOperator(E op) {
        ps.print(getID(op));
        ps.print(" [");
        join(", ", getAttributes(op));
        ps.println("];");
    }

    @Override
    protected void dumpEdge(Operator op, Operator suc) {
        String in = getID(op);
        String out = getID(suc);
        String attributes = "";

        if (mMultiInputSubgraphs.contains(op) 
            || mSubgraphs.contains(op) 
            || mMultiOutputSubgraphs.contains(op)) {
            in = getSubgraphID(op, false);
        }

        ps.print(in);

        if (mMultiInputSubgraphs.contains(suc)) {
            out = getSubgraphID(suc, op, true);
            attributes = " [lhead="+getClusterID(suc,op)+"]";
        }

        if (mSubgraphs.contains(suc)) {
            out = getSubgraphID(suc, true);
            attributes = " [lhead="+getClusterID(suc)+"]";
        }
        
        ps.print(" -> ");
        ps.print(out);
        ps.println(attributes);
    }

    @Override
    protected PlanDumper makeDumper(S plan, PrintStream ps) {
        return new DotPlanDumper(plan, ps, true, 
                                 mSubgraphs, mMultiInputSubgraphs, 
                                 mMultiOutputSubgraphs);
    }

    /**
     * Used to generate the label for an operator.
     * @param op operator to dump
     */
    protected String getName(E op) {
        return op.name();
    }
    
    /**
     * Used to generate the the attributes of a node
     * @param op operator
     */
    protected String[] getAttributes(E op) {
        String[] attributes = new String[1];
        attributes[0] =  "label=\""+getName(op)+"\"";
        return attributes;
    }


    private void connectInvisibleInput(E op1, E op2, S plan) {
        String in = getSubgraphID(op1, op2, true);
        
        for (N l: plan.getRoots()) {
            dumpInvisibleEdge(in, getID(l));
        }
    }

    private void connectInvisibleInput(E op, S plan) {
        String in = getSubgraphID(op, true);

        for (N l: plan.getRoots()) {
            String out;
            if (mSubgraphs.contains(l) || mMultiInputSubgraphs.contains(l)) {
                out = getSubgraphID(l, true);
            } else {
                out = getID(l);
            }

            dumpInvisibleEdge(in, out);
        }
    }

    private void connectInvisibleOutput(E op, 
                                        OperatorPlan<? extends Operator> plan) {
        String out = getSubgraphID(op, false);

        for (Operator l: plan.getLeaves()) {
            String in;
            if (mSubgraphs.contains(l) 
                || mMultiInputSubgraphs.contains(l)
                || mMultiOutputSubgraphs.contains(l)) {
                in = getSubgraphID(l, false);
            } else {
                in = getID(l);
            }

            dumpInvisibleEdge(in, out);
        }
    }

    private void connectInvisible(E op, S plan) {
        connectInvisibleInput(op, plan);
        connectInvisibleOutput(op, plan);
    }        

    private void dumpInvisibleInput(E op1, E op2) {
        ps.print(getSubgraphID(op1, op2, true));
        ps.print(" ");
        ps.print(getInvisibleAttributes(op1));
        ps.println(";");
    }
    
    private void dumpInvisibleInput(E op) {
        ps.print(getSubgraphID(op, true));
        ps.print(" ");
        ps.print(getInvisibleAttributes(op));
        ps.println(";");
    }

    private void dumpInvisibleOutput(E op) {
        ps.print(getSubgraphID(op, false));
        ps.print(" ");
        ps.print(getInvisibleAttributes(op));
        ps.println(";");
    }

    protected void dumpInvisibleOperators(E op) {
        dumpInvisibleInput(op);
        dumpInvisibleOutput(op);
    }

    private String getClusterID(Operator op1, Operator op2) {
        return getClusterID(op1)+"_"+getID(op2);
    }

    private String getClusterID(Operator op) {
        return "cluster_"+getID(op);
    }

    private String getSubgraphID(Operator op1, Operator op2, boolean in) {
        String id = "s"+getID(op1)+"_"+getID(op2);
        if (in) {
            id += "_in";
        }
        else {
            id += "_out";
        }
        return id;
    }

    private String getSubgraphID(Operator op, boolean in) {
        String id =  "s"+getID(op);
        if (in) {
            id += "_in";
        }
        else {
            id += "_out";
        }
        return id;
    }

    private String getID(Operator op) {
        return ""+Math.abs(op.hashCode());
    }

    private String getInvisibleAttributes(Operator op) {
        return "[label=\"\", style=invis, height=0, width=0]";
    }
    
    private void dumpInvisibleEdge(String op, String suc) {
        ps.print(op);
        ps.print(" -> ");
        ps.print(suc);
        ps.println(" [style=invis];");
    }
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOOr extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOOr.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOOr(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }
    
    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Or " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.builtin;

import java.io.BufferedOutputStream;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URL;
import java.util.Iterator;
import java.util.Map;

import org.apache.commons.logging.LogFactory;
import org.apache.commons.logging.Log;
import org.apache.pig.ExecType;
import org.apache.pig.PigException;
import org.apache.pig.PigWarning;
import org.apache.pig.ReversibleLoadStoreFunc;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.ElementDescriptor;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataReaderWriter;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.io.BufferedPositionedInputStream;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.SchemaMergeException;
import org.apache.pig.impl.util.LogUtils;
import org.apache.pig.impl.util.WrappedIOException;


public class BinStorage implements ReversibleLoadStoreFunc {
    public static final byte RECORD_1 = 0x01;
    public static final byte RECORD_2 = 0x02;
    public static final byte RECORD_3 = 0x03;

    Iterator<Tuple>     i              = null;
    protected BufferedPositionedInputStream in = null;
    private static final Log mLog = LogFactory.getLog(BinStorage.class);
    private DataInputStream inData = null;
    protected long                end            = Long.MAX_VALUE;
    
    /**
     * Simple binary nested reader format
     */
    public BinStorage() {
    }

    public Tuple getNext() throws IOException {
        
        byte b = 0;
//      skip to next record
        while (true) {
            if (in == null || in.getPosition() >=end) {
                return null;
            }
            // check if we saw RECORD_1 in our last attempt
            // this can happen if we have the following 
            // sequence RECORD_1-RECORD_1-RECORD_2-RECORD_3
            // After reading the second RECORD_1 in the above
            // sequence, we should not look for RECORD_1 again
            if(b != RECORD_1) {
                b = (byte) in.read();
                if(b != RECORD_1 && b != -1) {
                    continue;
                }
                if(b == -1) return null;
            }
            b = (byte) in.read();
            if(b != RECORD_2 && b != -1) {
                continue;
            }
            if(b == -1) return null;
            b = (byte) in.read();
            if(b != RECORD_3 && b != -1) {
                continue;
            }
            if(b == -1) return null;
            b = (byte) in.read();
            if(b != DataType.TUPLE && b != -1) {
                continue;
            }
            if(b == -1) return null;
            break;
        }
        try {
            // if we got here, we have seen RECORD_1-RECORD_2-RECORD_3-TUPLE_MARKER
            // sequence - lets now read the contents of the tuple 
            return (Tuple)DataReaderWriter.readDatum(inData, DataType.TUPLE);
        } catch (ExecException ee) {
            throw ee;
        }
    }

    public void bindTo(String fileName, BufferedPositionedInputStream in, long offset, long end) throws IOException {
        this.in = in;
        inData = new DataInputStream(in);
        this.end = end;
    }


    DataOutputStream         out     = null;
  
    public void bindTo(OutputStream os) throws IOException {
        this.out = new DataOutputStream(new BufferedOutputStream(os));
    }

    public void finish() throws IOException {
        out.flush();
    }

    public void putNext(Tuple t) throws IOException {
        out.write(RECORD_1);
        out.write(RECORD_2);
        out.write(RECORD_3);
        t.write(out);
    }

    public DataBag bytesToBag(byte[] b){
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return DataReaderWriter.bytesToBag(dis);
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to bag, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }        
    }

    public String bytesToCharArray(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return DataReaderWriter.bytesToCharArray(dis);
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to chararray, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }
    }

    public Double bytesToDouble(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return new Double(dis.readDouble());
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to double, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }
    }

    public Float bytesToFloat(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return new Float(dis.readFloat());
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to float, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
            
            return null;
        }
    }

    public Integer bytesToInteger(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return new Integer(dis.readInt());
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to integer, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }
    }

    public Long bytesToLong(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return new Long(dis.readLong());
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to long, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }
    }

    public Map<Object, Object> bytesToMap(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return DataReaderWriter.bytesToMap(dis);
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to map, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }
    }

    public Tuple bytesToTuple(byte[] b) {
        DataInputStream dis = new DataInputStream(new ByteArrayInputStream(b));
        try {
            return DataReaderWriter.bytesToTuple(dis);
        } catch (IOException e) {
            LogUtils.warn(this, "Unable to convert bytearray to tuple, " +
                    "caught IOException <" + e.getMessage() + ">",
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, 
                    mLog);
        
            return null;
        }
    }

    /* (non-Javadoc)
     * @see org.apache.pig.LoadFunc#determineSchema(java.lang.String, org.apache.pig.ExecType, org.apache.pig.backend.datastorage.DataStorage)
     */
    public Schema determineSchema(String fileName, ExecType execType,
            DataStorage storage) throws IOException {

        if (!FileLocalizer.fileExists(fileName, storage)) {
            // At compile time in batch mode, the file may not exist
            // (such as intermediate file). Just return null - the
            // same way as we would if we did not get a valid record
            return null;
        }
        
        InputStream is = FileLocalizer.open(fileName, execType, storage);
       
        bindTo(fileName, new BufferedPositionedInputStream(is), 0, Long.MAX_VALUE);
        // get the first record from the input file
        // and figure out the schema from the data in
        // the first record
        Tuple t = getNext();
        is.close();
        if(t == null) {
            // we couldn't get a valid record from the input
            return null;
        }
        int numFields = t.size();
        Schema s = new Schema();
        for (int i = 0; i < numFields; i++) {
            try {
                s.add(DataType.determineFieldSchema(t.get(i)));
            } catch (Exception e) {
                int errCode = 2104;
                String msg = "Error while determining schema of BinStorage data.";
                throw new ExecException(msg, errCode, PigException.BUG, e);
            } 
        }
        return s;
    }

    public void fieldsToRead(Schema schema) {
        // TODO Auto-generated method stub
        
    }

    public byte[] toBytes(DataBag bag) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, bag);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting bag to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(String s) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, s);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting chararray to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(Double d) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, d);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting double to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(Float f) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, f);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting float to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(Integer i) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, i);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting int to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(Long l) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, l);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting long to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(Map<Object, Object> m) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, m);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting map to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }

    public byte[] toBytes(Tuple t) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        DataOutputStream dos = new DataOutputStream(baos);
        try {
            DataReaderWriter.writeDatum(dos, t);
        } catch (Exception ee) {
            int errCode = 2105;
            String msg = "Error while converting tuple to bytes.";
            throw new ExecException(msg, errCode, PigException.BUG, ee);
        }
        return baos.toByteArray();
    }
    public boolean equals(Object obj) {
        return true;
    }

    /* (non-Javadoc)
     * @see org.apache.pig.StoreFunc#getStorePreparationClass()
     */
    @Override
    public Class getStorePreparationClass() throws IOException {
        // TODO Auto-generated method stub
        return null;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.List;
import java.util.ArrayList;

import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;

public class LOUserFunc extends ExpressionOperator {
    private static final long serialVersionUID = 2L;

    private FuncSpec mFuncSpec;
    
    /**
     * @param plan
     *            LogicalPlan this operator is a part of.
     * @param k
     *            OperatorKey for this operator.
     * @param funcSpec
     *            name of the user defined function.
     * @param returnType
     *            return type of this function.
     */
    public LOUserFunc(LogicalPlan plan, OperatorKey k, FuncSpec funcSpec,
            byte returnType) {
        super(plan, k, -1);
        mFuncSpec = funcSpec;
        mType = returnType;
    }

    public FuncSpec getFuncSpec() {
        return mFuncSpec;
    }

    public List<ExpressionOperator> getArguments() {
        List<LogicalOperator> preds = getPlan().getPredecessors(this);
        List<ExpressionOperator> args = new ArrayList<ExpressionOperator>();
        if(preds == null)
                return args;
            
        for(LogicalOperator lo : preds){
            args.add((ExpressionOperator)lo);
        }
        return args;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    @Override
    public String name() {
        return "UserFunc " + mKey.scope + "-" + mKey.id + " function: " + mFuncSpec;
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            Schema inputSchema = new Schema();
            List<ExpressionOperator> args = getArguments();
            for(ExpressionOperator op: args) {
                if (!DataType.isUsableType(op.getType())) {
                    mFieldSchema = null;
                    mIsFieldSchemaComputed = false;
                    int errCode = 1014;
                    String msg = "Problem with input: " + op + " of User-defined function: " + this ;
                    throw new FrontendException(msg, errCode, PigException.INPUT, false, null) ;
                }
                inputSchema.add(op.getFieldSchema());    
            }
    
            EvalFunc<?> ef = (EvalFunc<?>) PigContext.instantiateFuncFromSpec(mFuncSpec);
            Schema udfSchema = ef.outputSchema(inputSchema);
            byte returnType = DataType.findType(ef.getReturnType());

            if (null != udfSchema) {
                Schema.FieldSchema fs;
//                try {
                    if(udfSchema.size() == 0) {
                        fs = new Schema.FieldSchema(null, null, returnType);
                    } else if(udfSchema.size() == 1) {
                        fs = new Schema.FieldSchema(udfSchema.getField(0));
                    } else {
                        fs = new Schema.FieldSchema(null, udfSchema, DataType.TUPLE);
                    }
//                } catch (ParseException pe) {
//                    throw new FrontendException(pe.getMessage());
//                }
                setType(fs.type);
                mFieldSchema = fs;
                mIsFieldSchemaComputed = true;
            } else {
                setType(returnType);
                mFieldSchema = new Schema.FieldSchema(null, null, returnType);
                mIsFieldSchemaComputed = true;
            }
        }
        return mFieldSchema;
    }


    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    /**
     * @param funcSpec the FuncSpec to set
     */
    public void setFuncSpec(FuncSpec funcSpec) {
        mFuncSpec = funcSpec;
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.ExpressionOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        LOUserFunc clone = (LOUserFunc)super.clone();
        //note that mFuncSpec cannot be null in LOUserFunc
        clone.mFuncSpec = mFuncSpec.clone();
        return clone;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import java.util.List;

import org.apache.pig.FuncSpec;
import org.apache.pig.LoadFunc;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class LOCast extends ExpressionOperator {

    // Cast has an expression that has to be converted to a specified type

    private static final long serialVersionUID = 2L;
    private FuncSpec mLoadFuncSpec = null;

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     * @param type
     *            the type to which the expression is cast
     */
    public LOCast(LogicalPlan plan, OperatorKey k, byte type) {
        super(plan, k);
        mType = type;
    }// End Constructor LOCast

    public ExpressionOperator getExpression() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(0);
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, mType);
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public String name() {
        return "Cast " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    public FuncSpec getLoadFuncSpec() {
        return mLoadFuncSpec;
    }

    public void setLoadFuncSpec(FuncSpec loadFuncSpec) {
        mLoadFuncSpec = loadFuncSpec;
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.ExpressionOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        LOCast clone = (LOCast)super.clone();
        if(mLoadFuncSpec != null) {
            clone.mLoadFuncSpec = mLoadFuncSpec.clone();
        }
        return clone;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators;

import java.util.Iterator;
import java.util.Map;
import java.util.ArrayList;

import org.apache.pig.PigException;
import org.apache.pig.PigWarning;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.SingleTupleBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.data.Tuple;

/**
 * Implements the overloaded form of the project operator.
 * Projects the specified column from the input tuple.
 * However, if asked for tuples when the input is a bag,
 * the overloaded form is invoked and the project streams
 * the tuples through instead of the bag.
 */
public class POProject extends ExpressionOperator {
    
    /**
     * 
     */
    private static final long serialVersionUID = 1L;

	private static TupleFactory tupleFactory = TupleFactory.getInstance();
	
	protected static BagFactory bagFactory = BagFactory.getInstance();

	private boolean resultSingleTupleBag = false;
	
    //The column to project
    protected ArrayList<Integer> columns;
    
    //True if we are in the middle of streaming tuples
    //in a bag
    boolean processingBagOfTuples = false;
    
    //The bag iterator used while straeming tuple
    Iterator<Tuple> bagIterator = null;
    
    //Represents the fact that this instance of POProject
    //is overloaded to stream tuples in the bag rather
    //than passing the entire bag. It is the responsibility
    //of the translator to set this.
    boolean overloaded = false;
    
    boolean star = false;
    
    public POProject(OperatorKey k) {
        this(k,-1,0);
    }

    public POProject(OperatorKey k, int rp) {
        this(k, rp, 0);
    }
    
    public POProject(OperatorKey k, int rp, int col) {
        super(k, rp);
        columns = new ArrayList<Integer>();
        columns.add(col);
    }

    public POProject(OperatorKey k, int rp, ArrayList<Integer> cols) {
        super(k, rp);
        columns = cols;
    }

    @Override
    public String name() {
        
        return "Project" + "[" + DataType.findTypeName(resultType) + "]" + ((star) ? "[*]" : columns) + " - " + mKey.toString();
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    @Override
    public boolean supportsMultipleOutputs() {
        return false;
    }

    @Override
    public void visit(PhyPlanVisitor v) throws VisitorException {
        v.visitProject(this);
    }
    
    
    /**
     * Overridden since the attachment of the new input
     * should cause the old processing to end.
     */
    @Override
    public void attachInput(Tuple t) {
        super.attachInput(t);
        processingBagOfTuples = false;
    }
    
    /**
     * Fetches the input tuple and returns the requested
     * column
     * @return next value.
     * @throws ExecException
     */
    public Result getNext() throws ExecException{
        Result res = processInput();
        Tuple inpValue = (Tuple)res.result;

        Object ret;
        if(res.returnStatus != POStatus.STATUS_OK){
            return res;
        }
        if (star) {
            return res;
        } else if(columns.size() == 1) {
            try {
                ret = inpValue.get(columns.get(0));
            } catch (ExecException ee) {
                if(pigLogger != null) {
                    pigLogger.warn(this,"Attempt to access field " + 
                            "which was not found in the input", PigWarning.ACCESSING_NON_EXISTENT_FIELD);
                }
                res.returnStatus = POStatus.STATUS_OK;
                ret = null;
            }
        } else {
	        ArrayList<Object> objList = new ArrayList<Object>(columns.size()); 
                
            for(int i: columns) {
                try { 
                    objList.add(inpValue.get(i)); 
                } catch (ExecException ee) {
                    if(pigLogger != null) {
                        pigLogger.warn(this,"Attempt to access field " + i +
                                " which was not found in the input", PigWarning.ACCESSING_NON_EXISTENT_FIELD);
                    }
                    objList.add(null);
                }
            }
		    ret = tupleFactory.newTuple(objList);
        }
        res.result = ret;
        return res;
    }

    @Override
    public Result getNext(DataBag db) throws ExecException {
        
        Result res = processInputBag();
        if(res.returnStatus!=POStatus.STATUS_OK)
            return res;
        return(consumeInputBag(res));
    }

    /**
     * @param input
     * @throws ExecException 
     */
    protected Result consumeInputBag(Result input) throws ExecException {
        DataBag inpBag = (DataBag) input.result;
        Result retVal = new Result();
        if(isInputAttached() || star){
            retVal.result = inpBag;
            retVal.returnStatus = POStatus.STATUS_OK;
            detachInput();
            return retVal;
        }
        
        DataBag outBag;
        if(resultSingleTupleBag) {
            // we have only one tuple in a bag - so create
            // A SingleTupleBag for the result and fill it
            // appropriately from the input bag
            Tuple tuple = inpBag.iterator().next();
            Tuple tmpTuple = tupleFactory.newTuple(columns.size());
            for (int i = 0; i < columns.size(); i++)
                tmpTuple.set(i, tuple.get(columns.get(i)));
            outBag = new SingleTupleBag(tmpTuple);
        } else {
            outBag = bagFactory.newDefaultBag();
            for (Tuple tuple : inpBag) {
                Tuple tmpTuple = tupleFactory.newTuple(columns.size());
                for (int i = 0; i < columns.size(); i++)
                    tmpTuple.set(i, tuple.get(columns.get(i)));
                outBag.add(tmpTuple);
            }
        }
        retVal.result = outBag;
        retVal.returnStatus = POStatus.STATUS_OK;
        return retVal;
    }

    @Override
    public Result getNext(DataByteArray ba) throws ExecException {
        return getNext();
    }

    @Override
    public Result getNext(Double d) throws ExecException {
        return getNext();
    }

    @Override
    public Result getNext(Float f) throws ExecException {
        return getNext();
    }

    @Override
    public Result getNext(Integer i) throws ExecException {
        return getNext();
    }

    @Override
    public Result getNext(Long l) throws ExecException {
        return getNext();
    }
    
    

    @Override
    public Result getNext(Boolean b) throws ExecException {
        return getNext();
    }

    @Override
    public Result getNext(Map m) throws ExecException {
        return getNext();
    }

    @Override
    public Result getNext(String s) throws ExecException {
        return getNext();
    }
    
    /**
     * Asked for Tuples. Check if the input is a bag.
     * If so, stream the tuples in the bag instead of
     * the entire bag.
     */
    @Override
    public Result getNext(Tuple t) throws ExecException {
        Result res = new Result();
        if(!processingBagOfTuples){
            Tuple inpValue = null;
            res = processInput();
            if(res.returnStatus!=POStatus.STATUS_OK)
                return res;
            if(star)
                return res;
            
            inpValue = (Tuple)res.result;
            res.result = null;
            
            Object ret;

            if(columns.size() == 1) {
                ret = inpValue.get(columns.get(0));
            } else {
	            ArrayList<Object> objList = new ArrayList<Object>(columns.size()); 
                
                for(int i: columns) {
                   objList.add(inpValue.get(i)); 
                }
		        ret = tupleFactory.newTuple(objList);
                res.result = (Tuple)ret;
                return res;
            }

            if(overloaded){
                DataBag retBag = (DataBag)ret;
                bagIterator = retBag.iterator();
                if(bagIterator.hasNext()){
                    processingBagOfTuples = true;
                    res.result = bagIterator.next();
                }
            }
            else {
                res.result = (Tuple)ret;
            }
            return res;
        }
        if(bagIterator.hasNext()){
            res.result = bagIterator.next();
            res.returnStatus = POStatus.STATUS_OK;
            return res;
        }
        else{
            //done processing the bag of tuples
            processingBagOfTuples = false;
            return getNext(t);
        }
    }

    public ArrayList<Integer> getColumns() {
        return columns;
    }

    public int getColumn() throws ExecException {
        if(columns.size() != 1) {
            int errCode = 2068;
            String msg = "Internal error. Improper use of method getColumn() in "
                + POProject.class.getSimpleName(); 
            throw new ExecException(msg, errCode, PigException.BUG);
        }
        return columns.get(0);
    }

    public void setColumns(ArrayList<Integer> cols) {
        this.columns = cols;
    }

    public void setColumn(int col) {
        if(null == columns) {
            columns = new ArrayList<Integer>();
        } else {
            columns.clear();
        }
        columns.add(col);
    }

    public boolean isOverloaded() {
        return overloaded;
    }

    public void setOverloaded(boolean overloaded) {
        this.overloaded = overloaded;
    }

    public boolean isStar() {
        return star;
    }

    public void setStar(boolean star) {
        this.star = star;
    }

    @Override
    public POProject clone() throws CloneNotSupportedException {
        ArrayList<Integer> cols = new ArrayList<Integer>(columns.size());
        // Can resuse the same Integer objects, as they are immutable
        for (Integer i : columns) {
            cols.add(i);
        }
        POProject clone = new POProject(new OperatorKey(mKey.scope, 
            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
            requestedParallelism, cols);
        clone.cloneHelper(this);
        clone.star = star;
        clone.overloaded = overloaded;
        return clone;
    }
    
    protected Result processInputBag() throws ExecException {
        
        Result res = new Result();
        if (input==null && (inputs == null || inputs.size()==0)) {
//            log.warn("No inputs found. Signaling End of Processing.");
            res.returnStatus = POStatus.STATUS_EOP;
            return res;
        }
        
        //Should be removed once the model is clear
        if(reporter!=null) reporter.progress();
        
        if(!isInputAttached())
            return inputs.get(0).getNext(dummyBag);
        else{
            res.result = (DataBag)input.get(columns.get(0));
            res.returnStatus = POStatus.STATUS_OK;
            return res;
        }
    }

    public void setResultSingleTupleBag(boolean resultSingleTupleBag) {
        this.resultSingleTupleBag = resultSingleTupleBag;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.physicalLayer;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Stack;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.ComparisonFunc;
import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.LoadFunc;
import org.apache.pig.PigException;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.PigContext;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.*;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ExpressionOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.BinaryExpressionOperator;
import org.apache.pig.builtin.BinStorage;
import org.apache.pig.impl.builtin.GFCross;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.logicalLayer.*;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.DependencyOrderWalker;
import org.apache.pig.impl.plan.DependencyOrderWalkerWOSeenChk;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanException;
import org.apache.pig.impl.plan.PlanWalker;
import org.apache.pig.impl.plan.VisitorException;

public class LogToPhyTranslationVisitor extends LOVisitor {

    protected Map<LogicalOperator, PhysicalOperator> LogToPhyMap;

    Random r = new Random();

    protected Stack<PhysicalPlan> currentPlans;

    protected PhysicalPlan currentPlan;

    protected NodeIdGenerator nodeGen = NodeIdGenerator.getGenerator();

    private Log log = LogFactory.getLog(getClass());

    protected PigContext pc;

    LoadFunc load;

    public LogToPhyTranslationVisitor(LogicalPlan plan) {
        super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(
                plan));

        currentPlans = new Stack<PhysicalPlan>();
        currentPlan = new PhysicalPlan();
        LogToPhyMap = new HashMap<LogicalOperator, PhysicalOperator>();
    }

    public void setPigContext(PigContext pc) {
        this.pc = pc;
    }

    public PhysicalPlan getPhysicalPlan() {

        return currentPlan;
    }

    @Override
    public void visit(LOGreaterThan op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new GreaterThanExpr(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), op
                .getRequestedParallelism());
        exprOp.setOperandType(op.getLhsOperand().getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);

        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                // currentExprPlan.connect(from, exprOp);
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOLesserThan op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new LessThanExpr(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), op
                .getRequestedParallelism());
        exprOp.setOperandType(op.getLhsOperand().getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);

        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOGreaterThanEqual op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new GTOrEqualToExpr(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), op
                .getRequestedParallelism());
        exprOp.setOperandType(op.getLhsOperand().getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOLesserThanEqual op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new LTOrEqualToExpr(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), op
                .getRequestedParallelism());
        exprOp.setOperandType(op.getLhsOperand().getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOEqual op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new EqualToExpr(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), op
                .getRequestedParallelism());
        exprOp.setOperandType(op.getLhsOperand().getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LONotEqual op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new NotEqualToExpr(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), op
                .getRequestedParallelism());
        exprOp.setOperandType(op.getLhsOperand().getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LORegexp op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp =
            new PORegexp(new OperatorKey(scope, nodeGen.getNextNodeId(scope)),
            op.getRequestedParallelism());
        exprOp.setLhs((ExpressionOperator)LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator)LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOAdd op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryExpressionOperator exprOp = new Add(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setResultType(op.getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOSubtract op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryExpressionOperator exprOp = new Subtract(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setResultType(op.getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOMultiply op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryExpressionOperator exprOp = new Multiply(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setResultType(op.getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LODivide op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryExpressionOperator exprOp = new Divide(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setResultType(op.getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOMod op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryExpressionOperator exprOp = new Mod(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setResultType(op.getType());
        exprOp.setLhs((ExpressionOperator) LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator) LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();

        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if (predecessors == null)
            return;
        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }
    
    @Override
    public void visit(LOAnd op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new POAnd(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setLhs((ExpressionOperator)LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator)LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();
        
        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);
        
        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if(predecessors == null) return;
        for(LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOOr op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        BinaryComparisonOperator exprOp = new POOr(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setLhs((ExpressionOperator)LogToPhyMap.get(op.getLhsOperand()));
        exprOp.setRhs((ExpressionOperator)LogToPhyMap.get(op.getRhsOperand()));
        LogicalPlan lp = op.getPlan();
        
        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);
        
        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if(predecessors == null) return;
        for(LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LONot op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        UnaryComparisonOperator exprOp = new PONot(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        exprOp.setExpr((ExpressionOperator)LogToPhyMap.get(op.getOperand()));
        LogicalPlan lp = op.getPlan();
        
        currentPlan.add(exprOp);
        LogToPhyMap.put(op, exprOp);
        
        List<LogicalOperator> predecessors = lp.getPredecessors(op);
        if(predecessors == null) return;
        PhysicalOperator from = LogToPhyMap.get(predecessors.get(0));
        try {
            currentPlan.connect(from, exprOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    protected void visit(LOCross cs) throws VisitorException {
        String scope = cs.getOperatorKey().scope;
        List<LogicalOperator> inputs = cs.getInputs();
        
        POGlobalRearrange poGlobal = new POGlobalRearrange(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), cs
                .getRequestedParallelism());
        POPackage poPackage = new POPackage(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), cs.getRequestedParallelism());

        currentPlan.add(poGlobal);
        currentPlan.add(poPackage);
        
        int count = 0;
        
        try {
            currentPlan.connect(poGlobal, poPackage);
            List<Boolean> flattenLst = Arrays.asList(true, true);
            
            for (LogicalOperator op : inputs) {
                List<PhysicalOperator> pop = Arrays.asList(LogToPhyMap.get(op));
                PhysicalPlan fep1 = new PhysicalPlan();
                ConstantExpression ce1 = new ConstantExpression(new OperatorKey(scope, nodeGen.getNextNodeId(scope)),cs.getRequestedParallelism());
                ce1.setValue(inputs.size());
                ce1.setResultType(DataType.INTEGER);
                fep1.add(ce1);
                
                ConstantExpression ce2 = new ConstantExpression(new OperatorKey(scope, nodeGen.getNextNodeId(scope)),cs.getRequestedParallelism());
                ce2.setValue(count);
                ce2.setResultType(DataType.INTEGER);
                fep1.add(ce2);
                /*Tuple ce1val = TupleFactory.getInstance().newTuple(2);
                ce1val.set(0,inputs.size());
                ce1val.set(1,count);
                ce1.setValue(ce1val);
                ce1.setResultType(DataType.TUPLE);*/
                
                

                POUserFunc gfc = new POUserFunc(new OperatorKey(scope, nodeGen.getNextNodeId(scope)),cs.getRequestedParallelism(), Arrays.asList((PhysicalOperator)ce1,(PhysicalOperator)ce2), new FuncSpec(GFCross.class.getName()));
                gfc.setResultType(DataType.BAG);
                fep1.addAsLeaf(gfc);
                gfc.setInputs(Arrays.asList((PhysicalOperator)ce1,(PhysicalOperator)ce2));
                /*fep1.add(gfc);
                fep1.connect(ce1, gfc);
                fep1.connect(ce2, gfc);*/
                
                PhysicalPlan fep2 = new PhysicalPlan();
                POProject feproj = new POProject(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), cs.getRequestedParallelism());
                feproj.setResultType(DataType.TUPLE);
                feproj.setStar(true);
                feproj.setOverloaded(false);
                fep2.add(feproj);
                List<PhysicalPlan> fePlans = Arrays.asList(fep1, fep2);
                
                POForEach fe = new POForEach(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), cs.getRequestedParallelism(), fePlans, flattenLst );
                currentPlan.add(fe);
                currentPlan.connect(LogToPhyMap.get(op), fe);
                
                POLocalRearrange physOp = new POLocalRearrange(new OperatorKey(
                        scope, nodeGen.getNextNodeId(scope)), cs
                        .getRequestedParallelism());
                List<PhysicalPlan> lrPlans = new ArrayList<PhysicalPlan>();
                for(int i=0;i<inputs.size();i++){
                    PhysicalPlan lrp1 = new PhysicalPlan();
                    POProject lrproj1 = new POProject(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), cs.getRequestedParallelism(), i);
                    lrproj1.setOverloaded(false);
                    lrproj1.setResultType(DataType.INTEGER);
                    lrp1.add(lrproj1);
                    lrPlans.add(lrp1);
                }
                
                physOp.setCross(true);
                physOp.setIndex(count++);
                physOp.setKeyType(DataType.TUPLE);
                physOp.setPlans(lrPlans);
                physOp.setResultType(DataType.TUPLE);
                
                currentPlan.add(physOp);
                currentPlan.connect(fe, physOp);
                currentPlan.connect(physOp, poGlobal);
            }
        } catch (PlanException e1) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e1);
        } catch (ExecException e) {
        	int errCode = 2058;
        	String msg = "Unable to set index on newly create POLocalRearrange.";
            throw new VisitorException(msg, errCode, PigException.BUG, e);
        }
        
        poPackage.setKeyType(DataType.TUPLE);
        poPackage.setResultType(DataType.TUPLE);
        poPackage.setNumInps(count);
        boolean inner[] = new boolean[count];
        for (int i=0;i<count;i++) {
            inner[i] = true;
        }
        poPackage.setInner(inner);
        
        List<PhysicalPlan> fePlans = new ArrayList<PhysicalPlan>();
        List<Boolean> flattenLst = new ArrayList<Boolean>();
        for(int i=1;i<=count;i++){
            PhysicalPlan fep1 = new PhysicalPlan();
            POProject feproj1 = new POProject(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), cs.getRequestedParallelism(), i);
            feproj1.setResultType(DataType.BAG);
            feproj1.setOverloaded(false);
            fep1.add(feproj1);
            fePlans.add(fep1);
            flattenLst.add(true);
        }
        
        POForEach fe = new POForEach(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), cs.getRequestedParallelism(), fePlans, flattenLst );
        currentPlan.add(fe);
        try{
            currentPlan.connect(poPackage, fe);
        }catch (PlanException e1) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e1);
        }
        LogToPhyMap.put(cs, fe);
    }
    
    @Override
    public void visit(LOCogroup cg) throws VisitorException {
        boolean currentPhysicalPlan = false;
        String scope = cg.getOperatorKey().scope;
        List<LogicalOperator> inputs = cg.getInputs();

        POGlobalRearrange poGlobal = new POGlobalRearrange(new OperatorKey(
                scope, nodeGen.getNextNodeId(scope)), cg
                .getRequestedParallelism());
        POPackage poPackage = new POPackage(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), cg.getRequestedParallelism());

        currentPlan.add(poGlobal);
        currentPlan.add(poPackage);

        try {
            currentPlan.connect(poGlobal, poPackage);
        } catch (PlanException e1) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e1);
        }

        int count = 0;
        Byte type = null;
        for (LogicalOperator op : inputs) {
            List<LogicalPlan> plans = (List<LogicalPlan>) cg.getGroupByPlans()
                    .get(op);
            POLocalRearrange physOp = new POLocalRearrange(new OperatorKey(
                    scope, nodeGen.getNextNodeId(scope)), cg
                    .getRequestedParallelism());
            List<PhysicalPlan> exprPlans = new ArrayList<PhysicalPlan>();
            currentPlans.push(currentPlan);
            for (LogicalPlan lp : plans) {
                currentPlan = new PhysicalPlan();
                PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
                        .spawnChildWalker(lp);
                pushWalker(childWalker);
                mCurrentWalker.walk(this);
                exprPlans.add((PhysicalPlan) currentPlan);
                popWalker();

            }
            currentPlan = currentPlans.pop();
            try {
                physOp.setPlans(exprPlans);
            } catch (PlanException pe) {
                int errCode = 2071;
                String msg = "Problem with setting up local rearrange's plans.";
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, pe);
            }
            try {
                physOp.setIndex(count++);
            } catch (ExecException e1) {
            	int errCode = 2058;
            	String msg = "Unable to set index on newly create POLocalRearrange.";
                throw new VisitorException(msg, errCode, PigException.BUG, e1);
            }
            if (plans.size() > 1) {
                type = DataType.TUPLE;
                physOp.setKeyType(type);
            } else {
                type = exprPlans.get(0).getLeaves().get(0).getResultType();
                physOp.setKeyType(type);
            }
            physOp.setResultType(DataType.TUPLE);

            currentPlan.add(physOp);

            try {
                currentPlan.connect(LogToPhyMap.get(op), physOp);
                currentPlan.connect(physOp, poGlobal);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }

        }
        poPackage.setKeyType(type);
        poPackage.setResultType(DataType.TUPLE);
        poPackage.setNumInps(count);
        poPackage.setInner(cg.getInner());
        LogToPhyMap.put(cg, poPackage);
    }
    
    
    /**
     * Create the inner plans used to configure the Local Rearrange operators(ppLists)
     * Extract the keytypes and create the POFRJoin operator.
     */
    @Override
    protected void visit(LOFRJoin frj) throws VisitorException {
        String scope = frj.getOperatorKey().scope;
        List<LogicalOperator> inputs = frj.getInputs();
        List<List<PhysicalPlan>> ppLists = new ArrayList<List<PhysicalPlan>>();
        List<Byte> keyTypes = new ArrayList<Byte>();
        
        int fragment = findFrag(inputs,frj.getFragOp());
        List<PhysicalOperator> inp = new ArrayList<PhysicalOperator>();
        for (LogicalOperator op : inputs) {
            inp.add(LogToPhyMap.get(op));
            List<LogicalPlan> plans = (List<LogicalPlan>) frj.getJoinColPlans()
                    .get(op);
            
            List<PhysicalPlan> exprPlans = new ArrayList<PhysicalPlan>();
            currentPlans.push(currentPlan);
            for (LogicalPlan lp : plans) {
                currentPlan = new PhysicalPlan();
                PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
                        .spawnChildWalker(lp);
                pushWalker(childWalker);
                mCurrentWalker.walk(this);
                exprPlans.add((PhysicalPlan) currentPlan);
                popWalker();

            }
            currentPlan = currentPlans.pop();
            ppLists.add(exprPlans);
            
            if (plans.size() > 1) {
                keyTypes.add(DataType.TUPLE);
            } else {
                keyTypes.add(exprPlans.get(0).getLeaves().get(0).getResultType());
            }
        }
        POFRJoin pfrj;
        try {
            pfrj = new POFRJoin(new OperatorKey(scope,nodeGen.getNextNodeId(scope)),frj.getRequestedParallelism(),
                                        inp, ppLists, keyTypes, null, fragment);
        } catch (ExecException e1) {
        	int errCode = 2058;
        	String msg = "Unable to set index on newly create POLocalRearrange.";
            throw new VisitorException(msg, errCode, PigException.BUG, e1);
        }
        pfrj.setResultType(DataType.TUPLE);
        currentPlan.add(pfrj);
        for (LogicalOperator op : inputs) {
            try {
                currentPlan.connect(LogToPhyMap.get(op), pfrj);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
        LogToPhyMap.put(frj, pfrj);
    }

    private int findFrag(List<LogicalOperator> inputs, LogicalOperator fragOp) {
        int i=-1;
        for (LogicalOperator lop : inputs) {
            if(fragOp.getOperatorKey().equals(lop.getOperatorKey()))
                return ++i;
        }
        return -1;
    }

    @Override
    public void visit(LOFilter filter) throws VisitorException {
        String scope = filter.getOperatorKey().scope;
        POFilter poFilter = new POFilter(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), filter.getRequestedParallelism());
        poFilter.setResultType(filter.getType());
        currentPlan.add(poFilter);
        LogToPhyMap.put(filter, poFilter);
        currentPlans.push(currentPlan);

        currentPlan = new PhysicalPlan();

        PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
                .spawnChildWalker(filter.getComparisonPlan());
        pushWalker(childWalker);
        mCurrentWalker.walk(this);
        popWalker();

        poFilter.setPlan((PhysicalPlan) currentPlan);
        currentPlan = currentPlans.pop();

        List<LogicalOperator> op = filter.getPlan().getPredecessors(filter);

        PhysicalOperator from;
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Filter." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);
        }
        
        try {
            currentPlan.connect(from, poFilter);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visit(LOStream stream) throws VisitorException {
        String scope = stream.getOperatorKey().scope;
        POStream poStream = new POStream(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), stream.getExecutableManager(), 
                stream.getStreamingCommand(), this.pc.getProperties());
        currentPlan.add(poStream);
        LogToPhyMap.put(stream, poStream);
        
        List<LogicalOperator> op = stream.getPlan().getPredecessors(stream);

        PhysicalOperator from;
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));
        } else {                
            int errCode = 2051;
            String msg = "Did not find a predecessor for Stream." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);
        }
        
        try {
            currentPlan.connect(from, poStream);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visit(LOProject op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        POProject exprOp;
        if(op.isSendEmptyBagOnEOP()) {
            exprOp = new PORelationToExprProject(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), op.getRequestedParallelism());
        } else {
            exprOp = new POProject(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), op.getRequestedParallelism());
        }
        exprOp.setResultType(op.getType());
        exprOp.setColumns((ArrayList)op.getProjection());
        exprOp.setStar(op.isStar());
        exprOp.setOverloaded(op.getOverloaded());
        LogicalPlan lp = op.getPlan();
        LogToPhyMap.put(op, exprOp);
        currentPlan.add(exprOp);

        List<LogicalOperator> predecessors = lp.getPredecessors(op);

        // Project might not have any predecessors
        if (predecessors == null)
            return;

        for (LogicalOperator lo : predecessors) {
            PhysicalOperator from = LogToPhyMap.get(lo);
            try {
                currentPlan.connect(from, exprOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOForEach g) throws VisitorException {
        boolean currentPhysicalPlan = false;
        String scope = g.getOperatorKey().scope;
        List<PhysicalPlan> innerPlans = new ArrayList<PhysicalPlan>();
        List<LogicalPlan> plans = g.getForEachPlans();

        currentPlans.push(currentPlan);
        for (LogicalPlan plan : plans) {
            currentPlan = new PhysicalPlan();
            PlanWalker<LogicalOperator, LogicalPlan> childWalker = new DependencyOrderWalkerWOSeenChk<LogicalOperator, LogicalPlan>(
                    plan);
            pushWalker(childWalker);
            childWalker.walk(this);
            innerPlans.add(currentPlan);
            popWalker();
        }
        currentPlan = currentPlans.pop();

        // PhysicalOperator poGen = new POGenerate(new OperatorKey("",
        // r.nextLong()), inputs, toBeFlattened);
        POForEach poFE = new POForEach(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), g.getRequestedParallelism(), innerPlans,
                g.getFlatten());
        poFE.setResultType(g.getType());
        LogToPhyMap.put(g, poFE);
        currentPlan.add(poFE);

        // generate cannot have multiple inputs
        List<LogicalOperator> op = g.getPlan().getPredecessors(g);

        // generate may not have any predecessors
        if (op == null)
            return;

        PhysicalOperator from = LogToPhyMap.get(op.get(0));
        try {
            currentPlan.connect(from, poFE);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }

    }

    @Override
    public void visit(LOSort s) throws VisitorException {
        String scope = s.getOperatorKey().scope;
        List<LogicalPlan> logPlans = s.getSortColPlans();
        List<PhysicalPlan> sortPlans = new ArrayList<PhysicalPlan>(logPlans.size());

        // convert all the logical expression plans to physical expression plans
        currentPlans.push(currentPlan);
        for (LogicalPlan plan : logPlans) {
            currentPlan = new PhysicalPlan();
            PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
                    .spawnChildWalker(plan);
            pushWalker(childWalker);
            childWalker.walk(this);
            sortPlans.add((PhysicalPlan) currentPlan);
            popWalker();
        }
        currentPlan = currentPlans.pop();

        // get the physical operator for sort
        POSort sort;
        if (s.getUserFunc() == null) {
            sort = new POSort(new OperatorKey(scope, nodeGen
                    .getNextNodeId(scope)), s.getRequestedParallelism(), null,
                    sortPlans, s.getAscendingCols(), null);
        } else {
            POUserComparisonFunc comparator = new POUserComparisonFunc(new OperatorKey(
                    scope, nodeGen.getNextNodeId(scope)), s
                    .getRequestedParallelism(), null, s.getUserFunc());
            sort = new POSort(new OperatorKey(scope, nodeGen
                    .getNextNodeId(scope)), s.getRequestedParallelism(), null,
                    sortPlans, s.getAscendingCols(), comparator);
        }
        sort.setLimit(s.getLimit());
        // sort.setRequestedParallelism(s.getType());
        LogToPhyMap.put(s, sort);
        currentPlan.add(sort);
        List<LogicalOperator> op = s.getPlan().getPredecessors(s); 
        PhysicalOperator from;
        
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Sort." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }
        
        try {
            currentPlan.connect(from, sort);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }

        sort.setResultType(s.getType());

    }

    @Override
    public void visit(LODistinct op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        // This is simpler. No plans associated with this. Just create the
        // physical operator,
        // push it in the current plan and make the connections
        PhysicalOperator physOp = new PODistinct(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), op.getRequestedParallelism());
        physOp.setResultType(op.getType());
        LogToPhyMap.put(op, physOp);
        currentPlan.add(physOp);
        // Distinct will only have a single input
        List<LogicalOperator> inputs = op.getPlan().getPredecessors(op);
        PhysicalOperator from; 
        
        if(inputs != null) {
            from = LogToPhyMap.get(inputs.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Distinct." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }

        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visit(LOSplit split) throws VisitorException {
        String scope = split.getOperatorKey().scope;
        PhysicalOperator physOp = new POSplit(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), split.getRequestedParallelism());
        FileSpec splStrFile;
        try {
            splStrFile = new FileSpec(FileLocalizer.getTemporaryPath(null, pc).toString(),new FuncSpec(BinStorage.class.getName()));
        } catch (IOException e1) {
            byte errSrc = pc.getErrorSource();
            int errCode = 0;
            switch(errSrc) {
            case PigException.BUG:
                errCode = 2016;
                break;
            case PigException.REMOTE_ENVIRONMENT:
                errCode = 6002;
                break;
            case PigException.USER_ENVIRONMENT:
                errCode = 4003;
                break;
            }
            String msg = "Unable to obtain a temporary path." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, errSrc, e1);

        }
        ((POSplit)physOp).setSplitStore(splStrFile);
        LogToPhyMap.put(split, physOp);

        currentPlan.add(physOp);

        List<LogicalOperator> op = split.getPlan().getPredecessors(split); 
        PhysicalOperator from;
        
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Split." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }        

        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visit(LOSplitOutput split) throws VisitorException {
        String scope = split.getOperatorKey().scope;
        PhysicalOperator physOp = new POFilter(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), split.getRequestedParallelism());
        LogToPhyMap.put(split, physOp);

        currentPlan.add(physOp);
        currentPlans.push(currentPlan);
        currentPlan = new PhysicalPlan();
        PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
                .spawnChildWalker(split.getConditionPlan());
        pushWalker(childWalker);
        mCurrentWalker.walk(this);
        popWalker();

        ((POFilter) physOp).setPlan((PhysicalPlan) currentPlan);
        currentPlan = currentPlans.pop();
        currentPlan.add(physOp);

        List<LogicalOperator> op = split.getPlan().getPredecessors(split); 
        PhysicalOperator from;
        
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Split Output." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }        
        
        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
    }

    @Override
    public void visit(LOUserFunc func) throws VisitorException {
        String scope = func.getOperatorKey().scope;
        Object f = PigContext.instantiateFuncFromSpec(func.getFuncSpec());
        PhysicalOperator p;
        if (f instanceof EvalFunc) {
            p = new POUserFunc(new OperatorKey(scope, nodeGen
                    .getNextNodeId(scope)), func.getRequestedParallelism(),
                    null, func.getFuncSpec(), (EvalFunc) f);
        } else {
            p = new POUserComparisonFunc(new OperatorKey(scope, nodeGen
                    .getNextNodeId(scope)), func.getRequestedParallelism(),
                    null, func.getFuncSpec(), (ComparisonFunc) f);
        }
        p.setResultType(func.getType());
        currentPlan.add(p);
        List<org.apache.pig.impl.logicalLayer.ExpressionOperator> fromList = func.getArguments();
        if(fromList!=null){
            for (LogicalOperator op : fromList) {
                PhysicalOperator from = LogToPhyMap.get(op);
                try {
                    currentPlan.connect(from, p);
                } catch (PlanException e) {
                    int errCode = 2015;
                    String msg = "Invalid physical operators in the physical plan" ;
                    throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
                }
            }
        }
        LogToPhyMap.put(func, p);

    }

    @Override
    public void visit(LOLoad loLoad) throws VisitorException {
        String scope = loLoad.getOperatorKey().scope;
        POLoad load = new POLoad(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), loLoad.isSplittable());
        load.setLFile(loLoad.getInputFile());
        load.setPc(pc);
        load.setResultType(loLoad.getType());
        currentPlan.add(load);
        LogToPhyMap.put(loLoad, load);
        this.load = loLoad.getLoadFunc();

        // Load is typically a root operator, but in the multiquery
        // case it might have a store as a predecessor.
        List<LogicalOperator> op = loLoad.getPlan().getPredecessors(loLoad); 
        PhysicalOperator from;
        
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));

            try {
                currentPlan.connect(from, load);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

    @Override
    public void visit(LOStore loStore) throws VisitorException {
        String scope = loStore.getOperatorKey().scope;
        POStore store = new POStore(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)));
        store.setSFile(loStore.getOutputFile());
        store.setInputSpec(loStore.getInputSpec());
        try {
            // create a new schema for ourselves so that when
            // we serialize we are not serializing objects that
            // contain the schema - apparently Java tries to
            // serialize the object containing the schema if
            // we are trying to serialize the schema reference in
            // the containing object. The schema here will be serialized
            // in JobControlCompiler
            store.setSchema(new Schema(loStore.getSchema()));
        } catch (FrontendException e1) {
            int errorCode = 1060;
            String message = "Cannot resolve Store output schema";  
            throw new VisitorException(message, errorCode, PigException.BUG, e1);    
        }
        currentPlan.add(store);
        
        List<LogicalOperator> op = loStore.getPlan().getPredecessors(loStore); 
        PhysicalOperator from;
        
        if(op != null) {
            from = LogToPhyMap.get(op.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Store." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }        

        try {
            currentPlan.connect(from, store);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }
        LogToPhyMap.put(loStore, store);
    }

    @Override
    public void visit(LOConst op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        ConstantExpression ce = new ConstantExpression(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)));
        ce.setValue(op.getValue());
        ce.setResultType(op.getType());
        //this operator doesn't have any predecessors
        currentPlan.add(ce);
        LogToPhyMap.put(op, ce);
    }

    @Override
    public void visit(LOBinCond op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        ExpressionOperator physOp = new POBinCond(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism());
        LogToPhyMap.put(op, physOp);
        POBinCond phy = (POBinCond) physOp;
        ExpressionOperator cond = (ExpressionOperator) LogToPhyMap.get(op
                .getCond());
        phy.setCond(cond);
        ExpressionOperator lhs = (ExpressionOperator) LogToPhyMap.get(op
                .getLhsOp());
        phy.setLhs(lhs);
        ExpressionOperator rhs = (ExpressionOperator) LogToPhyMap.get(op
                .getRhsOp());
        phy.setRhs(rhs);
        phy.setResultType(op.getType());
        currentPlan.add(physOp);

        List<LogicalOperator> ops = op.getPlan().getPredecessors(op);

        for (LogicalOperator l : ops) {
            ExpressionOperator from = (ExpressionOperator) LogToPhyMap.get(l);
            try {
                currentPlan.connect(from, physOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }

    }

    @Override
    public void visit(LONegative op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        ExpressionOperator physOp = new PONegative(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism(),
                null);
        currentPlan.add(physOp);

        LogToPhyMap.put(op, physOp);

        List<LogicalOperator> inputs = op.getPlan().getPredecessors(op); 
        ExpressionOperator from;
        
        if(inputs != null) {
            from = (ExpressionOperator)LogToPhyMap.get(inputs.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Negative." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }
   
        ((PONegative) physOp).setExpr(from);
        ((PONegative) physOp).setResultType(op.getType());
        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }

    }

    @Override
    public void visit(LOIsNull op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        UnaryComparisonOperator physOp = new POIsNull(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), op.getRequestedParallelism(), null);

        List<LogicalOperator> inputs = op.getPlan().getPredecessors(op); 
        ExpressionOperator from;
        
        if(inputs != null) {
            from = (ExpressionOperator)LogToPhyMap.get(inputs.get(0));
        } else {
            int errCode = 2051;
            String msg = "Did not find a predecessor for Null." ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);            
        }

        
        physOp.setOperandType(op.getOperand().getType());
        currentPlan.add(physOp);

        LogToPhyMap.put(op, physOp);

        
        ((POIsNull) physOp).setExpr(from);
        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }

    }

    @Override
    public void visit(LOMapLookup op) throws VisitorException {
        String scope = ((OperatorKey) op.getOperatorKey()).scope;
        ExpressionOperator physOp = new POMapLookUp(new OperatorKey(scope,
                nodeGen.getNextNodeId(scope)), op.getRequestedParallelism(), op
                .getLookUpKey());
        physOp.setResultType(op.getType());
        currentPlan.add(physOp);

        LogToPhyMap.put(op, physOp);

        ExpressionOperator from = (ExpressionOperator) LogToPhyMap.get(op
                .getMap());
        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }

    }

    @Override
    public void visit(LOCast op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        ExpressionOperator physOp = new POCast(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), op.getRequestedParallelism());
        currentPlan.add(physOp);

        LogToPhyMap.put(op, physOp);
        ExpressionOperator from = (ExpressionOperator) LogToPhyMap.get(op
                .getExpression());
        physOp.setResultType(op.getType());
        FuncSpec lfSpec = op.getLoadFuncSpec();
        if(null != lfSpec) {
            ((POCast) physOp).setLoadFSpec(lfSpec);
        }
        try {
            currentPlan.connect(from, physOp);
        } catch (PlanException e) {
            int errCode = 2015;
            String msg = "Invalid physical operators in the physical plan" ;
            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
        }

    }

    @Override
    public void visit(LOLimit limit) throws VisitorException {
            String scope = limit.getOperatorKey().scope;
            POLimit poLimit = new POLimit(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), limit.getRequestedParallelism());
            poLimit.setResultType(limit.getType());
            poLimit.setLimit(limit.getLimit());
            currentPlan.add(poLimit);
            LogToPhyMap.put(limit, poLimit);

            List<LogicalOperator> op = limit.getPlan().getPredecessors(limit);

            PhysicalOperator from;
            if(op != null) {
                from = LogToPhyMap.get(op.get(0));
            } else {
                int errCode = 2051;
                String msg = "Did not find a predecessor for Limit." ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG);                
            }
            try {
                    currentPlan.connect(from, poLimit);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
    }

    @Override
    public void visit(LOUnion op) throws VisitorException {
        String scope = op.getOperatorKey().scope;
        POUnion physOp = new POUnion(new OperatorKey(scope, nodeGen
                .getNextNodeId(scope)), op.getRequestedParallelism());
        currentPlan.add(physOp);
        physOp.setResultType(op.getType());
        LogToPhyMap.put(op, physOp);
        List<LogicalOperator> ops = op.getInputs();

        for (LogicalOperator l : ops) {
            PhysicalOperator from = LogToPhyMap.get(l);
            try {
                currentPlan.connect(from, physOp);
            } catch (PlanException e) {
                int errCode = 2015;
                String msg = "Invalid physical operators in the physical plan" ;
                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
            }
        }
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.test;

import java.io.File;
import java.io.IOException;
import java.util.List;
import java.util.ArrayList;

import junit.framework.TestCase;

import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.impl.logicalLayer.validators.*;
import org.apache.pig.impl.logicalLayer.* ;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema ;
import org.apache.pig.impl.plan.DepthFirstWalker;
import org.apache.pig.impl.plan.PlanValidationException;
import org.apache.pig.impl.plan.CompilationMessageCollector;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.data.*;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.builtin.PigStorage;
import org.junit.Test;
import static org.apache.pig.test.utils.TypeCheckingTestUtil.* ;
import org.apache.pig.test.utils.LogicalPlanTester;
import org.apache.pig.test.utils.TypeCheckingTestUtil;

public class TestTypeCheckingValidator extends TestCase {

    LogicalPlanTester planTester = new LogicalPlanTester() ;
    
	private static final String simpleEchoStreamingCommand;
        static {
            if (System.getProperty("os.name").toUpperCase().startsWith("WINDOWS"))
                simpleEchoStreamingCommand = "perl -ne 'print \\\"$_\\\"'";
            else
                simpleEchoStreamingCommand = "perl -ne 'print \"$_\"'";
            File fileA = new File("a");
            File fileB = new File("b");
            try {
                fileA.delete();
                fileB.delete();
                if(!fileA.createNewFile() || !fileB.createNewFile())
                    fail("Unable to create input files");
            } catch (IOException e) {
                fail("Unable to create input files:" + e.getMessage());
            }
            fileA.deleteOnExit();
            fileB.deleteOnExit();
        }
        
    @Test
    public void testExpressionTypeChecking1() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20D) ;
        constant2.setType(DataType.DOUBLE) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), 123f) ;
        constant3.setType(DataType.FLOAT) ;
        
        LOAdd add1 = new LOAdd(plan, genNewOperatorKey()) ;
        LOCast cast1 = new LOCast(plan, genNewOperatorKey(), DataType.DOUBLE) ;
        LOMultiply mul1 = new LOMultiply(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(cast1) ;
        plan.add(add1) ;
        plan.add(mul1) ;
        
        plan.connect(constant1, add1) ;
        plan.connect(constant2, add1) ;
        plan.connect(add1, mul1) ;
        plan.connect(constant3, cast1) ;
        plan.connect(cast1, mul1) ;
                          
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;        
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }       
        
        // Induction check
        assertEquals(DataType.DOUBLE, add1.getType()) ;
        assertEquals(DataType.DOUBLE, mul1.getType()) ;
        
        // Cast insertion check
        assertEquals(DataType.DOUBLE, add1.getLhsOperand().getType()) ;
        assertEquals(DataType.DOUBLE, mul1.getRhsOperand().getType()) ;
        
    }
    
    @Test
    public void testExpressionTypeCheckingFail1() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20D) ;
        constant2.setType(DataType.DOUBLE) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), "123") ;
        constant3.setType(DataType.CHARARRAY) ;
        
        LOAdd add1 = new LOAdd(plan, genNewOperatorKey()) ;
        LOCast cast1 = new LOCast(plan, genNewOperatorKey(), DataType.BYTEARRAY) ;
        LOMultiply mul1 = new LOMultiply(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(cast1) ;
        plan.add(add1) ;
        plan.add(mul1) ;
        
        plan.connect(constant1, add1) ;
        plan.connect(constant2, add1) ;
        plan.connect(add1, mul1) ;
        plan.connect(constant3, cast1) ;
        plan.connect(cast1, mul1) ;
                          
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;        
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (!collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }       
    }

    @Test
    public void testExpressionTypeChecking2() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), new DataByteArray()) ;
        constant2.setType(DataType.BYTEARRAY) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), 123L) ;
        constant3.setType(DataType.LONG) ;        
        LOConst constant4 =  new LOConst(plan, genNewOperatorKey(), true) ;
        constant4.setType(DataType.BOOLEAN) ;
        
        LOSubtract sub1 = new LOSubtract(plan, genNewOperatorKey()) ;
        LOGreaterThan gt1 = new LOGreaterThan(plan, genNewOperatorKey()) ;
        LOAnd and1 = new LOAnd(plan, genNewOperatorKey()) ;
        LONot not1 = new LONot(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(constant4) ;
        
        plan.add(sub1) ;
        plan.add(gt1) ;
        plan.add(and1) ;
        plan.add(not1) ;

        
        plan.connect(constant1, sub1) ;
        plan.connect(constant2, sub1) ;
        plan.connect(sub1, gt1) ;
        plan.connect(constant3, gt1) ;
        plan.connect(gt1, and1) ;
        plan.connect(constant4, and1) ;
        plan.connect(and1, not1) ;
                      
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        
        typeValidator.validate(plan, collector) ;
        
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (collector.hasError()) {
            throw new Exception("Error not expected during type checking") ;
        }       


        // Induction check   
        assertEquals(DataType.INTEGER, sub1.getType()) ;    
        assertEquals(DataType.BOOLEAN, gt1.getType()) ;     
        assertEquals(DataType.BOOLEAN, and1.getType()) ;    
        assertEquals(DataType.BOOLEAN, not1.getType()) ;    

        // Cast insertion check     
        assertEquals(DataType.INTEGER, sub1.getRhsOperand().getType()) ;    
        assertEquals(DataType.LONG, gt1.getLhsOperand().getType()) ;
        
    }
    
    
    @Test
    public void testExpressionTypeChecking3() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20L) ;
        constant2.setType(DataType.LONG) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), 123) ;
        constant3.setType(DataType.INTEGER) ;
        
        LOMod mod1 = new LOMod(plan, genNewOperatorKey()) ;
        LOEqual equal1 = new LOEqual(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(equal1) ;
        plan.add(mod1) ;
        
        plan.connect(constant1, mod1) ;
        plan.connect(constant2, mod1) ;
        plan.connect(mod1, equal1) ;
        plan.connect(constant3, equal1) ;
        
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;        
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }      
        
        // Induction check
        assertEquals(DataType.LONG, mod1.getType()) ;
        assertEquals(DataType.BOOLEAN, equal1.getType()) ;
        
        // Cast insertion check
        assertEquals(DataType.LONG, mod1.getLhsOperand().getType()) ;
        assertEquals(DataType.LONG, equal1.getRhsOperand().getType()) ;
        
    }
     
    @Test
    public void testExpressionTypeChecking4() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20D) ;
        constant2.setType(DataType.DOUBLE) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), 123f) ;
        constant3.setType(DataType.FLOAT) ;
        
        LODivide div1 = new LODivide(plan, genNewOperatorKey()) ;
        LOCast cast1 = new LOCast(plan, genNewOperatorKey(), DataType.DOUBLE) ;
        LONotEqual notequal1 = new LONotEqual(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(div1) ;
        plan.add(cast1) ;
        plan.add(notequal1) ;
        
        plan.connect(constant1, div1) ;
        plan.connect(constant2, div1) ;
        plan.connect(constant3, cast1) ;
        plan.connect(div1, notequal1) ;
        plan.connect(cast1, notequal1) ;
        
                          
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;        
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }  
        
        // Induction check
        assertEquals(DataType.DOUBLE, div1.getType()) ;
        assertEquals(DataType.BOOLEAN, notequal1.getType()) ;
        
        // Cast insertion check
        assertEquals(DataType.DOUBLE, div1.getLhsOperand().getType()) ;
        assertEquals(DataType.DOUBLE, notequal1.getRhsOperand().getType()) ;
        
    }
    
    @Test
    public void testExpressionTypeCheckingFail4() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20D) ;
        constant2.setType(DataType.DOUBLE) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), "123") ;
        constant3.setType(DataType.CHARARRAY) ;
        
        LODivide div1 = new LODivide(plan, genNewOperatorKey()) ;
        LOCast cast1 = new LOCast(plan, genNewOperatorKey(), DataType.BYTEARRAY) ;
        LONotEqual notequal1 = new LONotEqual(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(div1) ;
        plan.add(cast1) ;
        plan.add(notequal1) ;
        
        plan.connect(constant1, div1) ;
        plan.connect(constant2, div1) ;
        plan.connect(constant3, cast1) ;
        plan.connect(div1, notequal1) ;
        plan.connect(cast1, notequal1) ;
        
                          
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try{
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (!collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }  
    }

    @Test
    public void testExpressionTypeChecking5() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10F) ;
        constant1.setType(DataType.FLOAT) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20L) ;
        constant2.setType(DataType.LONG) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), 123F) ;
        constant3.setType(DataType.FLOAT) ;
        LOConst constant4 =  new LOConst(plan, genNewOperatorKey(), 123D) ;
        constant4.setType(DataType.DOUBLE) ;
        
        LOLesserThanEqual lesser1 = new LOLesserThanEqual(plan, genNewOperatorKey()) ;
        LOBinCond bincond1 = new LOBinCond(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(constant4) ;
        plan.add(lesser1) ;
        plan.add(bincond1) ;
        
        plan.connect(constant1, lesser1) ;
        plan.connect(constant2, lesser1) ;
        plan.connect(lesser1, bincond1) ;
        plan.connect(constant3, bincond1) ;
        plan.connect(constant4, bincond1) ;
        
         
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;        
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }  
        
        // Induction check
        assertEquals(DataType.BOOLEAN, lesser1.getType()) ;
        assertEquals(DataType.DOUBLE, bincond1.getType()) ;
        
        // Cast insertion check
        assertEquals(DataType.FLOAT, lesser1.getLhsOperand().getType()) ;
        assertEquals(DataType.FLOAT, lesser1.getRhsOperand().getType()) ;
        assertEquals(DataType.DOUBLE, bincond1.getLhsOp().getType()) ;
        assertEquals(DataType.DOUBLE, bincond1.getRhsOp().getType()) ;
        
    }
    
    @Test
    public void testExpressionTypeChecking6() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), "10") ;
        constant1.setType(DataType.CHARARRAY) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20L) ;
        constant2.setType(DataType.LONG) ;
        
        LOAdd add1 = new LOAdd(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(add1) ;
        
        plan.connect(constant1, add1) ;
        plan.connect(constant2, add1) ;        
                          
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;    
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (!collector.hasError()) {
            throw new AssertionError("Error expected") ;
        }         
        
    }

    @Test
    public void testExpressionTypeChecking7() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20D) ;
        constant2.setType(DataType.BYTEARRAY) ;
        LOConst constant3 =  new LOConst(plan, genNewOperatorKey(), 123L) ;
        constant3.setType(DataType.LONG) ;        
        
        LOGreaterThan gt1 = new LOGreaterThan(plan, genNewOperatorKey()) ;
        LOEqual equal1 = new LOEqual(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(constant3) ;
        plan.add(gt1) ;
        plan.add(equal1) ;
              
        plan.connect(constant1, gt1) ;
        plan.connect(constant2, gt1) ;
        plan.connect(gt1, equal1) ;
        plan.connect(constant3, equal1) ;
                      
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;    
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }        
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (!collector.hasError()) {
            throw new AssertionError("Error expected") ;
        }   
    }
    
    @Test
    public void testExpressionTypeChecking8() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        
	    TupleFactory tupleFactory = TupleFactory.getInstance();

	    ArrayList<Object> innerObjList = new ArrayList<Object>(); 
	    ArrayList<Object> objList = new ArrayList<Object>(); 

        innerObjList.add(10);
        innerObjList.add(3);
        innerObjList.add(7);
        innerObjList.add(17);

		Tuple innerTuple = tupleFactory.newTuple(innerObjList);

        objList.add("World");
        objList.add(42);
        objList.add(innerTuple);
        
		Tuple tuple = tupleFactory.newTuple(objList);
        
        ArrayList<Schema.FieldSchema> innerFss = new ArrayList<Schema.FieldSchema>();
        ArrayList<Schema.FieldSchema> fss = new ArrayList<Schema.FieldSchema>();
        ArrayList<Schema.FieldSchema> castFss = new ArrayList<Schema.FieldSchema>();

        Schema.FieldSchema stringFs = new Schema.FieldSchema(null, DataType.CHARARRAY);
        Schema.FieldSchema intFs = new Schema.FieldSchema(null, DataType.INTEGER);

        for(int i = 0; i < innerObjList.size(); ++i) {
            innerFss.add(intFs);
        }

        Schema innerTupleSchema = new Schema(innerFss);

        fss.add(stringFs);
        fss.add(intFs);
        fss.add(new Schema.FieldSchema(null, innerTupleSchema, DataType.TUPLE));

        Schema tupleSchema = new Schema(fss);

        for(int i = 0; i < 3; ++i) {
            castFss.add(stringFs);
        }

        Schema castSchema = new Schema(castFss);


        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), innerTuple) ;
        constant1.setType(DataType.TUPLE) ;
        constant1.setFieldSchema(new Schema.FieldSchema(null, innerTupleSchema, DataType.TUPLE));
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), tuple) ;
        constant2.setType(DataType.TUPLE) ;
        constant2.setFieldSchema(new Schema.FieldSchema(null, tupleSchema, DataType.TUPLE));
        LOCast cast1 = new LOCast(plan, genNewOperatorKey(), DataType.TUPLE) ;
        cast1.setFieldSchema(new FieldSchema(null, castSchema, DataType.TUPLE));
        
        LOEqual equal1 = new LOEqual(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(cast1) ;
        plan.add(equal1) ;
              
        plan.connect(constant1, cast1) ;
        plan.connect(cast1, equal1) ;
        plan.connect(constant2, equal1) ;
                      
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;    
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (collector.hasError()) {
            throw new Exception("Error during type checking") ;
        }   

        assertEquals(DataType.BOOLEAN, equal1.getType()) ;
        assertEquals(DataType.TUPLE, equal1.getRhsOperand().getType()) ;
        assertEquals(DataType.TUPLE, equal1.getLhsOperand().getType()) ;
    }

    @Test
    public void testExpressionTypeCheckingFail8() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        
	    TupleFactory tupleFactory = TupleFactory.getInstance();

	    ArrayList<Object> innerObjList = new ArrayList<Object>(); 
	    ArrayList<Object> objList = new ArrayList<Object>(); 

        innerObjList.add("10");
        innerObjList.add("3");
        innerObjList.add(7);
        innerObjList.add("17");

		Tuple innerTuple = tupleFactory.newTuple(innerObjList);

        objList.add("World");
        objList.add(42);
        objList.add(innerTuple);
        
		Tuple tuple = tupleFactory.newTuple(objList);
        
        ArrayList<Schema.FieldSchema> innerFss = new ArrayList<Schema.FieldSchema>();
        ArrayList<Schema.FieldSchema> fss = new ArrayList<Schema.FieldSchema>();
        ArrayList<Schema.FieldSchema> castFss = new ArrayList<Schema.FieldSchema>();

        Schema.FieldSchema stringFs = new Schema.FieldSchema(null, DataType.CHARARRAY);
        Schema.FieldSchema intFs = new Schema.FieldSchema(null, DataType.INTEGER);
        Schema.FieldSchema doubleFs = new Schema.FieldSchema(null, DataType.DOUBLE);

        innerFss.add(stringFs);
        innerFss.add(stringFs);
        innerFss.add(intFs);
        innerFss.add(stringFs);

        Schema innerTupleSchema = new Schema(innerFss);

        fss.add(stringFs);
        fss.add(intFs);
        fss.add(new Schema.FieldSchema(null, innerTupleSchema, DataType.TUPLE));

        Schema tupleSchema = new Schema(fss);

        castFss.add(stringFs);
        castFss.add(stringFs);
        castFss.add(doubleFs);
        castFss.add(intFs);

        Schema castSchema = new Schema(castFss);


        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), innerTuple) ;
        constant1.setType(DataType.TUPLE) ;
        constant1.setFieldSchema(new Schema.FieldSchema(null, innerTupleSchema, DataType.TUPLE));
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), tuple) ;
        constant2.setType(DataType.TUPLE) ;
        constant2.setFieldSchema(new Schema.FieldSchema(null, tupleSchema, DataType.TUPLE));
        LOCast cast1 = new LOCast(plan, genNewOperatorKey(), DataType.TUPLE) ;
        cast1.setFieldSchema(new FieldSchema(null, castSchema, DataType.TUPLE));
        
        LOEqual equal1 = new LOEqual(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(cast1) ;
        plan.add(equal1) ;
              
        plan.connect(constant1, cast1) ;
        plan.connect(cast1, equal1) ;
        plan.connect(constant2, equal1) ;
                      
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ; 
            fail("Exception expected") ;
        } catch(PlanValidationException pve) {
            //good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        
        if (!collector.hasError()) {
            throw new Exception("Error expected") ;
        }   
    }

    @Test
    public void testArithmeticOpCastInsert1() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20D) ;
        constant2.setType(DataType.DOUBLE) ;
        
        LOMultiply mul1 = new LOMultiply(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(mul1) ;
        
        plan.connect(constant1, mul1) ;
        plan.connect(constant2, mul1) ;       
        
        // Before type checking its set correctly - PIG-421
        System.out.println(DataType.findTypeName(mul1.getType())) ;
        assertEquals(DataType.DOUBLE, mul1.getType()) ;    
        
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;        
        printMessageCollector(collector) ;
        
        printTypeGraph(plan) ;
        
        // After type checking
        System.out.println(DataType.findTypeName(mul1.getType())) ;
        assertEquals(DataType.DOUBLE, mul1.getType()) ;
    }

    @Test
    public void testArithmeticOpCastInsert2() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.INTEGER) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20L) ;
        constant2.setType(DataType.LONG) ;

        LONegative neg1 = new LONegative(plan, genNewOperatorKey()) ;
        LOSubtract subtract1 = new LOSubtract(plan, genNewOperatorKey()) ;

        plan.add(constant1) ;
        plan.add(neg1) ; 
        plan.add(constant2) ;
        plan.add(subtract1) ;

        plan.connect(constant1, neg1) ;
        plan.connect(neg1, subtract1) ;
        plan.connect(constant2, subtract1) ;

        // Before type checking its set correctly = PIG-421
        assertEquals(DataType.LONG, subtract1.getType()) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        printTypeGraph(plan) ;

        // After type checking
        System.out.println(DataType.findTypeName(subtract1.getType())) ;
        assertEquals(DataType.LONG, subtract1.getType()) ;

        assertTrue(subtract1.getLhsOperand() instanceof LOCast);
        assertEquals(((LOCast)subtract1.getLhsOperand()).getType(), DataType.LONG);
        assertTrue(((LOCast)subtract1.getLhsOperand()).getExpression() == neg1);
    }

    @Test
    public void testModCastInsert1() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.BYTEARRAY) ;
        LOConst constant2 =  new LOConst(plan, genNewOperatorKey(), 20L) ;
        constant2.setType(DataType.LONG) ;

        LOMod mod1 = new LOMod(plan, genNewOperatorKey()) ;

        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(mod1) ;

        plan.connect(constant1, mod1) ;
        plan.connect(constant2, mod1) ;

        // Before type checking its set correctly = PIG-421
        assertEquals(DataType.LONG, mod1.getType()) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        printTypeGraph(plan) ;

        // After type checking
        System.out.println(DataType.findTypeName(mod1.getType())) ;
        assertEquals(DataType.LONG, mod1.getType()) ;

        assertTrue(mod1.getLhsOperand() instanceof LOCast);
        assertEquals(((LOCast)mod1.getLhsOperand()).getType(), DataType.LONG);
        assertTrue(((LOCast)mod1.getLhsOperand()).getExpression() == constant1);
    }


    // Positive case
    @Test
    public void testRegexTypeChecking1() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), "10") ;
        LOConst constant2 = new LOConst(plan, genNewOperatorKey(), "Regex");
        constant1.setType(DataType.CHARARRAY) ;

        LORegexp regex = new LORegexp(plan, genNewOperatorKey()) ;
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(regex) ;

        plan.connect(constant1, regex) ;     
        plan.connect(constant2, regex) ;     

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        printTypeGraph(plan) ;

        // After type checking
        System.out.println(DataType.findTypeName(regex.getType())) ;
        assertEquals(DataType.BOOLEAN, regex.getType()) ;
    }

    // Positive case with cast insertion
    @Test
    public void testRegexTypeChecking2() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), new DataByteArray()) ;
        LOConst constant2 = new LOConst(plan, genNewOperatorKey(), "Regex");
        constant1.setType(DataType.BYTEARRAY) ;

        LORegexp regex = new LORegexp(plan, genNewOperatorKey());

        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(regex) ;

        plan.connect(constant1, regex) ;
        plan.connect(constant2, regex) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        printTypeGraph(plan) ;

        // After type checking

        if (collector.hasError()) {
            throw new Exception("Error not expected during type checking") ;
        }       
        
        // check type
        System.out.println(DataType.findTypeName(regex.getType())) ;
        assertEquals(DataType.BOOLEAN, regex.getType()) ;
                                         
        // check wiring      
        LOCast cast = (LOCast) regex.getOperand() ;      
        assertEquals(cast.getType(), DataType.CHARARRAY);    
        assertEquals(cast.getExpression(), constant1) ;
    }

        // Negative case
    @Test
    public void testRegexTypeChecking3() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        LOConst constant2 = new LOConst(plan, genNewOperatorKey(), "Regex");
        constant1.setType(DataType.INTEGER) ;

        LORegexp regex = new LORegexp(plan, genNewOperatorKey());
        
        plan.add(constant1) ;
        plan.add(constant2) ;
        plan.add(regex) ;

        plan.connect(constant1, regex) ;
        plan.connect(constant2, regex) ;

        try {
            CompilationMessageCollector collector = new CompilationMessageCollector() ;
            TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
            typeValidator.validate(plan, collector) ;
            printMessageCollector(collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

    }

    // Expression plan has to support DAG before this can be used.
    // Currently it supports only tree.

    /*
    @Test
    public void testDiamondShapedExpressionPlan1() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;
        LOConst constant1 = new LOConst(plan, genNewOperatorKey(), 10) ;
        constant1.setType(DataType.LONG) ;

        LONegative neg1 = new LONegative(plan, genNewOperatorKey(), constant1) ;
        LONegative neg2 = new LONegative(plan, genNewOperatorKey(), constant1) ;

        LODivide div1 = new LODivide(plan, genNewOperatorKey(), neg1, neg2) ;

        LONegative neg3 = new LONegative(plan, genNewOperatorKey(), div1) ;
        LONegative neg4 = new LONegative(plan, genNewOperatorKey(), div1) ;

        LOAdd add1 = new LOAdd(plan, genNewOperatorKey(), neg3, neg4) ;

        plan.add(constant1) ;
        plan.add(neg1) ;
        plan.add(neg2) ;
        plan.add(div1) ;
        plan.add(neg3) ;
        plan.add(neg4) ;
        plan.add(add1) ;

        plan.connect(constant1, neg1) ;
        plan.connect(constant1, neg2) ;

        plan.connect(neg1, div1) ;
        plan.connect(neg2, div1) ;

        plan.connect(div1, neg3) ;
        plan.connect(div1, neg3) ;

        plan.connect(neg3, add1) ;
        plan.connect(neg4, add1) ;

        // Before type checking
        assertEquals(DataType.UNKNOWN, add1.getType()) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        printTypeGraph(plan) ;

        // After type checking
        assertEquals(DataType.LONG, div1.getType()) ;
        assertEquals(DataType.LONG, add1.getType()) ;

    }
    */
    
    // This tests when both inputs need casting
    @Test
    public void testUnionCastingInsert1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;
        
        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;
        LOLoad load2 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            fsList1.add(new FieldSchema(null, DataType.BYTEARRAY)) ;
            fsList1.add(new FieldSchema(null, DataType.CHARARRAY)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // schema for input#2
        Schema inputSchema2 = null ;
        {
            List<FieldSchema> fsList2 = new ArrayList<FieldSchema>() ;
            fsList2.add(new FieldSchema("field1b", DataType.DOUBLE)) ;
            fsList2.add(new FieldSchema(null, DataType.INTEGER)) ;
            fsList2.add(new FieldSchema("field3b", DataType.FLOAT)) ;
            fsList2.add(new FieldSchema("field4b", DataType.CHARARRAY)) ;
            inputSchema2 = new Schema(fsList2) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(inputSchema2) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOUnion union = new LOUnion(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(union) ;

        plan.connect(load1, union);
        plan.connect(load2, union);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        // check end result schema
        Schema outputSchema = union.getSchema() ;

        Schema expectedSchema = null ;
        {
            List<FieldSchema> fsListExpected = new ArrayList<FieldSchema>() ;
            fsListExpected.add(new FieldSchema("field1a", DataType.DOUBLE)) ;
            fsListExpected.add(new FieldSchema("field2a", DataType.LONG)) ;
            fsListExpected.add(new FieldSchema("field3b", DataType.FLOAT)) ;
            fsListExpected.add(new FieldSchema("field4b", DataType.CHARARRAY)) ;
            expectedSchema = new Schema(fsListExpected) ;
        }

        assertTrue(Schema.equals(outputSchema, expectedSchema, true, false)) ;

        printTypeGraph(plan) ;

        // check the inserted casting of input1
        {
            // Check wiring
            List<LogicalOperator> sucList1 = plan.getSuccessors(load1) ;
            assertEquals(sucList1.size(), 1);
            LOForEach foreach = (LOForEach) sucList1.get(0) ;
            assertTrue(foreach instanceof LOForEach) ;

            List<LogicalOperator> sucList2 = plan.getSuccessors(foreach) ;
            assertEquals(sucList2.size(), 1);
            assertTrue(sucList2.get(0) instanceof LOUnion) ;

            // Check inserted casting
            checkForEachCasting(foreach, 0, true, DataType.DOUBLE) ;
            checkForEachCasting(foreach, 1, false, DataType.UNKNOWN) ;
            checkForEachCasting(foreach, 2, true, DataType.FLOAT) ;
            checkForEachCasting(foreach, 3, false, DataType.UNKNOWN) ;

        }

        // check the inserted casting of input2
        {
            // Check wiring
            List<LogicalOperator> sucList1 = plan.getSuccessors(load2) ;
            assertEquals(sucList1.size(), 1);
            LOForEach foreach = (LOForEach) sucList1.get(0) ;
            assertTrue(foreach instanceof LOForEach) ;

            List<LogicalOperator> sucList2 = plan.getSuccessors(foreach) ;
            assertEquals(sucList2.size(), 1);
            assertTrue(sucList2.get(0) instanceof LOUnion) ;

            // Check inserted casting
            checkForEachCasting(foreach, 0, false, DataType.UNKNOWN) ;
            checkForEachCasting(foreach, 1, true, DataType.LONG) ;
            checkForEachCasting(foreach, 2, false, DataType.UNKNOWN) ;
            checkForEachCasting(foreach, 3, false, DataType.UNKNOWN) ;

        }

    }


    // This tests when both only on input needs casting
    @Test
    public void testUnionCastingInsert2() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;
        LOLoad load2 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.BYTEARRAY)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // schema for input#2
        Schema inputSchema2 = null ;
        {
            List<FieldSchema> fsList2 = new ArrayList<FieldSchema>() ;
            fsList2.add(new FieldSchema("field1b", DataType.DOUBLE)) ;
            fsList2.add(new FieldSchema("field2b", DataType.DOUBLE)) ;
            inputSchema2 = new Schema(fsList2) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(inputSchema2) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOUnion union = new LOUnion(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(union) ;

        plan.connect(load1, union);
        plan.connect(load2, union);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        // check end result schema
        Schema outputSchema = union.getSchema() ;

        Schema expectedSchema = null ;
        {
            List<FieldSchema> fsListExpected = new ArrayList<FieldSchema>() ;
            fsListExpected.add(new FieldSchema("field1a", DataType.DOUBLE)) ;
            fsListExpected.add(new FieldSchema("field2a", DataType.DOUBLE)) ;
            expectedSchema = new Schema(fsListExpected) ;
        }

        assertTrue(Schema.equals(outputSchema, expectedSchema, true, false)) ;

        printTypeGraph(plan) ;

        // check the inserted casting of input1
        {
            // Check wiring
            List<LogicalOperator> sucList1 = plan.getSuccessors(load1) ;
            assertEquals(sucList1.size(), 1);
            LOForEach foreach = (LOForEach) sucList1.get(0) ;
            assertTrue(foreach instanceof LOForEach) ;

            List<LogicalOperator> sucList2 = plan.getSuccessors(foreach) ;
            assertEquals(sucList2.size(), 1);
            assertTrue(sucList2.get(0) instanceof LOUnion) ;

            // Check inserted casting
            checkForEachCasting(foreach, 0, true, DataType.DOUBLE) ;
            checkForEachCasting(foreach, 1, true, DataType.DOUBLE) ;

        }

        // check the inserted casting of input2
        {
            // Check wiring
            List<LogicalOperator> sucList1 = plan.getSuccessors(load2) ;
            assertEquals(sucList1.size(), 1);
            assertTrue(sucList1.get(0) instanceof LOUnion) ;
        }

    }
    
    // This has to fail under strict typing mode
    /*
    // This is a negative test
    // Two inputs cannot be merged due to incompatible schemas
    @Test
    public void testUnionCastingInsert3() throws Throwable {
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null) ;
        LOLoad load2 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.BYTEARRAY)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // schema for input#2
        Schema inputSchema2 = null ;
        {
            List<FieldSchema> fsList2 = new ArrayList<FieldSchema>() ;
            fsList2.add(new FieldSchema("field1b", DataType.CHARARRAY)) ;
            fsList2.add(new FieldSchema("field2b", DataType.DOUBLE)) ;
            inputSchema2 = new Schema(fsList2) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(inputSchema2) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOUnion union = new LOUnion(plan, genNewOperatorKey(), inputList) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(union) ;

        plan.connect(load1, union);
        plan.connect(load2, union);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }
        printMessageCollector(collector) ;

    }
    */

    @Test
    public void testDistinct1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> innerList = new ArrayList<FieldSchema>() ;
            innerList.add(new FieldSchema("innerfield1", DataType.BAG)) ;
            innerList.add(new FieldSchema("innerfield2", DataType.FLOAT)) ;
            Schema innerSchema = new Schema(innerList) ;

            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2", DataType.BYTEARRAY)) ;
            fsList1.add(new FieldSchema("field3", innerSchema)) ;
            fsList1.add(new FieldSchema("field4", DataType.BAG)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        LODistinct distinct1 = new LODistinct(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(distinct1) ;

        plan.connect(load1, distinct1);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;

        // check end result schema
        Schema outputSchema = distinct1.getSchema() ;
        assertTrue(Schema.equals(load1.getSchema(), outputSchema, false, false)) ;       
    }

    // Positive test
    @Test
    public void testFilterWithInnerPlan1() throws Throwable {
        testFilterWithInnerPlan(DataType.INTEGER, DataType.LONG) ;
    }

    // Positive test
    @Test
    public void testFilterWithInnerPlan2() throws Throwable {
        testFilterWithInnerPlan(DataType.INTEGER, DataType.BYTEARRAY) ;    
    }

    // Filter test helper
    public void testFilterWithInnerPlan(byte field1Type, byte field2Type) throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", field1Type)) ;
            fsList1.add(new FieldSchema("field2", field2Type)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create inner plan
        LogicalPlan innerPlan = new LogicalPlan() ;
        LOProject project1 = new LOProject(innerPlan, genNewOperatorKey(), load1, 0) ;
        project1.setSentinel(true);
        LOProject project2 = new LOProject(innerPlan, genNewOperatorKey(), load1, 1) ;
        project2.setSentinel(true);

        LOGreaterThan gt1 = new LOGreaterThan(innerPlan,
                                              genNewOperatorKey()) ;

        innerPlan.add(project1) ;
        innerPlan.add(project2) ;
        innerPlan.add(gt1) ;

        innerPlan.connect(project1, gt1) ;
        innerPlan.connect(project2, gt1) ;

        // filter
        LOFilter filter1 = new LOFilter(plan, genNewOperatorKey(), innerPlan) ;

        plan.add(load1);
        plan.add(filter1);
        plan.connect(load1, filter1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        Schema endResultSchema = filter1.getSchema() ;
        assertEquals(endResultSchema.getField(0).type, field1Type) ;
        assertEquals(endResultSchema.getField(1).type, field2Type) ;

    }

    // Negative test
    @Test
    public void testFilterWithInnerPlan3() throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2", DataType.LONG)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create inner plan
        LogicalPlan innerPlan = new LogicalPlan() ;
        LOProject project1 = new LOProject(innerPlan, genNewOperatorKey(), load1, 0) ;
        project1.setSentinel(true);
        LOProject project2 = new LOProject(innerPlan, genNewOperatorKey(), load1, 1) ;
        project2.setSentinel(true);

        LOAdd add1 = new LOAdd(innerPlan, genNewOperatorKey()) ;

        innerPlan.add(project1) ;
        innerPlan.add(project2) ;
        innerPlan.add(add1) ;

        innerPlan.connect(project1, add1) ;
        innerPlan.connect(project2, add1) ;

        // filter
        LOFilter filter1 = new LOFilter(plan, genNewOperatorKey(), innerPlan) ;

        plan.add(load1);
        plan.add(filter1);
        plan.connect(load1, filter1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Error expected") ;
        }
        catch (Exception t) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }


    // Simple project sort columns
    @Test
    public void testSortWithInnerPlan1() throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.LONG)) ;
            fsList1.add(new FieldSchema("field2", DataType.INTEGER)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create project inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project1 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project1.setSentinel(true);
        innerPlan1.add(project1) ;

        // Create project inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project2 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project1.setSentinel(true);
        innerPlan2.add(project2) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // List of ASC flags
        List<Boolean> ascList = new ArrayList<Boolean>() ;
        ascList.add(true);
        ascList.add(true);

        // Sort
        LOSort sort1 = new LOSort(plan, genNewOperatorKey(), innerPlans,  ascList, null) ;


        plan.add(load1);
        plan.add(sort1);
        plan.connect(load1, sort1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        Schema endResultSchema = sort1.getSchema() ;

        // outer
        assertEquals(endResultSchema.getField(0).type, DataType.LONG) ;
        assertEquals(endResultSchema.getField(1).type, DataType.INTEGER) ;

        // inner
        assertEquals(innerPlan1.getSingleLeafPlanOutputType(), DataType.INTEGER);
        assertEquals(innerPlan2.getSingleLeafPlanOutputType(), DataType.LONG);

    }

    // Positive expression sort columns
    @Test
    public void testSortWithInnerPlan2() throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.BYTEARRAY)) ;
            fsList1.add(new FieldSchema("field2", DataType.INTEGER)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LOMultiply mul1 = new LOMultiply(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(mul1) ;

        innerPlan1.connect(project11, mul1);
        innerPlan1.connect(project12, mul1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), 26L) ;
        const21.setType(DataType.LONG);
        LOMod mod21 = new LOMod(innerPlan2, genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(mod21) ;

        innerPlan2.connect(project21, mod21);
        innerPlan2.connect(const21, mod21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // List of ASC flags
        List<Boolean> ascList = new ArrayList<Boolean>() ;
        ascList.add(true);
        ascList.add(true);

        // Sort
        LOSort sort1 = new LOSort(plan, genNewOperatorKey(), innerPlans,  ascList, null) ;


        plan.add(load1);
        plan.add(sort1);
        plan.connect(load1, sort1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        Schema endResultSchema = sort1.getSchema() ;

        // outer
        assertEquals(endResultSchema.getField(0).type, DataType.BYTEARRAY) ;
        assertEquals(endResultSchema.getField(1).type, DataType.INTEGER) ;

        // inner
        assertEquals(innerPlan1.getSingleLeafPlanOutputType(), DataType.INTEGER);
        assertEquals(innerPlan2.getSingleLeafPlanOutputType(), DataType.LONG);

    }
    
    // Negative test on expression sort columns
    @Test
    public void testSortWithInnerPlan3() throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.BYTEARRAY)) ;
            fsList1.add(new FieldSchema("field2", DataType.INTEGER)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LOMultiply mul1 = new LOMultiply(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(mul1) ;

        innerPlan1.connect(project11, mul1);
        innerPlan1.connect(project12, mul1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), "26") ;
        const21.setType(DataType.CHARARRAY);
        LOMod mod21 = new LOMod(innerPlan2, genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(mod21) ;

        innerPlan2.connect(project21, mod21);
        innerPlan2.connect(const21, mod21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // List of ASC flags
        List<Boolean> ascList = new ArrayList<Boolean>() ;
        ascList.add(true);
        ascList.add(true);

        // Sort
        LOSort sort1 = new LOSort(plan, genNewOperatorKey(), innerPlans,  ascList, null) ;


        plan.add(load1);
        plan.add(sort1);
        plan.connect(load1, sort1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Error expected") ;
        }
        catch (Exception t) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (!collector.hasError()) {
            throw new AssertionError("Error expected") ;
        }

    }


    // Positive expression cond columns
    @Test
    public void testSplitWithInnerPlan1() throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.BYTEARRAY)) ;
            fsList1.add(new FieldSchema("field2", DataType.INTEGER)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LONotEqual notequal1 = new LONotEqual(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(notequal1) ;

        innerPlan1.connect(project11, notequal1);
        innerPlan1.connect(project12, notequal1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), 26) ;
        const21.setType(DataType.LONG);
        LOLesserThanEqual lesser21 = new LOLesserThanEqual(innerPlan2,
                                                           genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(lesser21) ;

        innerPlan2.connect(project21, lesser21);
        innerPlan2.connect(const21, lesser21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // split
        LOSplit split1 = new LOSplit(plan,
                                     genNewOperatorKey(),
                                     new ArrayList<LogicalOperator>());

        // output1
        LOSplitOutput splitOutput1 = new LOSplitOutput(plan, genNewOperatorKey(), 0, innerPlan1) ;
        split1.addOutput(splitOutput1);

        // output2
        LOSplitOutput splitOutput2 = new LOSplitOutput(plan, genNewOperatorKey(), 1, innerPlan2) ;
        split1.addOutput(splitOutput2);

        plan.add(load1);
        plan.add(split1);
        plan.add(splitOutput1);
        plan.add(splitOutput2);

        plan.connect(load1, split1) ;
        plan.connect(split1, splitOutput1) ;
        plan.connect(split1, splitOutput2) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check split itself
        {
            Schema endResultSchema1 = split1.getSchema() ;
            // outer
            assertEquals(endResultSchema1.getField(0).type, DataType.BYTEARRAY) ;
            assertEquals(endResultSchema1.getField(1).type, DataType.INTEGER) ;
        }

        // check split output #1
        {
            Schema endResultSchema1 = splitOutput1.getSchema() ;
            // outer
            assertEquals(endResultSchema1.getField(0).type, DataType.BYTEARRAY) ;
            assertEquals(endResultSchema1.getField(1).type, DataType.INTEGER) ;
        }

        // check split output #2
        {
            Schema endResultSchema2 = splitOutput2.getSchema() ;
            // outer
            assertEquals(endResultSchema2.getField(0).type, DataType.BYTEARRAY) ;
            assertEquals(endResultSchema2.getField(1).type, DataType.INTEGER) ;
        }

        // inner conditions: all have to be boolean
        assertEquals(innerPlan1.getSingleLeafPlanOutputType(), DataType.BOOLEAN);
        assertEquals(innerPlan2.getSingleLeafPlanOutputType(), DataType.BOOLEAN);

    }

    // Negative test: expression cond columns not evaluate to boolean
    @Test
    public void testSplitWithInnerPlan2() throws Throwable {

        // Create outer plan
        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1", DataType.BYTEARRAY)) ;
            fsList1.add(new FieldSchema("field2", DataType.INTEGER)) ;

            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LONotEqual notequal1 = new LONotEqual(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(notequal1) ;

        innerPlan1.connect(project11, notequal1);
        innerPlan1.connect(project12, notequal1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), 26) ;
        const21.setType(DataType.LONG);
        LOSubtract subtract21 = new LOSubtract(innerPlan2,
                                               genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(subtract21) ;

        innerPlan2.connect(project21, subtract21);
        innerPlan2.connect(const21, subtract21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // split
        LOSplit split1 = new LOSplit(plan,
                                     genNewOperatorKey(),
                                     new ArrayList<LogicalOperator>());

        // output1
        LOSplitOutput splitOutput1 = new LOSplitOutput(plan, genNewOperatorKey(), 0, innerPlan1) ;
        split1.addOutput(splitOutput1);

        // output2
        LOSplitOutput splitOutput2 = new LOSplitOutput(plan, genNewOperatorKey(), 1, innerPlan2) ;
        split1.addOutput(splitOutput2);

        plan.add(load1);
        plan.add(split1);
        plan.add(splitOutput1);
        plan.add(splitOutput2);

        plan.connect(load1, split1) ;
        plan.connect(split1, splitOutput1) ;
        plan.connect(split1, splitOutput2) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (Exception t) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (!collector.hasError()) {
            throw new AssertionError("Error expected") ;
        }

    }

    // Positive test
    @Test
    public void testCOGroupWithInnerPlan1GroupByTuple1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;
        LOLoad load2 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // schema for input#2
        Schema inputSchema2 = null ;
        {
            List<FieldSchema> fsList2 = new ArrayList<FieldSchema>() ;
            fsList2.add(new FieldSchema("field1b", DataType.DOUBLE)) ;
            fsList2.add(new FieldSchema(null, DataType.INTEGER)) ;
            inputSchema2 = new Schema(fsList2) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(inputSchema2) ;

        // Create expression inner plan #1 of input #1
        LogicalPlan innerPlan11 = new LogicalPlan() ;
        LOProject project111 = new LOProject(innerPlan11, genNewOperatorKey(), load1, 0) ;
        project111.setSentinel(true);
        LOConst const111 = new LOConst(innerPlan11, genNewOperatorKey(), 26F) ;
        const111.setType(DataType.FLOAT);
        LOSubtract subtract111 = new LOSubtract(innerPlan11,
                                                genNewOperatorKey()) ;

        innerPlan11.add(project111) ;
        innerPlan11.add(const111) ;
        innerPlan11.add(subtract111) ;

        innerPlan11.connect(project111, subtract111);
        innerPlan11.connect(const111, subtract111) ;

        // Create expression inner plan #2 of input #1
        LogicalPlan innerPlan21 = new LogicalPlan() ;
        LOProject project211 = new LOProject(innerPlan21, genNewOperatorKey(), load1, 0) ;
        project211.setSentinel(true);
        LOProject project212 = new LOProject(innerPlan21, genNewOperatorKey(), load1, 1) ;
        project212.setSentinel(true);

        LOAdd add211 = new LOAdd(innerPlan21,
                                 genNewOperatorKey()) ;

        innerPlan21.add(project211) ;
        innerPlan21.add(project212) ;
        innerPlan21.add(add211) ;

        innerPlan21.connect(project211, add211);
        innerPlan21.connect(project212, add211) ;


        // Create expression inner plan #1 of input #2
        LogicalPlan innerPlan12 = new LogicalPlan() ;
        LOProject project121 = new LOProject(innerPlan12, genNewOperatorKey(), load2, 0) ;
        project121.setSentinel(true);
        LOConst const121 = new LOConst(innerPlan12, genNewOperatorKey(), 26) ;
        const121.setType(DataType.INTEGER);
        LOSubtract subtract121 = new LOSubtract(innerPlan12,
                                                genNewOperatorKey()) ;

        innerPlan12.add(project121) ;
        innerPlan12.add(const121) ;
        innerPlan12.add(subtract121) ;

        innerPlan12.connect(project121, subtract121);
        innerPlan12.connect(const121, subtract121) ;

        // Create expression inner plan #2 of input #2
        LogicalPlan innerPlan22 = new LogicalPlan() ;
        LOConst const122 = new LOConst(innerPlan22, genNewOperatorKey(), 26) ;
        const122.setType(DataType.INTEGER);
        innerPlan22.add(const122) ;

        // Create Cogroup
        ArrayList<LogicalOperator> inputs = new ArrayList<LogicalOperator>() ;
        inputs.add(load1) ;
        inputs.add(load2) ;

        MultiMap<LogicalOperator, LogicalPlan> maps
                            = new MultiMap<LogicalOperator, LogicalPlan>() ;
        maps.put(load1, innerPlan11);
        maps.put(load1, innerPlan21);
        maps.put(load2, innerPlan12);
        maps.put(load2, innerPlan22);

        boolean[] isInner = new boolean[inputs.size()] ;
        for (int i=0; i < isInner.length ; i++) {
            isInner[i] = false ;
        }

        LOCogroup cogroup1 = new LOCogroup(plan,
                                           genNewOperatorKey(),
                                           maps,
                                           isInner) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cogroup1) ;

        plan.connect(load1, cogroup1);
        plan.connect(load2, cogroup1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }
        
        // check outer schema
        Schema endResultSchema = cogroup1.getSchema() ;

        // Tuple group column
        assertEquals(endResultSchema.getField(0).type, DataType.TUPLE) ;
        assertEquals(endResultSchema.getField(0).schema.getField(0).type, DataType.DOUBLE) ;
        assertEquals(endResultSchema.getField(0).schema.getField(1).type, DataType.LONG);

        assertEquals(endResultSchema.getField(1).type, DataType.BAG) ;
        assertEquals(endResultSchema.getField(2).type, DataType.BAG) ;

        // check inner schema1
        Schema innerSchema1 = endResultSchema.getField(1).schema ;
        assertEquals(innerSchema1.getField(0).type, DataType.INTEGER);
        assertEquals(innerSchema1.getField(1).type, DataType.LONG);

        // check inner schema2
        Schema innerSchema2 = endResultSchema.getField(2).schema ;
        assertEquals(innerSchema2.getField(0).type, DataType.DOUBLE);
        assertEquals(innerSchema2.getField(1).type, DataType.INTEGER);

        // check group by col end result
        assertEquals(innerPlan11.getSingleLeafPlanOutputType(), DataType.DOUBLE) ;
        assertEquals(innerPlan21.getSingleLeafPlanOutputType(), DataType.LONG) ;
        assertEquals(innerPlan12.getSingleLeafPlanOutputType(), DataType.DOUBLE) ;
        assertEquals(innerPlan22.getSingleLeafPlanOutputType(), DataType.LONG) ;
    }


    // Positive test
    @Test
    public void testCOGroupWithInnerPlan1GroupByAtom1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;
        LOLoad load2 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // schema for input#2
        Schema inputSchema2 = null ;
        {
            List<FieldSchema> fsList2 = new ArrayList<FieldSchema>() ;
            fsList2.add(new FieldSchema("field1b", DataType.DOUBLE)) ;
            fsList2.add(new FieldSchema(null, DataType.INTEGER)) ;
            inputSchema2 = new Schema(fsList2) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(inputSchema2) ;

        // Create expression inner plan #1 of input #1
        LogicalPlan innerPlan11 = new LogicalPlan() ;
        LOProject project111 = new LOProject(innerPlan11, genNewOperatorKey(), load1, 0) ;
        project111.setSentinel(true);
        LOConst const111 = new LOConst(innerPlan11, genNewOperatorKey(), 26F) ;
        const111.setType(DataType.FLOAT);
        LOSubtract subtract111 = new LOSubtract(innerPlan11,
                                                genNewOperatorKey()) ;

        innerPlan11.add(project111) ;
        innerPlan11.add(const111) ;
        innerPlan11.add(subtract111) ;

        innerPlan11.connect(project111, subtract111);
        innerPlan11.connect(const111, subtract111) ;

        // Create expression inner plan #1 of input #2
        LogicalPlan innerPlan12 = new LogicalPlan() ;
        LOProject project121 = new LOProject(innerPlan12, genNewOperatorKey(), load2, 0) ;
        project121.setSentinel(true);
        LOConst const121 = new LOConst(innerPlan12, genNewOperatorKey(), 26) ;
        const121.setType(DataType.INTEGER);
        LOSubtract subtract121 = new LOSubtract(innerPlan12,
                                                genNewOperatorKey()) ;

        innerPlan12.add(project121) ;
        innerPlan12.add(const121) ;
        innerPlan12.add(subtract121) ;

        innerPlan12.connect(project121, subtract121);
        innerPlan12.connect(const121, subtract121) ;

        // Create Cogroup
        ArrayList<LogicalOperator> inputs = new ArrayList<LogicalOperator>() ;
        inputs.add(load1) ;
        inputs.add(load2) ;

        MultiMap<LogicalOperator, LogicalPlan> maps
                            = new MultiMap<LogicalOperator, LogicalPlan>() ;
        maps.put(load1, innerPlan11);
        maps.put(load2, innerPlan12);

        boolean[] isInner = new boolean[inputs.size()] ;
        for (int i=0; i < isInner.length ; i++) {
            isInner[i] = false ;
        }

        LOCogroup cogroup1 = new LOCogroup(plan,
                                           genNewOperatorKey(),
                                           maps,
                                           isInner) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cogroup1) ;

        plan.connect(load1, cogroup1);
        plan.connect(load2, cogroup1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check outer schema
        Schema endResultSchema = cogroup1.getSchema() ;

        // Tuple group column
        assertEquals(endResultSchema.getField(0).type, DataType.DOUBLE) ;

        assertEquals(endResultSchema.getField(1).type, DataType.BAG) ;
        assertEquals(endResultSchema.getField(2).type, DataType.BAG) ;

        // check inner schema1
        Schema innerSchema1 = endResultSchema.getField(1).schema ;
        assertEquals(innerSchema1.getField(0).type, DataType.INTEGER);
        assertEquals(innerSchema1.getField(1).type, DataType.LONG);

        // check inner schema2
        Schema innerSchema2 = endResultSchema.getField(2).schema ;
        assertEquals(innerSchema2.getField(0).type, DataType.DOUBLE);
        assertEquals(innerSchema2.getField(1).type, DataType.INTEGER);

        // check group by col end result
        assertEquals(innerPlan11.getSingleLeafPlanOutputType(), DataType.DOUBLE) ;
        assertEquals(innerPlan12.getSingleLeafPlanOutputType(), DataType.DOUBLE) ;
    }


    // Positive test
    @Test
    public void testCOGroupWithInnerPlan1GroupByIncompatibleAtom1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;
        LOLoad load2 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // schema for input#2
        Schema inputSchema2 = null ;
        {
            List<FieldSchema> fsList2 = new ArrayList<FieldSchema>() ;
            fsList2.add(new FieldSchema("field1b", DataType.DOUBLE)) ;
            fsList2.add(new FieldSchema(null, DataType.INTEGER)) ;
            inputSchema2 = new Schema(fsList2) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(inputSchema2) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan11 = new LogicalPlan() ;
        LOProject project111 = new LOProject(innerPlan11, genNewOperatorKey(), load1, 0) ;
        project111.setSentinel(true);
        LOConst const111 = new LOConst(innerPlan11, genNewOperatorKey(), 26F) ;
        const111.setType(DataType.FLOAT);
        LOSubtract subtract111 = new LOSubtract(innerPlan11,
                                                genNewOperatorKey()) ;

        innerPlan11.add(project111) ;
        innerPlan11.add(const111) ;
        innerPlan11.add(subtract111) ;

        innerPlan11.connect(project111, subtract111);
        innerPlan11.connect(const111, subtract111) ;

        // Create expression inner plan #2
        LogicalPlan innerPlan12 = new LogicalPlan() ;
        LOConst const121 = new LOConst(innerPlan12, genNewOperatorKey(), 26) ;
        const121.setType(DataType.INTEGER);
        innerPlan12.add(const121) ;

        // Create Cogroup
        ArrayList<LogicalOperator> inputs = new ArrayList<LogicalOperator>() ;
        inputs.add(load1) ;
        inputs.add(load2) ;

        MultiMap<LogicalOperator, LogicalPlan> maps
                            = new MultiMap<LogicalOperator, LogicalPlan>() ;
        maps.put(load1, innerPlan11);
        maps.put(load2, innerPlan12);

        boolean[] isInner = new boolean[inputs.size()] ;
        for (int i=0; i < isInner.length ; i++) {
            isInner[i] = false ;
        }

        LOCogroup cogroup1 = new LOCogroup(plan,
                                           genNewOperatorKey(),
                                           maps,
                                           isInner) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cogroup1) ;

        plan.connect(load1, cogroup1);
        plan.connect(load2, cogroup1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check outer schema
        Schema endResultSchema = cogroup1.getSchema() ;

        // Tuple group column
        assertEquals(endResultSchema.getField(0).type, DataType.FLOAT) ;

        assertEquals(endResultSchema.getField(1).type, DataType.BAG) ;
        assertEquals(endResultSchema.getField(2).type, DataType.BAG) ;

        // check inner schema1
        Schema innerSchema1 = endResultSchema.getField(1).schema ;
        assertEquals(innerSchema1.getField(0).type, DataType.INTEGER);
        assertEquals(innerSchema1.getField(1).type, DataType.LONG);

        // check inner schema2
        Schema innerSchema2 = endResultSchema.getField(2).schema ;
        assertEquals(innerSchema2.getField(0).type, DataType.DOUBLE);
        assertEquals(innerSchema2.getField(1).type, DataType.INTEGER);

        // check group by col end result
        assertEquals(innerPlan11.getSingleLeafPlanOutputType(), DataType.FLOAT) ;
        assertEquals(innerPlan12.getSingleLeafPlanOutputType(), DataType.FLOAT) ;
    }

    // Positive test
    @Test
    public void testForEachGenerate1() throws Throwable {

        printCurrentMethodName() ;

        LogicalPlan plan = new LogicalPlan() ;
        LOLoad load1 = genDummyLOLoad(plan) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOConst const11 = new LOConst(innerPlan1, genNewOperatorKey(), 26F) ;
        const11.setType(DataType.FLOAT);
        LOSubtract subtract11 = new LOSubtract(innerPlan1,
                                                genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(const11) ;
        innerPlan1.add(subtract11) ;

        innerPlan1.connect(project11, subtract11);
        innerPlan1.connect(const11, subtract11) ;

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOProject project22 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 1) ;
        project21.setSentinel(true);
        LOAdd add21 = new LOAdd(innerPlan2,
                                genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(project22) ;
        innerPlan2.add(add21) ;

        innerPlan2.connect(project21, add21);
        innerPlan2.connect(project22, add21);

        // List of plans
        ArrayList<LogicalPlan> generatePlans = new ArrayList<LogicalPlan>() ;
        generatePlans.add(innerPlan1);
        generatePlans.add(innerPlan2);

        // List of flatten flags
        ArrayList<Boolean> flattens = new ArrayList<Boolean>() ;
        flattens.add(true) ;
        flattens.add(false) ;

        // Create LOForEach
        LOForEach foreach1 = new LOForEach(plan, genNewOperatorKey(), generatePlans, flattens) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(foreach1) ;

        plan.connect(load1, foreach1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check outer schema
        Schema endResultSchema = foreach1.getSchema() ;

        assertEquals(endResultSchema.getField(0).type, DataType.FLOAT) ;
        assertEquals(endResultSchema.getField(1).type, DataType.LONG) ;

    }

    // Negative test
    @Test
    public void testForEachGenerate2() throws Throwable {

        printCurrentMethodName() ;

        LogicalPlan plan = new LogicalPlan() ;
        LOLoad load1 = genDummyLOLoad(plan) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOConst const11 = new LOConst(innerPlan1, genNewOperatorKey(), "26F") ;
        const11.setType(DataType.CHARARRAY);
        LOSubtract subtract11 = new LOSubtract(innerPlan1,
                                                genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(const11) ;
        innerPlan1.add(subtract11) ;

        innerPlan1.connect(project11, subtract11);
        innerPlan1.connect(const11, subtract11) ;

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOProject project22 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 1) ;
        project21.setSentinel(true);
        LOAdd add21 = new LOAdd(innerPlan2,
                                genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(project22) ;
        innerPlan2.add(add21) ;

        innerPlan2.connect(project21, add21);
        innerPlan2.connect(project22, add21);

        // List of plans
        ArrayList<LogicalPlan> generatePlans = new ArrayList<LogicalPlan>() ;
        generatePlans.add(innerPlan1);
        generatePlans.add(innerPlan2);

        // List of flatten flags
        ArrayList<Boolean> flattens = new ArrayList<Boolean>() ;
        flattens.add(true) ;
        flattens.add(false) ;

        // Create LOForEach
        LOForEach foreach1 = new LOForEach(plan, genNewOperatorKey(), generatePlans, flattens) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(foreach1) ;

        plan.connect(load1, foreach1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    /*

    // Positive test
    // This one does project bag in inner plans
    @Test
    public void testForEachGenerate3() throws Throwable {

        printCurrentMethodName() ;

        LogicalPlan plan = new LogicalPlan() ;
        LOLoad load1 = genDummyLOLoad(plan) ;

        String[] aliases = new String[]{ "a", "b", "c" } ;
        byte[] types = new byte[] { DataType.INTEGER, DataType.LONG, DataType.BYTEARRAY } ;
        Schema innerSchema1 = genFlatSchema(aliases, types) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            fsList1.add(new FieldSchema("field3a", innerSchema1, DataType.BAG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1 of input #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 2) ;
        project11.setSentinel(true);
        List<Integer> projections1 = new ArrayList<Integer>() ;
        projections1.add(1) ;
        projections1.add(2) ;
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), project11, projections1) ;
        project12.setSentinel(false);

        innerPlan1.add(project12) ;

        // Create expression inner plan #1 of input #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOProject project22 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project21.setSentinel(true);
        LOAdd add21 = new LOAdd(innerPlan1,
                                genNewOperatorKey(),
                                project21,
                                project22) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(project22) ;
        innerPlan2.add(add21) ;

        innerPlan2.connect(project21, add21);
        innerPlan2.connect(project22, add21);

        // List of plans
        ArrayList<LogicalPlan> generatePlans = new ArrayList<LogicalPlan>() ;
        generatePlans.add(innerPlan1);
        generatePlans.add(innerPlan2);

        // List of flatten flags
        ArrayList<Boolean> flattens = new ArrayList<Boolean>() ;
        flattens.add(false) ;
        flattens.add(false) ;

        // Create LOGenerate
        LOGenerate generate1 = new LOGenerate(plan, genNewOperatorKey(), generatePlans, flattens) ;

        LogicalPlan foreachPlan = new LogicalPlan() ;
        foreachPlan.add(generate1) ;

        // Create LOForEach
        LOForEach foreach1 = new LOForEach(plan, genNewOperatorKey(), foreachPlan) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(foreach1) ;

        plan.connect(load1, foreach1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }
        
        // check outer schema
        Schema endResultSchema = foreach1.getSchema() ;
        assertEquals(endResultSchema.getField(0).type, DataType.BAG) ;
        assertEquals(endResultSchema.getField(1).type, DataType.LONG) ;

        // check inner bag schema
        Schema bagSchema = endResultSchema.getField(0).schema ;
        assertEquals(bagSchema.getField(0).type, DataType.LONG) ;
        assertEquals(bagSchema.getField(1).type, DataType.BYTEARRAY) ;

    }

    */

    // Positive test
    // This one does project bag in inner plans with flatten
    @Test
    public void testForEachGenerate4() throws Throwable {

        printCurrentMethodName() ;

        LogicalPlan plan = new LogicalPlan() ;
        LOLoad load1 = genDummyLOLoad(plan) ;

        String[] aliases = new String[]{ "a", "b", "c" } ;
        byte[] types = new byte[] { DataType.INTEGER, DataType.LONG, DataType.BYTEARRAY } ;
        Schema innerSchema1 = genFlatSchema(aliases, types) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.LONG)) ;
            fsList1.add(new FieldSchema("field3a", innerSchema1, DataType.BAG)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;

        // Create expression inner plan #1 of input #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 2) ;
        project11.setSentinel(true);
        List<Integer> projections1 = new ArrayList<Integer>() ;
        projections1.add(1) ;
        projections1.add(2) ;
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), project11, projections1) ;
        project12.setSentinel(false);

        innerPlan1.add(project12) ;

        // Create expression inner plan #1 of input #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOProject project22 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 1) ;
        project21.setSentinel(true);
        LOAdd add21 = new LOAdd(innerPlan2,
                                genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(project22) ;
        innerPlan2.add(add21) ;

        innerPlan2.connect(project21, add21);
        innerPlan2.connect(project22, add21);

        // List of plans
        ArrayList<LogicalPlan> generatePlans = new ArrayList<LogicalPlan>() ;
        generatePlans.add(innerPlan1);
        generatePlans.add(innerPlan2);

        // List of flatten flags
        ArrayList<Boolean> flattens = new ArrayList<Boolean>() ;
        flattens.add(true) ;
        flattens.add(false) ;

        // Create LOForEach
        LOForEach foreach1 = new LOForEach(plan, genNewOperatorKey(), generatePlans, flattens) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(foreach1) ;

        plan.connect(load1, foreach1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        // check outer schema
        Schema endResultSchema = foreach1.getSchema() ;
        assertEquals(endResultSchema.getField(0).type, DataType.LONG) ;
        assertEquals(endResultSchema.getField(1).type, DataType.BYTEARRAY) ;
        assertEquals(endResultSchema.getField(2).type, DataType.LONG) ;
    }

    @Test
    public void testCross1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        String[] aliases1 = new String[]{ "a", "b", "c" } ;
        byte[] types1 = new byte[] { DataType.INTEGER, DataType.LONG, DataType.BYTEARRAY } ;
        Schema schema1 = genFlatSchema(aliases1, types1) ;

        String[] aliases2 = new String[]{ "e", "f" } ;
        byte[] types2 = new byte[] { DataType.FLOAT, DataType.DOUBLE } ;
        Schema schema2 = genFlatSchema(aliases2, types2) ;

        // set schemas
        load1.setEnforcedSchema(schema1) ;
        load2.setEnforcedSchema(schema2) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOCross cross = new LOCross(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cross) ;

        plan.connect(load1, cross);
        plan.connect(load2, cross);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        assertEquals(cross.getSchema().size(), 5) ;
        assertEquals(cross.getSchema().getField(0).type, DataType.INTEGER);
        assertEquals(cross.getSchema().getField(1).type, DataType.LONG);
        assertEquals(cross.getSchema().getField(2).type, DataType.BYTEARRAY);
        assertEquals(cross.getSchema().getField(3).type, DataType.FLOAT);
        assertEquals(cross.getSchema().getField(4).type, DataType.DOUBLE);

    }

    @Test
    public void testLineage1() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1: int, field2: float, field3: chararray );") ;
        LogicalPlan plan = planTester.buildPlan("b = foreach a generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec() == null);

    }

    @Test
    public void testLineage1NoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a';") ;
        LogicalPlan plan = planTester.buildPlan("b = foreach a generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testLineage2() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        LogicalPlan plan = planTester.buildPlan("b = foreach a generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testGroupLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = group a by field1 ;") ;
        planTester.buildPlan("c = foreach b generate flatten(a) ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testGroupLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = group a by $0 ;") ;
        planTester.buildPlan("c = foreach b generate flatten(a) ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $0 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testGroupLineage2() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = group a by field1 ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate group + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testGroupLineage2NoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = group a by $0 ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate group + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testGroupLineageStar() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (name, age, gpa);") ;
        planTester.buildPlan("b = group a by *;") ;
        planTester.buildPlan("c = foreach b generate flatten(group);") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $0 + 1;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testGroupLineageStarNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = group a by *;") ;
        planTester.buildPlan("c = foreach b generate flatten(group);") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $0 + 1;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testCogroupLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupMapLookupLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, field1#'key' + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOMapLookup map = (LOMapLookup)foreachPlan.getSuccessors(exOp).get(0);
        LOCast cast = (LOCast)foreachPlan.getSuccessors(map).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupStarLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'b' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by *, b by * ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten($1), flatten($2);") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, field1 + 1, field4 + 2.0;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupStarLineageFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'b' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by *, b by * ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten($1), flatten($2);") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group + 1, field1 + 1, field4 + 2.0;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testCogroupStarLineage1() throws Throwable {
        planTester.buildPlan("a = load 'a' using PigStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'b' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by *, b by * ;") ;
        planTester.buildPlan("d = foreach c generate flatten(group), flatten($1), flatten($2);") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate $0 + 1, a::field1 + 1, field4 + 2.0;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

        foreachPlan = foreach.getForEachPlans().get(1);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupStarLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'b' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by *, b by * ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten($1), flatten($2);") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, $1 + 1, $2 + 2.0;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupStarLineageNoSchemaFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'b' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by *, b by * ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten($1), flatten($2);") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group + 1, $1 + 1, $2 + 2.0;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testCogroupMultiColumnProjectLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'b' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, a.(field1, field2), b.(field4);") ;
        planTester.buildPlan("e = foreach d generate group, flatten($1), flatten($2);") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, field1 + 1, field4 + 2.0;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupProjectStarLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'b' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate * ;") ;
        planTester.buildPlan("f = foreach d generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach f generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupProjectStarLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'b' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate * ;") ;
        planTester.buildPlan("f = foreach d generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach f generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));
    }

    @Test
    public void testCogroupProjectStarLineageMixSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'b' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by field1, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate * ;") ;
        planTester.buildPlan("f = foreach d generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach f generate group, field1 + 1, $4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            //not good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));
    }

    @Test
    public void testCogroupLineageFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group + 1, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    // The following test is commented out with PIG-505
    /*
    @Test
    public void testCogroupUDFLineageFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate flatten(DIFF(a, b)) as diff_a_b ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate diff_a_b + 1;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }
    */

    @Test
    public void testCogroupLineage2NoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testUnionLineage() throws Throwable {
        //here the type checker will insert a cast for the union, converting the column field2 into a float
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate field2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec() == null);

    }

    @Test
    public void testUnionLineageFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate field1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testUnionLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));
    }

    @Test
    public void testUnionLineageNoSchemaFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testUnionLineageDifferentSchema() throws Throwable {
        //here the type checker will insert a cast for the union, converting the column field2 into a float
        planTester.buildPlan("a = load 'a' using PigStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray, field7 );") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testUnionLineageDifferentSchemaFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray, field7 );") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testUnionLineageMixSchema() throws Throwable {
        //here the type checker will insert a cast for the union, converting the column field2 into a float
        planTester.buildPlan("a = load 'a' using PigStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testUnionLineageMixSchemaFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = union a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testFilterLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        LogicalPlan plan = planTester.buildPlan("b = filter a by field1 > 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOFilter filter = (LOFilter)plan.getLeaves().get(0);
        LogicalPlan filterPlan = filter.getComparisonPlan();

        LogicalOperator exOp = filterPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = filterPlan.getRoots().get(1);

        LOCast cast = (LOCast)filterPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testFilterLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        LogicalPlan plan = planTester.buildPlan("b = filter a by $0 > 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOFilter filter = (LOFilter)plan.getLeaves().get(0);
        LogicalPlan filterPlan = filter.getComparisonPlan();

        LogicalOperator exOp = filterPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = filterPlan.getRoots().get(1);

        LOCast cast = (LOCast)filterPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testFilterLineage1() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = filter a by field2 > 1.0 ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testFilterLineage1NoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        planTester.buildPlan("b = filter a by $0 > 1.0 ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testCogroupFilterLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = filter d by field4 > 5;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupFilterLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = filter d by $2 > 5;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testSplitLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        LogicalPlan plan = planTester.buildPlan("split a into b if field1 > 1.0, c if field1 <= 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOSplitOutput splitOutputB = (LOSplitOutput)plan.getLeaves().get(0);
        LogicalPlan bPlan = splitOutputB.getConditionPlan();

        LogicalOperator exOp = bPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = bPlan.getRoots().get(1);

        LOCast cast = (LOCast)bPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

        LOSplitOutput splitOutputC = (LOSplitOutput)plan.getLeaves().get(0);
        LogicalPlan cPlan = splitOutputC.getConditionPlan();

        exOp = cPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = cPlan.getRoots().get(1);

        cast = (LOCast)cPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));
    }

    @Test
    public void testSplitLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        LogicalPlan plan = planTester.buildPlan("split a into b if $0 > 1.0, c if $1 <= 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOSplitOutput splitOutputB = (LOSplitOutput)plan.getLeaves().get(0);
        LogicalPlan bPlan = splitOutputB.getConditionPlan();

        LogicalOperator exOp = bPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = bPlan.getRoots().get(1);

        LOCast cast = (LOCast)bPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

        LOSplitOutput splitOutputC = (LOSplitOutput)plan.getLeaves().get(0);
        LogicalPlan cPlan = splitOutputC.getConditionPlan();

        exOp = cPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = cPlan.getRoots().get(1);

        cast = (LOCast)cPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testSplitLineage1() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("split a into b if field2 > 1.0, c if field2 <= 1.0 ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testSplitLineage1NoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        planTester.buildPlan("split a into b if $0 > 1.0, c if $1 <= 1.0 ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testCogroupSplitLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("split d into e if field4 > 'm', f if field6 > 'm'  ;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach e generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupSplitLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("split d into e if $1 > 'm', f if $1 > 'm'  ;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach e generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testDistinctLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = distinct a;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testDistinctLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        planTester.buildPlan("b = distinct a;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testCogroupDistinctLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = distinct d ;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupDistinctLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = distinct d ;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testSortLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = order a by field1;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testSortLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        planTester.buildPlan("b = order a by $1;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testCogroupSortLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = order d by field4 desc;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupSortLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = order d by $2 desc;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupSortStarLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = order d by * desc;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupSortStarLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = order d by * desc;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCrossLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cross a, b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(1);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));
    }

    @Test
    public void testCrossLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("c = cross a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));
    }

    @Test
    public void testCrossLineageNoSchemaFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cross a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testCrossLineageMixSchema() throws Throwable {
        //here the type checker will insert a cast for the union, converting the column field2 into a float
        planTester.buildPlan("a = load 'a' using PigStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cross a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCrossLineageMixSchemaFail() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cross a , b ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (!collector.hasError()) {
            throw new AssertionError("Expect error") ;
        }

    }

    @Test
    public void testJoinLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = join a by field1, b by field4 ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(1);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));
    }

    @Test
    public void testJoinLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("c = join a by $0, b by $0 ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));
    }

    @Test
    public void testJoinLineageNoSchemaFail() throws Throwable {
        //this test case should change when we decide on what flattening a tuple or bag
        //with null schema results in a foreach flatten and hence a join
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = join a by $0, b by $0 ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $1 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));
    }

    @Test
    public void testJoinLineageMixSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using PigStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = join a by field1, b by $0 ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testJoinLineageMixSchemaFail() throws Throwable {
        //this test case should change when we decide on what flattening a tuple or bag
        //with null schema results in a foreach flatten and hence a join
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = join a by field1, b by $0 ;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c generate $3 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;

        try {
            typeValidator.validate(plan, collector) ;
        }
        catch (PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));
    }

    @Test
    public void testLimitLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = limit a 100;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate field1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testLimitLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' ;") ;
        planTester.buildPlan("b = limit a 100;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testCogroupLimitLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = limit d 100;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupLimitLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = limit d 100;") ;
        LogicalPlan plan = planTester.buildPlan("f = foreach e generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupTopKLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = load 'a' using PigStorage() as (field4, field5, field6: chararray );") ;
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = order d by field1 desc;") ;
        planTester.buildPlan("f = limit e 100;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach f generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testCogroupTopKLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = load 'a' using PigStorage() ;") ;
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        planTester.buildPlan("e = order d by $2 desc;") ;
        planTester.buildPlan("f = limit e 100;") ;
        LogicalPlan plan = planTester.buildPlan("g = foreach f generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("PigStorage"));

    }

    @Test
    public void testStreamingLineage1() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1: int, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = stream a through `" + simpleEchoStreamingCommand + "`;");
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $1 + 1.0 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testStreamingLineage2() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1: int, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = stream a through `" + simpleEchoStreamingCommand + "` as (f1, f2: float);");
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate f1 + 1.0, f2 + 4 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

        foreachPlan = foreach.getForEachPlans().get(1);

        exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        assertTrue(foreachPlan.getSuccessors(exOp).get(0) instanceof LOAdd);
    }

    @Test
    public void testCogroupStreamingLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = stream a through `" + simpleEchoStreamingCommand + "` as (field4, field5, field6: chararray);");
        planTester.buildPlan("c = cogroup a by field1, b by field4 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, field1 + 1, field4 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testCogroupStreamingLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = stream a through `" + simpleEchoStreamingCommand + "` ;");
        planTester.buildPlan("c = cogroup a by $0, b by $0 ;") ;
        planTester.buildPlan("d = foreach c generate group, flatten(a), flatten(b)  ;") ;
        LogicalPlan plan = planTester.buildPlan("e = foreach d generate group, $1 + 1, $2 + 2.0  ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOCast cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

        foreachPlan = foreach.getForEachPlans().get(2);
        exOp = foreachPlan.getRoots().get(0);
        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);
        cast = (LOCast)foreachPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testMapLookupLineage() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() as (field1, field2: float, field3: chararray );") ;
        planTester.buildPlan("b = foreach a generate field1#'key1' as map1;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate map1#'key2' + 1 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOMapLookup map = (LOMapLookup)foreachPlan.getSuccessors(exOp).get(0);
        LOCast cast = (LOCast)foreachPlan.getSuccessors(map).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testMapLookupLineageNoSchema() throws Throwable {
        planTester.buildPlan("a = load 'a' using BinStorage() ;") ;
        planTester.buildPlan("b = foreach a generate $0#'key1';") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate $0#'key2' + 1 ;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(0);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = foreachPlan.getRoots().get(1);

        LOMapLookup map = (LOMapLookup)foreachPlan.getSuccessors(exOp).get(0);
        LOCast cast = (LOCast)foreachPlan.getSuccessors(map).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("BinStorage"));

    }

    @Test
    public void testMapLookupLineage2() throws Throwable {
        planTester.buildPlan("a = load 'a' as (s, m, l);") ;
        planTester.buildPlan("b = foreach a generate s#'x' as f1, s#'y' as f2, s#'z' as f3;") ;
        planTester.buildPlan("c = group b by f1;") ;
        LogicalPlan plan = planTester.buildPlan("d = foreach c {fil = filter b by f2 == 1; generate flatten(group), SUM(fil.f3);};") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }


        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        LogicalPlan foreachPlan = foreach.getForEachPlans().get(1);

        LogicalOperator exOp = foreachPlan.getRoots().get(0);

        LOFilter filter = (LOFilter)foreachPlan.getSuccessors(exOp).get(0);
        LogicalPlan filterPlan = filter.getComparisonPlan();

        exOp = filterPlan.getRoots().get(0);

        if(! (exOp instanceof LOProject)) exOp = filterPlan.getRoots().get(1);

        
        LOCast cast = (LOCast)filterPlan.getSuccessors(exOp).get(0);
        assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));

    }

    @Test
    public void testMapLookupLineage3() throws Throwable {
        planTester.buildPlan("a = load 'a' as (s, m, l);") ;
        planTester.buildPlan("b = foreach a generate s#'src_spaceid' AS vspaceid, flatten(l#'viewinfo') as viewinfo ;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate (chararray)vspaceid#'foo', (chararray)viewinfo#'pos' as position;") ;

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());

        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }

        CastFinder cf = new CastFinder(plan);
        cf.visit();
        List<LOCast> casts = cf.casts;
        for (LOCast cast : casts) {
            assertTrue(cast.getLoadFuncSpec().getClassName().startsWith("org.apache.pig.builtin.PigStorage"));    
        }
    }
    
    class CastFinder extends LOVisitor {
        List<LOCast> casts = new ArrayList<LOCast>();
        /**
         * 
         */
        public CastFinder(LogicalPlan lp) {
            // TODO Auto-generated constructor stub
            super(lp, new DepthFirstWalker<LogicalOperator, LogicalPlan>(lp));
        }
        
        /* (non-Javadoc)
         * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOCast)
         */
        @Override
        protected void visit(LOCast cast) throws VisitorException {
            casts.add(cast);
        }
    }
    
    @Test
    public void testBincond() throws Throwable {
        planTester.buildPlan("a = load 'a' as (name: chararray, age: int, gpa: float);") ;
        planTester.buildPlan("b = group a by name;") ;
        LogicalPlan plan = planTester.buildPlan("c = foreach b generate (IsEmpty(a) ? " + TestBinCondFieldSchema.class.getName() + "(*): a) ;") ;
    
        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        
        typeValidator.validate(plan, collector) ;
    
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());
    
        if (collector.hasError()) {
            throw new AssertionError("Did not expect an error") ;
        }
    
    
        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        
        Schema.FieldSchema charFs = new FieldSchema(null, DataType.CHARARRAY);
        Schema.FieldSchema intFs = new FieldSchema(null, DataType.INTEGER);
        Schema.FieldSchema floatFs = new FieldSchema(null, DataType.FLOAT);
        Schema bagSchema = new Schema();
        bagSchema.add(charFs);
        bagSchema.add(intFs);
        bagSchema.add(floatFs);
        Schema.FieldSchema bagFs = null;
        try {
            bagFs = new Schema.FieldSchema(null, bagSchema, DataType.BAG);
        } catch (FrontendException fee) {
            fail("Did not expect an error");
        }
        
        Schema expectedSchema = new Schema(bagFs);
        
        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));
    
    }

    @Test
    public void testBinCondForOuterJoin() throws Throwable {
        planTester.buildPlan("a = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);");
        planTester.buildPlan("b = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararray, contributions: float);");
        planTester.buildPlan("c = COGROUP a BY name, b BY name;");
        LogicalPlan plan = planTester.buildPlan("d = FOREACH c GENERATE group, flatten((not IsEmpty(a) ? a : (bag{tuple(chararray, int, float)}){(null, null, null)})), flatten((not IsEmpty(b) ? b : (bag{tuple(chararray, int, chararray, float)}){(null,null,null, null)}));");
    
        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
    
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;
        planTester.printPlan(plan, TypeCheckingTestUtil.getCurrentMethodName());
    
        if (collector.hasError()) {
            throw new AssertionError("Expect no  error") ;
        }
    
    
        LOForEach foreach = (LOForEach)plan.getLeaves().get(0);
        String expectedSchemaString = "mygroup: chararray,A::name: chararray,A::age: int,A::gpa: float,B::name: chararray,B::age: int,B::registration: chararray,B::contributions: float";
        Schema expectedSchema = Util.getSchemaFromString(expectedSchemaString);
        assertTrue(Schema.equals(foreach.getSchema(), expectedSchema, false, true));
    
    }

    /*
     * A test UDF that does not data processing but implements the getOutputSchema for
     * checking the type checker
     */
    public static class TestBinCondFieldSchema extends EvalFunc<DataBag> {
        //no-op exec method
        public DataBag exec(Tuple input) {
            return null;
        }
        
        @Override
        public Schema outputSchema(Schema input) {
            Schema.FieldSchema charFs = new FieldSchema(null, DataType.CHARARRAY);
            Schema.FieldSchema intFs = new FieldSchema(null, DataType.INTEGER);
            Schema.FieldSchema floatFs = new FieldSchema(null, DataType.FLOAT);
            Schema bagSchema = new Schema();
            bagSchema.add(charFs);
            bagSchema.add(intFs);
            bagSchema.add(floatFs);
            Schema.FieldSchema bagFs;
            try {
                bagFs = new Schema.FieldSchema(null, bagSchema, DataType.BAG);
            } catch (FrontendException fee) {
                return null;
            }
            return new Schema(bagFs);
        }
    }
    
    ////////////////////////// Helper //////////////////////////////////
    private void checkForEachCasting(LOForEach foreach, int idx, boolean isCast, byte toType) {
        LogicalPlan plan = foreach.getForEachPlans().get(idx) ;

        if (isCast) {
            List<LogicalOperator> leaveList = plan.getLeaves() ;
            assertEquals(leaveList.size(), 1);
            assertTrue(leaveList.get(0) instanceof LOCast);
            assertTrue(leaveList.get(0).getType() == toType) ;
        }
        else {
            List<LogicalOperator> leaveList = plan.getLeaves() ;
            assertEquals(leaveList.size(), 1);
            assertTrue(leaveList.get(0) instanceof LOProject);
        }
        
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.Iterator;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;

import org.apache.pig.impl.plan.PlanException;
import org.apache.pig.impl.plan.DependencyOrderWalker;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.MultiMap;

/**
 * LogicalPlanCloneHelper implements a visitor mechanism to clone a logical plan
 * and then patch up the connections held within the operators of the logical plan.
 * This class should not be used for cloning the logical plan. Use {@link LogicalPlanCloner}
 * instead.
 */
public class LogicalPlanCloneHelper extends LOVisitor {
    
    public static Map<LogicalOperator, LogicalOperator> mOpToCloneMap;
    private LogicalPlan mOriginalPlan;

    /**
     * @param plan logical plan to be cloned
     */
    public LogicalPlanCloneHelper(LogicalPlan plan) throws CloneNotSupportedException {
        super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(plan));
        mOriginalPlan = plan;
        //LOVisitor does not have a default constructor and super needs to be the first
        //statement in the constructor. As a result, mPlan and mCurrentWalker are being
        //re-initialized here
        mPlan = plan.clone();
        mCurrentWalker = new DependencyOrderWalker<LogicalOperator, LogicalPlan>(mPlan);
    }
    
    /**
     * @param plan
     * @param origCloneMap the lookup table used for tracking operators cloned in the plan
     */
    public LogicalPlanCloneHelper(LogicalPlan plan,
            Map<LogicalOperator, LogicalOperator> origCloneMap) throws CloneNotSupportedException {
        super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(plan));
        mOpToCloneMap = origCloneMap;
        mOriginalPlan = plan;
        //LOVisitor does not have a default constructor and super needs to be the first
        //statement in the constructor. As a result, mPlan and mCurrentWalker are being
        //re-initialized here
        mPlan = plan.clone();
        mCurrentWalker = new DependencyOrderWalker<LogicalOperator, LogicalPlan>(mPlan);
    }
    
    public LogicalPlan getClonedPlan() throws CloneNotSupportedException {
        
        // set the "mPlan" member of all the Logical operators 
        // in the cloned plan to the cloned plan
        PlanSetter ps = new PlanSetter(mPlan);
        try {
            ps.visit();
            //patch up the connections
            this.visit();
        
        } catch (VisitorException e) {
            CloneNotSupportedException cnse = new CloneNotSupportedException("Unable to set plan correctly during cloning");
            cnse.initCause(e);
            throw cnse;
        }

        return mPlan;
    }

    public static void resetState() {
        mOpToCloneMap.clear();
    }
    
    @Override
    public void visit(BinaryExpressionOperator binOp) {
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOAdd)
     */
    @Override
    public void visit(LOAdd op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOAnd)
     */
    @Override
    public void visit(LOAnd binOp) throws VisitorException {
        this.visit((BinaryExpressionOperator)binOp);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOBinCond)
     */
    @Override
    protected void visit(LOBinCond binCond) throws VisitorException {
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOCast)
     */
    @Override
    protected void visit(LOCast cast) throws VisitorException {
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOCogroup)
     */
    @Override
    protected void visit(LOCogroup cg) throws VisitorException {
        MultiMap<LogicalOperator, LogicalPlan> groupByPlans = cg.getGroupByPlans();
        MultiMap<LogicalOperator, LogicalPlan> groupByPlansClone = new MultiMap<LogicalOperator, LogicalPlan>();
        for(LogicalOperator cgInput: groupByPlans.keySet()) {
            LogicalOperator cgInputClone = mOpToCloneMap.get(cgInput);
            if(cgInputClone != null) {
                groupByPlansClone.put(cgInputClone, groupByPlans.get(cgInput));
            } else {
                groupByPlansClone.put(cgInput, groupByPlans.get(cgInput));
            }
        }
        cg.setGroupByPlans(groupByPlansClone);
        super.visit(cg);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOConst)
     */
    @Override
    protected void visit(LOConst constant) throws VisitorException {
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOCross)
     */
    @Override
    protected void visit(LOCross cs) throws VisitorException {
        super.visit(cs);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LODistinct)
     */
    @Override
    protected void visit(LODistinct dt) throws VisitorException {
        super.visit(dt);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LODivide)
     */
    @Override
    public void visit(LODivide op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOEqual)
     */
    @Override
    public void visit(LOEqual op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOFilter)
     */
    @Override
    protected void visit(LOFilter filter) throws VisitorException {
        super.visit(filter);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOForEach)
     */
    @Override
    protected void visit(LOForEach forEach) throws VisitorException {
        super.visit(forEach);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOGenerate)
     */
    @Override
    protected void visit(LOGenerate g) throws VisitorException {
        super.visit(g);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LogicalOperator)
     */
    @Override
    protected void visit(LogicalOperator op) throws VisitorException {
        super.visit(op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOGreaterThan)
     */
    @Override
    public void visit(LOGreaterThan op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOGreaterThanEqual)
     */
    @Override
    public void visit(LOGreaterThanEqual op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOIsNull)
     */
    @Override
    public void visit(LOIsNull uniOp) throws VisitorException {
        this.visit((UnaryExpressionOperator) uniOp);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOLesserThan)
     */
    @Override
    public void visit(LOLesserThan op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOLesserThanEqual)
     */
    @Override
    public void visit(LOLesserThanEqual op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOLimit)
     */
    @Override
    protected void visit(LOLimit limOp) throws VisitorException {
        super.visit(limOp);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOLoad)
     */
    @Override
    protected void visit(LOLoad load) throws VisitorException {
        //TODO
        //LOLoad cloning is not implemented
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOMapLookup)
     */
    @Override
    public void visit(LOMapLookup op) throws VisitorException {
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOMod)
     */
    @Override
    public void visit(LOMod op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOMultiply)
     */
    @Override
    public void visit(LOMultiply op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LONegative)
     */
    @Override
    public void visit(LONegative op) throws VisitorException {
        this.visit((UnaryExpressionOperator) op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LONot)
     */
    @Override
    public void visit(LONot uniOp) throws VisitorException {
        this.visit((UnaryExpressionOperator) uniOp);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LONotEqual)
     */
    @Override
    public void visit(LONotEqual op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOOr)
     */
    @Override
    public void visit(LOOr binOp) throws VisitorException {
        this.visit((BinaryExpressionOperator)binOp);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOProject)
     */
    @Override
    protected void visit(LOProject project) throws VisitorException {
        LogicalOperator projectInputClone = mOpToCloneMap.get(project.getExpression());
        if(projectInputClone != null) {
            project.setExpression(projectInputClone);
        }
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LORegexp)
     */
    @Override
    protected void visit(LORegexp binOp) throws VisitorException {
        this.visit((BinaryExpressionOperator)binOp);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOSort)
     */
    @Override
    protected void visit(LOSort s) throws VisitorException {
        super.visit(s);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOSplit)
     */
    @Override
    protected void visit(LOSplit split) throws VisitorException {
        List<LogicalOperator> splitOutputs = split.getOutputs();
        ArrayList<LogicalOperator> splitOutputClones = new ArrayList<LogicalOperator>(splitOutputs.size());
        for(LogicalOperator splitOutput: splitOutputs) {
            LogicalOperator splitOutputClone = mOpToCloneMap.get(splitOutput);
            if(splitOutputClone != null) {
                splitOutputClones.add(splitOutputClone);
            } else {
                splitOutputClones.add(splitOutput);
            }
        }
        split.setOutputs(splitOutputClones);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOSplitOutput)
     */
    @Override
    protected void visit(LOSplitOutput sop) throws VisitorException {
        super.visit(sop);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOStore)
     */
    @Override
    protected void visit(LOStore store) throws VisitorException {
        //TODO
        //LOStore cloning is not implemented
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOStream)
     */
    @Override
    protected void visit(LOStream stream) throws VisitorException {
        //TODO
        //LOStream cloning is not implemented
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOSubtract)
     */
    @Override
    public void visit(LOSubtract op) throws VisitorException {
        this.visit((BinaryExpressionOperator)op);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOUnion)
     */
    @Override
    protected void visit(LOUnion u) throws VisitorException {
        super.visit(u);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.LOUserFunc)
     */
    @Override
    protected void visit(LOUserFunc func) throws VisitorException {
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LOVisitor#visit(org.apache.pig.impl.logicalLayer.UnaryExpressionOperator)
     */
    @Override
    protected void visit(UnaryExpressionOperator uniOp) throws VisitorException {
    }
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.plan.optimizer;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Queue;
import java.util.Set;

import org.apache.pig.PigException;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorPlan;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.plan.optimizer.RuleOperator.NodeType;
import org.apache.pig.impl.util.Pair;

/**
 * RuleMatcher contains the logic to determine whether a given rule matches.
 * This alone does not mean the rule will be applied.  Transformer.check()
 * still has to pass before Transfomer.transform() is called. 
 *
 */

public class RuleMatcher<O extends Operator, P extends OperatorPlan<O>> {

    private Rule<O, P> mRule;
    private List<Pair<O, RuleOperator.NodeType>> mMatch;
    private List<List<Pair<O, RuleOperator.NodeType>>> mPrelimMatches = new ArrayList<List<Pair<O, RuleOperator.NodeType>>>();
    private List<List<O>> mMatches = new ArrayList<List<O>>();
    private P mPlan; // for convenience.
    private int mNumCommonNodes = 0;
    private List<RuleOperator> mCommonNodes = null;

    /**
     * Test a rule to see if it matches the current plan. Save all matched nodes using BFS
     * @param rule Rule to test for a match.
     * @return true if the plan matches.
     */
    public boolean match(Rule<O, P> rule) throws OptimizerException {
        mRule = rule;
        CommonNodeFinder commonNodeFinder = new CommonNodeFinder(mRule.getPlan());
        try {
            commonNodeFinder.visit();
            mNumCommonNodes = commonNodeFinder.getCount();
            mCommonNodes = commonNodeFinder.getCommonNodes();
        } catch (VisitorException ve) {
            int errCode = 2125;
            String msg = "Internal error. Problem in computing common nodes in the Rule Plan.";
            throw new OptimizerException(msg, errCode, PigException.BUG, ve);
        }
        mPlan = mRule.getTransformer().getPlan();
        mMatches.clear();
        mPrelimMatches.clear();
        
        if (mRule.getWalkerAlgo() == Rule.WalkerAlgo.DependencyOrderWalker)
        	DependencyOrderWalker();
        else if (mRule.getWalkerAlgo() == Rule.WalkerAlgo.DepthFirstWalker)
        	DepthFirstWalker();        
        
        return (mMatches.size()!=0);
    }
    
    private void DependencyOrderWalker()
    {
        List<O> fifo = new ArrayList<O>();
        Set<O> seen = new HashSet<O>();
        List<O> leaves = mPlan.getLeaves();
        if (leaves == null) return;
        for (O op : leaves) {
        	BFSDoAllPredecessors(op, seen, fifo);
        }

        for (O op: fifo) {
        	if (beginMatch(op))
			{
        		mPrelimMatches.add(mMatch);
			}
        }
        
        if(mPrelimMatches.size() > 0) {
            processPreliminaryMatches();
        }
    }
    
    /**
     * A method to compute the final matches
     */
    private void processPreliminaryMatches() {
        //The preliminary matches contain paths that match
        //the specification in the RulePlan. However, if there
        //are twigs and DAGs, then a further computation is required
        //to extract the nodes in the mPlan that correspond to the
        //roots of the RulePlan
        
        //compute the number of common nodes in each preliminary match
        
        List<List<O>> commonNodesPerMatch = new ArrayList<List<O>>();
        for(int i = 0; i < mPrelimMatches.size(); ++i) {
            commonNodesPerMatch.add(getCommonNodesFromMatch(mPrelimMatches.get(i)));
        }
        
        if(mNumCommonNodes == 0) {
            //the rule plan had simple paths
            
            //verification step
            //if any of the preliminary matches had common nodes 
            //then its an anomaly
            
            for(int i = 0; i < commonNodesPerMatch.size(); ++i) {
                if(commonNodesPerMatch.get(i) != null) {
                    //we have found common nodes when there should be none
                    //just return as mMatches will be empty
                    return;
                }
            }
            
            //pick the first node of each match and put them into individual lists
            //put the lists inside the list of lists mMatches
            
            for(int i = 0; i < mPrelimMatches.size(); ++i) {
                List<O> match = new ArrayList<O>();
                match.add(mPrelimMatches.get(i).get(0).first);
                mMatches.add(match);
            }
            //all the matches have been computed for the simple path
            return;
        } else {
            for(int i = 0; i < commonNodesPerMatch.size(); ++i) {
                int commonNodes = (commonNodesPerMatch.get(i) == null? 0 : commonNodesPerMatch.get(i).size());
                if(commonNodes != mNumCommonNodes) {
                    //if there are is a mismatch in the common nodes then we have a problem
                    //the rule plan states that we have mNumCommonNodes but we have commonNodes 
                    //in the match. Just return
                    
                    return;
                }
            }
        }
        
        //keep track of the matches that have been processed
        List<Boolean> processedMatches = new ArrayList<Boolean>();
        for(int i = 0; i < mPrelimMatches.size(); ++i) {
            processedMatches.add(false);
        }
        
        //a do while loop to handle single matches
        int outerIndex = 0;
        do {
            
            if(processedMatches.get(outerIndex)) {
               ++outerIndex;
               continue;
            }
            
            List<Pair<O, RuleOperator.NodeType>> outerMatch = mPrelimMatches.get(outerIndex);
            List<O> outerCommonNodes = commonNodesPerMatch.get(outerIndex);
            Set<O> outerSetCommonNodes = new HashSet<O>(outerCommonNodes);
            Set<O> finalIntersection = new HashSet<O>(outerCommonNodes);
            Set<O> cumulativeIntersection = new HashSet<O>(outerCommonNodes);
            List<O> patternMatchingRoots = new ArrayList<O>();
            Set<O> unionOfRoots = new HashSet<O>();
            boolean innerMatchProcessed = false;
            unionOfRoots.add(outerMatch.get(0).first);
            
            
            for(int innerIndex = outerIndex + 1; 
                (innerIndex < mPrelimMatches.size()) && (!processedMatches.get(innerIndex)); 
                ++innerIndex) {
                List<Pair<O, RuleOperator.NodeType>> innerMatch = mPrelimMatches.get(innerIndex);
                List<O> innerCommonNodes = commonNodesPerMatch.get(innerIndex);
                Set<O> innerSetCommonNodes = new HashSet<O>(innerCommonNodes);
                
                //we need to compute the intersection of the common nodes
                //the size of the intersection should be equal to the number
                //of common nodes and the type of each rule node class
                //if there is no match then it could be that we hit a match
                //for a different path, i.e., another pattern that matched
                //with a different set of nodes. In this case, we mark this
                //match as not processed and move onto the next match
                
                outerSetCommonNodes.retainAll(innerSetCommonNodes);
                
                if(outerSetCommonNodes.size() != mNumCommonNodes) {
                    //there was no match
                    //continue to the next match
                    continue;
                } else {
                    Set<O> tempCumulativeIntersection = new HashSet<O>(cumulativeIntersection);
                    tempCumulativeIntersection.retainAll(outerSetCommonNodes);
                    if(tempCumulativeIntersection.size() != mNumCommonNodes) {
                        //problem - there was a set intersection with a size mismatch
                        //between the cumulative intersection and the intersection of the
                        //inner and outer common nodes 
                        //set mMatches to empty and return
                        mMatches = new ArrayList<List<O>>();
                        return;
                    } else {
                        processedMatches.set(innerIndex, true);
                        innerMatchProcessed = true;
                        cumulativeIntersection = tempCumulativeIntersection;
                        unionOfRoots.add(innerMatch.get(0).first);
                    }
                }
            }
            
            cumulativeIntersection.retainAll(finalIntersection);
            if(cumulativeIntersection.size() != mNumCommonNodes) {
                //the cumulative and final intersections did not intersect
                //this could happen when each of the matches are disjoint
                //check if the innerMatches were processed at all
                if(innerMatchProcessed) {
                    //problem - the inner matches were processed and we did
                    //not find common intersections
                    mMatches = new ArrayList<List<O>>();
                    return;
                }
            }
            processedMatches.set(outerIndex, true);
            for(O node: unionOfRoots) {
                patternMatchingRoots.add(node);
            }
            mMatches.add(patternMatchingRoots);
            ++outerIndex;
        } while (outerIndex < mPrelimMatches.size() - 1);        
    }

    private List<O> getCommonNodesFromMatch(List<Pair<O, NodeType>> match) {
        List<O> commonNodes = null;
        //A lookup table to weed out duplicates
        Map<O, Boolean> lookup = new HashMap<O, Boolean>();
        for(int index = 0; index < match.size(); ++index) {
            if(match.get(index).second.equals(RuleOperator.NodeType.COMMON_NODE)) {
                if(commonNodes == null) {
                    commonNodes = new ArrayList<O>();
                }
                O node = match.get(index).first;
                //lookup the node under question
                //if the node is not found in the table
                //then we are examining it for the first time
                //add it to the output list and mark it as seen
                //else continue to the next iteration
                if(lookup.get(node) == null) {
                    commonNodes.add(node);
                    lookup.put(node, true);
                }
            }
        }
        return commonNodes;
    }

    private void BFSDoAllPredecessors(O node, Set<O> seen, Collection<O> fifo)  {
		if (!seen.contains(node)) {
		// We haven't seen this one before.
		Collection<O> preds = mPlan.getPredecessors(node);
		if (preds != null && preds.size() > 0) {
		// Do all our predecessors before ourself
			for (O op : preds) {
				BFSDoAllPredecessors(op, seen, fifo);
			}
		}
		// Now do ourself
		seen.add(node);
		fifo.add(node);
		}
    }
    
    private void DepthFirstWalker()
    {
    	Set<O> seen = new HashSet<O>();
        DFSVisit(null, mPlan.getRoots(), seen);
    }
    
    private void DFSVisit(O node, Collection<O> successors,Set<O> seen)
    {
        if (successors == null) return;
        for (O suc : successors) {
            if (seen.add(suc)) {
            	if (beginMatch(suc))
            		mPrelimMatches.add(mMatch);
                Collection<O> newSuccessors = mPlan.getSuccessors(suc);
                DFSVisit(suc, newSuccessors, seen);
            }
        }
    }

    /**
     * @return first occurrence of matched list of nodes that with the instances of nodes that matched the
     * pattern defined by
     * the rule.  The nodes will be in the vector in the order they are
     * specified in the rule.
     */
    List<O> getMatches() {
        if (mMatches.size()>=1)
            return mMatches.get(0);
        return null;

    }

    /**
     * @return all occurrences of matches. lists of nodes that with the instances of nodes that matched the
     * pattern defined by
     * the rule.  The nodes will be in the vector in the order they are
     * specified in the rule.
     */
    List<List<O>> getAllMatches() {
        return mMatches;
    }

    /*
     * This pattern matching is fairly simple and makes some important
     * assumptions.
     * 1)  The pattern to be matched must be expressible as a graph.
     * 2)  The pattern must always begin with one of the root nodes in the rule plan.
     *     After that it can go where it wants.
     *
     */
    private boolean beginMatch(O node) {
        if (node == null) return false;
        
        mMatch = new ArrayList<Pair<O, RuleOperator.NodeType>>();
        
        List<O> nodeSuccessors;
        List<RuleOperator> ruleRoots = mRule.getPlan().getRoots();
        for(RuleOperator ruleRoot: ruleRoots) {
            if (node.getClass().getName().equals(ruleRoot.getNodeClass().getName()) || 
                    ruleRoot.getNodeType().equals(RuleOperator.NodeType.ANY_NODE)) {
                mMatch.add(new Pair<O, RuleOperator.NodeType>(node, ruleRoot.getNodeType()));
                // Follow the edge to see the next node we should be looking for.
                List<RuleOperator> ruleRootSuccessors = mRule.getPlan().getSuccessors(ruleRoot);
                if (ruleRootSuccessors == null) {
                    // This was looking for a single node
                    return true;
                }
                nodeSuccessors = mPlan.getSuccessors(node);
                if ((nodeSuccessors == null) || (nodeSuccessors.size() != ruleRootSuccessors.size())) {
                    //the ruleRoot has successors but the node does not
                    //OR
                    //the number of successors for the ruleRoot does not match 
                    //the number of successors for the node
                    return false; 
                }
                boolean foundMatch = false;
                for (O nodeSuccessor : nodeSuccessors) {
                    foundMatch |= continueMatch(nodeSuccessor, ruleRootSuccessors);
                }
                return foundMatch;
            }
        }
        // If we get here we haven't found it.
        return false;
    }

    private boolean continueMatch(O node, List<RuleOperator> ruleOperators) {
        for(RuleOperator ruleOperator: ruleOperators) {
            if (node.getClass().getName().equals(ruleOperator.getNodeClass().getName()) || 
                    ruleOperator.getNodeType().equals(RuleOperator.NodeType.ANY_NODE)) {
                mMatch.add(new Pair<O, RuleOperator.NodeType>(node,ruleOperator.getNodeType()));
    
                // Follow the edge to see the next node we should be looking for.
                List<RuleOperator> ruleOperatorSuccessors = mRule.getPlan().getSuccessors(ruleOperator);
                if (ruleOperatorSuccessors == null) {
                    // We've completed the match
                    return true;
                }
                List<O> nodeSuccessors;
                nodeSuccessors = mPlan.getSuccessors(node);
                if ((nodeSuccessors == null) || 
                        (nodeSuccessors.size() != ruleOperatorSuccessors.size())) {
                    //the ruleOperator has successors but the node does not
                    //OR
                    //the number of successors for the ruleOperator does not match 
                    //the number of successors for the node
                    return false;
                }
                boolean foundMatch = false;
                for (O nodeSuccessor : nodeSuccessors) {
                    foundMatch |= continueMatch(nodeSuccessor, ruleOperatorSuccessors);
                }
                return foundMatch;
            }
    
            // We can arrive here either because we didn't match at this node or
            // further down the line.  One way or another we need to remove ourselves
            // from the match vector and return false.
            //SMS - I don't think we need this as mMatch will be discarded anyway
            //mMatch.set(nodeNumber, null);
            return false;
        }
        return false;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Collection;
import java.util.Iterator;
import java.util.Set;

import org.apache.pig.PigException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOUnion extends LogicalOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOUnion.class);

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOUnion(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    public List<LogicalOperator> getInputs() {
        return mPlan.getPredecessors(this);
    }
    
    @Override
    public Schema getSchema() throws FrontendException {
        if (!mIsSchemaComputed) {
            Collection<LogicalOperator> s = mPlan.getPredecessors(this);
            log.debug("Number of predecessors in the graph: " + s.size());
            try {
                Iterator<LogicalOperator> iter = s.iterator();
                LogicalOperator op = iter.next();
                if (null == op) {
                    int errCode = 1006;
                    String msg = "Could not find operator in plan";
                    throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
                }
                mSchema = op.getSchema();
                while(iter.hasNext()) {
                    op = iter.next();
                    if(null != mSchema) {
                        mSchema = mSchema.merge(op.getSchema(), false);
                    } else {
                        mSchema = null;
                        break;
                    }
                }
                if(null != mSchema) {
                    for(Schema.FieldSchema fs: mSchema.getFields()) {
                        iter = s.iterator();
                        while(iter.hasNext()) {
                            op = iter.next();
                            Schema opSchema = op.getSchema();
                            if(null != opSchema) {
                                for(Schema.FieldSchema opFs: opSchema.getFields()) {
                                    fs.setParent(opFs.canonicalName, op);
                                }
                            } else {
                                fs.setParent(null, op);
                            }
                        }
                    }
                }
                mIsSchemaComputed = true;
            } catch (FrontendException fe) {
                mSchema = null;
                mIsSchemaComputed = false;
                throw fe;
            }
        }
        return mSchema;
    }

    @Override
    public String name() {
        return "Union " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    public byte getType() {
        return DataType.BAG;
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.LogicalOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        LOUnion unionClone = (LOUnion)super.clone();
        return unionClone;
    }
    
    @Override
    public ProjectionMap getProjectionMap() {
        Schema outputSchema;
        
        try {
            outputSchema = getSchema();
        } catch (FrontendException fee) {
            return null;
        }
        
        if(outputSchema == null) {
            return null;
        }
        
        List<LogicalOperator> predecessors = (ArrayList<LogicalOperator>)mPlan.getPredecessors(this);
        if(predecessors == null) {
            return null;
        }
        
        MultiMap<Integer, Pair<Integer, Integer>> mapFields = new MultiMap<Integer, Pair<Integer, Integer>>();
        
        for(int inputNum = 0; inputNum < predecessors.size(); ++inputNum) {
            LogicalOperator predecessor = predecessors.get(inputNum);
            Schema inputSchema = null;        
            
            try {
                inputSchema = predecessor.getSchema();
            } catch (FrontendException fee) {
                return null;
            }
            
            if(inputSchema == null) {
                return null;
            } else {
                for(int inputColumn = 0; inputColumn < inputSchema.size(); ++inputColumn) {
                    mapFields.put(inputColumn, new Pair<Integer, Integer>(inputNum, inputColumn));
                    //removedFields.add(new Pair<Integer, Integer>(inputNum, inputColumn));
                }
            }
        }
        
        return new ProjectionMap(mapFields, null, null);
    }

    @Override
    public List<RequiredFields> getRequiredFields() {
        List<LogicalOperator> predecessors = mPlan.getPredecessors(this);
        
        if(predecessors == null) {
            return null;
        }

        List<RequiredFields> requiredFields = new ArrayList<RequiredFields>();
        
        for(int inputNum = 0; inputNum < predecessors.size(); ++inputNum) {
            requiredFields.add(new RequiredFields(true));
        }
        
        return (requiredFields.size() == 0? null: requiredFields);
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.builtin;

import java.io.IOException;
import java.io.ObjectInputStream;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.pig.ComparisonFunc;
import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.HDataType;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.CountingMap;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.DiscreteProbabilitySampleGenerator;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataType;
import org.apache.pig.data.DefaultDataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.NullableBytesWritable;
import org.apache.pig.impl.io.NullableDoubleWritable;
import org.apache.pig.impl.io.NullableFloatWritable;
import org.apache.pig.impl.io.NullableIntWritable;
import org.apache.pig.impl.io.NullableLongWritable;
import org.apache.pig.impl.io.NullableText;
import org.apache.pig.impl.io.NullableTuple;
import org.apache.pig.impl.io.PigNullableWritable;


public class FindQuantiles extends EvalFunc<Map<Object, Object>>{
    // keys for the weightedparts Map
    public static final String QUANTILES_LIST = "quantiles.list";
    public static final String WEIGHTED_PARTS = "weighted.parts";

    BagFactory mBagFactory = BagFactory.getInstance();
    TupleFactory mTupleFactory = TupleFactory.getInstance();

    boolean[] mAsc;
    enum State { ALL_ASC, ALL_DESC, MIXED };
    State mState;
    
    private class SortComparator implements Comparator<Tuple> {
        @SuppressWarnings("unchecked")
        public int compare(Tuple t1, Tuple t2) {
            switch (mState) {
            case ALL_ASC:
                return t1.compareTo(t2);

            case ALL_DESC:
                return t2.compareTo(t1);

            case MIXED:
                // Have to break the tuple down and compare it field to field.
                int sz1 = t1.size();
                int sz2 = t2.size();
                if (sz2 < sz1) {
                    return 1;
                } else if (sz2 > sz1) {
                    return -1;
                } else {
                    for (int i = 0; i < sz1; i++) {
                        try {
                            int c = DataType.compare(t1.get(i), t2.get(i));
                            if (c != 0) {
                                if (!mAsc[i]) c *= -1;
                                return c;
                            }
                        } catch (ExecException e) {
                            throw new RuntimeException("Unable to compare tuples", e);
                        }
                    }
                    return 0;
                }
            }
            return -1; // keep the compiler happy
        }
    }

    private Comparator<Tuple> mComparator = new SortComparator();
    private FuncSpec mUserComparisonFuncSpec;
    private ComparisonFunc mUserComparisonFunc;
    
    
    @SuppressWarnings("unchecked")
    private void instantiateFunc() {
        if(mUserComparisonFunc != null) {
            this.mUserComparisonFunc = (ComparisonFunc) PigContext.instantiateFuncFromSpec(this.mUserComparisonFuncSpec);
            this.mUserComparisonFunc.setReporter(reporter);
            this.mComparator = mUserComparisonFunc;
        }
    }
    
    // We need to instantiate any user defined comparison function 
    // on the backend when the FindQuantiles udf is deserialized
    private void readObject(ObjectInputStream is) throws IOException, ClassNotFoundException{
        is.defaultReadObject();
        instantiateFunc();
    }
    

    public FindQuantiles() {
        mState = State.ALL_ASC;
    }

    public FindQuantiles(String[] args) {
        int startIndex = 0;
        int ascFlagsLength = args.length;
        // the first argument may be the information
        // about user defined comparison function if one
        // was specified
        if(args[0].startsWith(MRCompiler.USER_COMPARATOR_MARKER)) {
            mUserComparisonFuncSpec = new FuncSpec(
                    args[0].substring(MRCompiler.USER_COMPARATOR_MARKER.length()));
            // skip the first argument now that we used it
            startIndex++;
            ascFlagsLength--;
        }
        
        mAsc = new boolean[ascFlagsLength];
        boolean sawAsc = false;
        boolean sawDesc = false;
        for (int i = startIndex; i < ascFlagsLength; i++) {
            mAsc[i] = Boolean.parseBoolean(args[i]);
            if (mAsc[i]) sawAsc = true;
            else sawDesc = true;
        }
        if (sawAsc && sawDesc) mState = State.MIXED;
        else if (sawDesc) mState = State.ALL_DESC;
        else mState = State.ALL_ASC; // In cast they gave us no args this
                                     // defaults to all ascending.
    }

    /**
     * first field in the input tuple is the number of quantiles to generate
     * second field is the *sorted* bag of samples
     */
    
    @Override
    public Map<Object, Object> exec(Tuple in) throws IOException {
        Map<Object, Object> output = new HashMap<Object, Object>();
        if(in==null || in.size()==0)
            return null;
        Integer numQuantiles = null;
        DataBag samples = null;
        ArrayList<Tuple> quantilesList = new ArrayList<Tuple>();
        Map<Tuple,Tuple> weightedParts = new HashMap<Tuple, Tuple>();
        // the sample file has a tuple as under:
        // (numQuantiles, bag of samples) 
        // numQuantiles here is the reduce parallelism
        try{
            numQuantiles = (Integer)in.get(0);
            samples = (DataBag)in.get(1);
            
            long numSamples = samples.size();
            long toSkip = numSamples / numQuantiles;
            if(toSkip == 0) {
                // numSamples is < numQuantiles;
                // set numQuantiles to numSamples
                numQuantiles = (int)numSamples;
                toSkip = 1;
            }
            
            long ind=0, j=-1, nextQuantile = toSkip-1;
            for (Tuple it : samples) {
                if (ind==nextQuantile){
                    ++j;
                    quantilesList.add(it);
                    nextQuantile+=toSkip;
                    if(j==numQuantiles-1)
                        break;
                }
                ind++;
                if (ind % 1000 == 0) progress();
            }
            long i=-1;
            Map<Tuple,CountingMap<Integer>> contribs = new HashMap<Tuple, CountingMap<Integer>>();
            for (Tuple it : samples){
                ++i;
                if (i % 1000 == 0) progress();
                int partInd = new Long(i/toSkip).intValue(); // which partition
                if(partInd==numQuantiles) break;
                // the quantiles array has the element from the sample which is the
                // last element for a given partition. For example: if numQuantiles 
                // is 5 and number of samples is 100, then toSkip = 20 
                // quantiles[0] = sample[19] // the 20th element
                // quantiles[1] = sample[39] // the 40th element
                // and so on. For any element in the sample between 0 and 19, partInd
                // will be 0. We want to check if a sample element which is
                // present between 0 and 19 is also the 19th (quantiles[0] element).
                // This would mean that element might spread over the 0th and 1st 
                // partition. We are looking for contributions to a partition
                // from such elements. 
                
                // First We only check for sample elements in partitions other than the last one
                // < numQuantiles -1 (partInd is 0 indexed). 
                if(partInd<numQuantiles-1 && areEqual(it,quantilesList.get(partInd))){
                    if(!contribs.containsKey(it)){
                        CountingMap<Integer> cm = new CountingMap<Integer>();
                        cm.put(partInd, 1);
                        contribs.put(it, cm);
                    }
                    else
                        contribs.get(it).put(partInd, 1);
                }
                else{ 
                    // we are either in the last partition (last quantile)
                    // OR the sample element we are currently processing is not
                    // the same as the element in the quantile array for this partition
                    // if we haven't seen this sample item earlier, this is not an
                    // element which crosses partitions - so ignore
                    if(!contribs.containsKey(it))
                        continue;
                    else
                        // we have seen this sample before (in a previous partInd), 
                        // add to the contribution associated with this sample - if we had 
                        // not seen this sample in a previous partInd, then we would have not
                        // had this in the contribs map! (because of the if above).This 
                        // "key" (represented by the sample item) can either go to the 
                        // previous partInd or this partInd in the final sort reduce stage. 
                        // That is where the amount of contribution to each partInd will
                        // matter and influence the choice.
                        contribs.get(it).put(partInd, 1);
                }
            }
            int k = 0;
            for(Entry<Tuple, CountingMap<Integer>> ent : contribs.entrySet()){
                if (k % 1000 == 0) progress();
                Tuple key = ent.getKey(); // sample item which repeats
                
                // this map will have the contributions of the sample item to the different partitions
                CountingMap<Integer> value = ent.getValue(); 
                
                long total = value.getTotalCount();
                Tuple probVec =  mTupleFactory.newTuple(numQuantiles.intValue());
                // initialize all contribution fractions for different
                // partitions to 0.0
                for (int l = 0; l < numQuantiles; l++) {
                    probVec.set(l, new Float(0.0));
                }
                // for each partition that this sample item is present in,
                // compute the fraction of the total occurences for that
                // partition - this will be the probability with which we
                // will pick this partition in the final sort reduce job
                // for this sample item
                for (Entry<Integer,Integer> valEnt : value.entrySet()) {
                    probVec.set(valEnt.getKey(), (float)valEnt.getValue()/total);
                }
                weightedParts.put(key, probVec);
            }
            output.put(QUANTILES_LIST, mBagFactory.newDefaultBag(quantilesList));
            output.put(WEIGHTED_PARTS, weightedParts);
            return output;
        }catch (Exception e){
            e.printStackTrace();
            throw new RuntimeException(e);
        }
    }

    private boolean areEqual(Tuple it, Tuple tuple) {
        return mComparator.compare(it, tuple)==0;
    }
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOMod extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOMod.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOMod(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.mergeType(getLhsOperand().getType(), getRhsOperand().getType()));
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Mod " + mKey.scope + "-" + mKey.id;
    }
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LONot extends UnaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LONot.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LONot(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }
    
    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getOperand().getFieldSchema().canonicalName, getOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Not " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;

import java.io.IOException;
import java.io.OutputStream;

import org.apache.pig.data.Tuple;

import java.text.NumberFormat;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.OutputFormat;
import org.apache.hadoop.mapred.RecordWriter;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

import org.apache.pig.StoreConfig;
import org.apache.pig.StoreFunc;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.util.ObjectSerializer;

import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStoreImpl;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat;

/**
 * This class is used to have a POStore write to DFS via a output
 * collector/record writer. It sets up a modified job configuration to
 * force a write to a specific subdirectory of the main output
 * directory. This is done so that multiple output directories can be
 * used in the same job. Since the hadoop framework requires a
 * reporter to be available to create the record writer the main
 * function (createStoreFunc) has to be called from within a map or
 * reduce function.
 */
public class MapReducePOStoreImpl extends POStoreImpl {

    private PigContext pc;
    private StoreFunc storer;
    private FileSpec sFile;
    private Reporter reporter;
    private RecordWriter writer;
    private JobConf job;

    private final Log log = LogFactory.getLog(getClass());
    public static final String PIG_STORE_CONFIG = "pig.store.config";
    
    public MapReducePOStoreImpl(JobConf job) {
        this.job = job;
    }

    public void setReporter(Reporter reporter) {
        this.reporter = reporter;
    }

    @Override
    public StoreFunc createStoreFunc(FileSpec sFile, Schema schema) 
        throws IOException {

        // set up a new job conf
        JobConf outputConf = new JobConf(job);
        String tmpPath = PlanHelper.makeStoreTmpPath(sFile.getFileName());

        // If the StoreFunc associate with the POStore is implements
        // getStorePreparationClass() and returns a non null value,
        // then it could be wanting to implement OutputFormat for writing out to hadoop
        // Check if this is the case, if so, use the OutputFormat class the 
        // StoreFunc gives us else use our default PigOutputFormat
        Object storeFunc = PigContext.instantiateFuncFromSpec(sFile.getFuncSpec());
        Class sPrepClass = null;
        try {
            sPrepClass = ((StoreFunc)storeFunc).getStorePreparationClass();
        } catch(AbstractMethodError e) {
            // this is for backward compatibility wherein some old StoreFunc
            // which does not implement getStorePreparationClass() is being
            // used. In this case, we want to just use PigOutputFormat
            sPrepClass = null;
        }
        if(sPrepClass != null && OutputFormat.class.isAssignableFrom(sPrepClass)) {
            outputConf.setOutputFormat(sPrepClass);
        } else {
            outputConf.setOutputFormat(PigOutputFormat.class);
        }

        // PigOuputFormat will look for pig.storeFunc to actually
        // write stuff out.
        // serialize the store func spec using ObjectSerializer
        // ObjectSerializer.serialize() uses default java serialization
        // and then further encodes the output so that control characters
        // get encoded as regular characters. Otherwise any control characters
        // in the store funcspec would break the job.xml which is created by
        // hadoop from the jobconf.
        outputConf.set("pig.storeFunc", ObjectSerializer.serialize(sFile.getFuncSpec().toString()));

        // We set the output dir to the final location of the output,
        // the output dir set in the original job config points to the
        // temp location for the multi store.
        Path outputDir = new Path(sFile.getFileName()).makeQualified(FileSystem.get(outputConf));
        outputConf.set("mapred.output.dir", outputDir.toString());

        // Set the schema
        outputConf.set(PIG_STORE_CONFIG, 
                       ObjectSerializer.serialize(new StoreConfig(outputDir.toString(), schema)));

        // The workpath is set to a unique-per-store subdirectory of
        // the current working directory.
        String workPath = outputConf.get("mapred.work.output.dir");
        outputConf.set("mapred.work.output.dir",
                       new Path(workPath, tmpPath).toString());
        OutputFormat outputFormat = outputConf.getOutputFormat();

        // Generate a unique part name (part-<task_partition_number>).
        String fileName = getPartName(outputConf);
        
        // create a new record writer
        writer = outputFormat.getRecordWriter(FileSystem.get(outputConf), 
                                              outputConf, fileName, reporter);

        // return an output collector using the writer we just created.
        return new StoreFuncAdaptor(new OutputCollector() 
            {
                @SuppressWarnings({"unchecked"})
                public void collect(Object key, Object value) throws IOException {
                    writer.write(key,value);
                }
            });
    }

    @Override
    public void tearDown() throws IOException{
        if (writer != null) {
            writer.close(reporter);
            writer = null;
        }
    }

    @Override
    public void cleanUp() throws IOException{
        if (writer != null) {
            writer.close(reporter);
            writer = null;
        }
    }

    private String getPartName(JobConf conf) {
        int partition = conf.getInt("mapred.task.partition", -1);   

        NumberFormat numberFormat = NumberFormat.getInstance();
        numberFormat.setMinimumIntegerDigits(5);
        numberFormat.setGroupingUsed(false);

        return "part-" + numberFormat.format(partition);
    }

    /**
     * This is a simple adaptor class to allow the physical store operator
     * to be used in the map reduce case. It will allow to use an output
     * collector instead of an output stream to write tuples.
     */
    private class StoreFuncAdaptor implements StoreFunc {
        private OutputCollector collector;
        
        public StoreFuncAdaptor(OutputCollector collector) {
            this.collector = collector;
        }
        
        @Override
        public void bindTo(OutputStream os) throws IOException {
        }
        
        @Override
        public void putNext(Tuple f) throws IOException {
            collector.collect(null,f);
        }
        
        @Override
        public void finish() throws IOException {
        }

        @Override
        public Class getStorePreparationClass() throws IOException {
            return null;
        }
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.test;

import junit.framework.TestCase;
import org.junit.Test;
import org.apache.pig.FuncSpec;
import org.apache.pig.impl.logicalLayer.*;
import org.apache.pig.impl.logicalLayer.validators.TypeCheckingValidator;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.plan.CompilationMessageCollector;
import org.apache.pig.impl.plan.PlanValidationException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema ;
import static org.apache.pig.test.utils.TypeCheckingTestUtil.* ;
import org.apache.pig.test.utils.TypeCheckingTestUtil;

import java.util.List;
import java.util.ArrayList;

public class TestTypeCheckingValidatorNoSchema  extends TestCase {


    @Test
    public void testUnion1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;
        load2.setEnforcedSchema(null) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOUnion union = new LOUnion(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(union) ;

        plan.connect(load1, union);
        plan.connect(load2, union);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        // check end result schema
        Schema outputSchema = union.getSchema() ;
        assertEquals(outputSchema, null);

    }


    @Test
    public void testUnion2() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        String[] aliases = new String[]{ "a", "b", "c" } ;
        byte[] types = new byte[] { DataType.INTEGER, DataType.LONG, DataType.BYTEARRAY } ;
        Schema schema1 = genFlatSchema(aliases, types) ;

        // set schemas
        load1.setEnforcedSchema(schema1) ;
        load2.setEnforcedSchema(null) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOUnion union = new LOUnion(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(union) ;

        plan.connect(load1, union);
        plan.connect(load2, union);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        // check end result schema
        Schema outputSchema = union.getSchema() ;
        assertEquals(outputSchema, null);

    }


    // Positive expression cond columns
    @Test
    public void testSplitWithInnerPlan1() throws Throwable {

        printCurrentMethodName();
        // Create outer plan
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LONotEqual notequal1 = new LONotEqual(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(notequal1) ;

        innerPlan1.connect(project11, notequal1);
        innerPlan1.connect(project12, notequal1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), 26) ;
        const21.setType(DataType.LONG);
        LOLesserThanEqual lesser21 = new LOLesserThanEqual(innerPlan2,
                                                           genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(lesser21) ;

        innerPlan2.connect(project21, lesser21);
        innerPlan2.connect(const21, lesser21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // split
        LOSplit split1 = new LOSplit(plan,
                                     genNewOperatorKey(),
                                     new ArrayList<LogicalOperator>());

        // output1
        LOSplitOutput splitOutput1 = new LOSplitOutput(plan, genNewOperatorKey(), 0, innerPlan1) ;
        split1.addOutput(splitOutput1);

        // output2
        LOSplitOutput splitOutput2 = new LOSplitOutput(plan, genNewOperatorKey(), 1, innerPlan2) ;
        split1.addOutput(splitOutput2);

        plan.add(load1);
        plan.add(split1);
        plan.add(splitOutput1);
        plan.add(splitOutput2);

        plan.connect(load1, split1) ;
        plan.connect(split1, splitOutput1) ;
        plan.connect(split1, splitOutput2) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check split itself
        assertEquals(split1.getSchema(), null) ;

        // check split output #1
        assertEquals(splitOutput1.getSchema(), null) ;
        assertEquals(splitOutput2.getSchema(), null) ;

        // inner conditions: all have to be boolean
        assertEquals(innerPlan1.getSingleLeafPlanOutputType(), DataType.BOOLEAN);
        assertEquals(innerPlan2.getSingleLeafPlanOutputType(), DataType.BOOLEAN);

    }


    // Negative test in cond plan
    @Test
    public void testSplitWithInnerPlan2() throws Throwable {

        printCurrentMethodName();
        // Create outer plan
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LONotEqual notequal1 = new LONotEqual(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(notequal1) ;

        innerPlan1.connect(project11, notequal1);
        innerPlan1.connect(project12, notequal1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), 26) ;
        const21.setType(DataType.LONG);
        LOAdd add21 = new LOAdd(innerPlan2, genNewOperatorKey()) ;
        LOConst const22 = new LOConst(innerPlan2, genNewOperatorKey(), "hoho") ;
        const22.setType(DataType.CHARARRAY);
        LOSubtract subtract21 = new LOSubtract(innerPlan2, genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(add21) ;
        innerPlan2.add(const22) ;
        innerPlan2.add(subtract21) ;

        innerPlan2.connect(project21, add21);
        innerPlan2.connect(const21, add21) ;       
        innerPlan2.connect(add21, subtract21) ;
        innerPlan2.connect(const22, subtract21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // split
        LOSplit split1 = new LOSplit(plan,
                                     genNewOperatorKey(),
                                     new ArrayList<LogicalOperator>());

        // output1
        LOSplitOutput splitOutput1 = new LOSplitOutput(plan, genNewOperatorKey(), 0, innerPlan1) ;
        split1.addOutput(splitOutput1);

        // output2
        LOSplitOutput splitOutput2 = new LOSplitOutput(plan, genNewOperatorKey(), 1, innerPlan2) ;
        split1.addOutput(splitOutput2);

        plan.add(load1);
        plan.add(split1);
        plan.add(splitOutput1);
        plan.add(splitOutput2);

        plan.connect(load1, split1) ;
        plan.connect(split1, splitOutput1) ;
        plan.connect(split1, splitOutput2) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        try {
            TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch(PlanValidationException pve) {
            // good
        }

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

    }


    @Test
    public void testDistinct1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        LODistinct distinct1 = new LODistinct(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(distinct1) ;

        plan.connect(load1, distinct1);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        // check end result schema
        assertEquals(distinct1.getSchema(), null);
    }

    // Positive expression sort columns
    @Test
    public void testSort1() throws Throwable {

        printCurrentMethodName();
        // Create outer plan
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOProject project12 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 1) ;
        project11.setSentinel(true);
        LOMultiply mul1 = new LOMultiply(innerPlan1, genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(project12) ;
        innerPlan1.add(mul1) ;

        innerPlan1.connect(project11, mul1);
        innerPlan1.connect(project12, mul1);

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOConst const21 = new LOConst(innerPlan2, genNewOperatorKey(), 26) ;
        const21.setType(DataType.LONG);
        LOMod mod21 = new LOMod(innerPlan2, genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(const21) ;
        innerPlan2.add(mod21) ;

        innerPlan2.connect(project21, mod21);
        innerPlan2.connect(const21, mod21) ;

        // List of innerplans
        List<LogicalPlan> innerPlans = new ArrayList<LogicalPlan>() ;
        innerPlans.add(innerPlan1) ;
        innerPlans.add(innerPlan2) ;

        // List of ASC flags
        List<Boolean> ascList = new ArrayList<Boolean>() ;
        ascList.add(true);
        ascList.add(true);

        // Sort
        LOSort sort1 = new LOSort(plan, genNewOperatorKey(), innerPlans,  ascList, null) ;


        plan.add(load1);
        plan.add(sort1);
        plan.connect(load1, sort1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        TypeCheckingTestUtil.printMessageCollector(collector) ;
        TypeCheckingTestUtil.printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        assertEquals(sort1.getSchema(), null) ;

    }

    // Positive expression column
    @Test
    public void testFilter1() throws Throwable {

        printCurrentMethodName();
        // Create outer plan
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // Create inner plan
        LogicalPlan innerPlan = new LogicalPlan() ;
        LOProject project1 = new LOProject(innerPlan, genNewOperatorKey(), load1, 0) ;
        project1.setSentinel(true);
        LOProject project2 = new LOProject(innerPlan, genNewOperatorKey(), load1, 1) ;
        project2.setSentinel(true);
        LOAdd add1 = new LOAdd(innerPlan, genNewOperatorKey()) ;
        LOConst const1  = new LOConst(innerPlan, genNewOperatorKey(), 10) ;
        const1.setType(DataType.LONG);

        LOGreaterThan gt1 = new LOGreaterThan(innerPlan,
                                              genNewOperatorKey()) ;

        innerPlan.add(project1) ;
        innerPlan.add(project2) ;
        innerPlan.add(add1) ;
        innerPlan.add(const1) ;
        innerPlan.add(gt1) ;

        innerPlan.connect(project1, add1) ;
        innerPlan.connect(project2, add1) ;
        innerPlan.connect(add1, gt1) ;
        innerPlan.connect(const1, gt1) ;

        // filter
        LOFilter filter1 = new LOFilter(plan, genNewOperatorKey(), innerPlan) ;

        plan.add(load1);
        plan.add(filter1);
        plan.connect(load1, filter1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        TypeCheckingTestUtil.printMessageCollector(collector) ;
        TypeCheckingTestUtil.printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        assertEquals(filter1.getSchema(), null) ;

    }


    // Negative expression column
    @Test
    public void testFilter2() throws Throwable {

        printCurrentMethodName();
        // Create outer plan
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = new LOLoad(plan,
                                  genNewOperatorKey(),
                                  new FileSpec("pi", new FuncSpec(pigStorage)),
                                  null, null, true) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // Create inner plan
        LogicalPlan innerPlan = new LogicalPlan() ;
        LOProject project1 = new LOProject(innerPlan, genNewOperatorKey(), load1, 0) ;
        project1.setSentinel(true);
        LOProject project2 = new LOProject(innerPlan, genNewOperatorKey(), load1, 1) ;
        project2.setSentinel(true);
        LOAdd add1 = new LOAdd(innerPlan, genNewOperatorKey()) ;
        LOConst const1  = new LOConst(innerPlan, genNewOperatorKey(), "10") ;
        const1.setType(DataType.CHARARRAY);

        LOGreaterThan gt1 = new LOGreaterThan(innerPlan,
                                              genNewOperatorKey()) ;

        innerPlan.add(project1) ;
        innerPlan.add(project2) ;
        innerPlan.add(add1) ;
        innerPlan.add(const1) ;
        innerPlan.add(gt1) ;

        innerPlan.connect(project1, add1) ;
        innerPlan.connect(project2, add1) ;
        innerPlan.connect(add1, gt1) ;
        innerPlan.connect(const1, gt1) ;

        // filter
        LOFilter filter1 = new LOFilter(plan, genNewOperatorKey(), innerPlan) ;

        plan.add(load1);
        plan.add(filter1);
        plan.connect(load1, filter1) ;

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        try {
            typeValidator.validate(plan, collector) ;
            fail("Exception expected") ;
        }
        catch(PlanValidationException pve) {
            // good
        }

        TypeCheckingTestUtil.printMessageCollector(collector) ;
        TypeCheckingTestUtil.printTypeGraph(plan) ;

        if (!collector.hasError()) {
            throw new AssertionError("Expect an error") ;
        }

    }

    @Test
    public void testCross1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        // schema for input#1
        Schema inputSchema1 = null ;
        {
            List<FieldSchema> fsList1 = new ArrayList<FieldSchema>() ;
            fsList1.add(new FieldSchema("field1a", DataType.INTEGER)) ;
            fsList1.add(new FieldSchema("field2a", DataType.BYTEARRAY)) ;
            inputSchema1 = new Schema(fsList1) ;
        }

        // set schemas
        load1.setEnforcedSchema(inputSchema1) ;
        load2.setEnforcedSchema(null) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOCross cross = new LOCross(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cross) ;

        plan.connect(load1, cross);
        plan.connect(load2, cross);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        assertEquals(cross.getSchema(), null);

    }


    @Test
    public void testCross2() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;
        load2.setEnforcedSchema(null) ;

        // create union operator
        ArrayList<LogicalOperator> inputList = new ArrayList<LogicalOperator>() ;
        inputList.add(load1) ;
        inputList.add(load2) ;
        LOCross cross = new LOCross(plan, genNewOperatorKey()) ;

        // wiring
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cross) ;

        plan.connect(load1, cross);
        plan.connect(load2, cross);

        // validate
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;

        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        assertEquals(cross.getSchema(), null);

    }


    // Positive test
    @Test
    public void testCOGroupByAtom1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;
        load2.setEnforcedSchema(null) ;

        // Create expression inner plan #1 of input #1
        LogicalPlan innerPlan11 = new LogicalPlan() ;
        LOProject project111 = new LOProject(innerPlan11, genNewOperatorKey(), load1, 0) ;
        project111.setSentinel(true);
        LOConst const111 = new LOConst(innerPlan11, genNewOperatorKey(), 26F) ;
        const111.setType(DataType.FLOAT);
        LOSubtract subtract111 = new LOSubtract(innerPlan11,
                                                genNewOperatorKey()) ;

        innerPlan11.add(project111) ;
        innerPlan11.add(const111) ;
        innerPlan11.add(subtract111) ;

        innerPlan11.connect(project111, subtract111);
        innerPlan11.connect(const111, subtract111) ;

        // Create expression inner plan #1 of input #2
        LogicalPlan innerPlan12 = new LogicalPlan() ;
        LOProject project121 = new LOProject(innerPlan12, genNewOperatorKey(), load2, 0) ;
        project121.setSentinel(true);
        LOConst const121 = new LOConst(innerPlan12, genNewOperatorKey(), 26) ;
        const121.setType(DataType.INTEGER);
        LOSubtract subtract121 = new LOSubtract(innerPlan12,
                                                genNewOperatorKey()) ;

        innerPlan12.add(project121) ;
        innerPlan12.add(const121) ;
        innerPlan12.add(subtract121) ;

        innerPlan12.connect(project121, subtract121);
        innerPlan12.connect(const121, subtract121) ;

        // Create Cogroup
        ArrayList<LogicalOperator> inputs = new ArrayList<LogicalOperator>() ;
        inputs.add(load1) ;
        inputs.add(load2) ;

        MultiMap<LogicalOperator, LogicalPlan> maps
                            = new MultiMap<LogicalOperator, LogicalPlan>() ;
        maps.put(load1, innerPlan11);
        maps.put(load2, innerPlan12);

        boolean[] isInner = new boolean[inputs.size()] ;
        for (int i=0; i < isInner.length ; i++) {
            isInner[i] = false ;
        }

        LOCogroup cogroup1 = new LOCogroup(plan,
                                           genNewOperatorKey(),
                                           maps,
                                           isInner) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cogroup1) ;

        plan.connect(load1, cogroup1);
        plan.connect(load2, cogroup1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check outer schema
        Schema endResultSchema = cogroup1.getSchema() ;

        // Tuple group column
        assertEquals(endResultSchema.getField(0).type, DataType.FLOAT) ;

        // check inner schema1
        assertEquals(endResultSchema.getField(1).schema, null);
        assertEquals(endResultSchema.getField(2).schema, null);

        // check group by col end result
        assertEquals(innerPlan11.getSingleLeafPlanOutputType(), DataType.FLOAT) ;
        assertEquals(innerPlan12.getSingleLeafPlanOutputType(), DataType.FLOAT) ;
    }


    // Positive test
    @Test
    public void testCOGroupByTuple1() throws Throwable {

        printCurrentMethodName();
        LogicalPlan plan = new LogicalPlan() ;

        String pigStorage = PigStorage.class.getName() ;

        LOLoad load1 = genDummyLOLoad(plan) ;
        LOLoad load2 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;
        load2.setEnforcedSchema(null) ;

        // Create expression inner plan #1 of input #1
        LogicalPlan innerPlan11 = new LogicalPlan() ;
        LOProject project111 = new LOProject(innerPlan11, genNewOperatorKey(), load1, 0) ;
        project111.setSentinel(true);
        LOConst const111 = new LOConst(innerPlan11, genNewOperatorKey(), 26F) ;
        const111.setType(DataType.FLOAT);
        LOSubtract subtract111 = new LOSubtract(innerPlan11,
                                                genNewOperatorKey()) ;

        innerPlan11.add(project111) ;
        innerPlan11.add(const111) ;
        innerPlan11.add(subtract111) ;

        innerPlan11.connect(project111, subtract111);
        innerPlan11.connect(const111, subtract111) ;

        // Create expression inner plan #2 of input #1
        LogicalPlan innerPlan21 = new LogicalPlan() ;
        LOProject project211 = new LOProject(innerPlan21, genNewOperatorKey(), load1, 0) ;
        project211.setSentinel(true);
        LOProject project212 = new LOProject(innerPlan21, genNewOperatorKey(), load1, 1) ;
        project212.setSentinel(true);

        LOAdd add211 = new LOAdd(innerPlan21,
                                 genNewOperatorKey()) ;

        innerPlan21.add(project211) ;
        innerPlan21.add(project212) ;
        innerPlan21.add(add211) ;

        innerPlan21.connect(project211, add211);
        innerPlan21.connect(project212, add211) ;


        // Create expression inner plan #1 of input #2
        LogicalPlan innerPlan12 = new LogicalPlan() ;
        LOProject project121 = new LOProject(innerPlan12, genNewOperatorKey(), load2, 0) ;
        project121.setSentinel(true);
        LOConst const121 = new LOConst(innerPlan12, genNewOperatorKey(), 26) ;
        const121.setType(DataType.INTEGER);
        LOSubtract subtract121 = new LOSubtract(innerPlan12,
                                                genNewOperatorKey()) ;

        innerPlan12.add(project121) ;
        innerPlan12.add(const121) ;
        innerPlan12.add(subtract121) ;

        innerPlan12.connect(project121, subtract121);
        innerPlan12.connect(const121, subtract121) ;

        // Create expression inner plan #2 of input #2
        LogicalPlan innerPlan22 = new LogicalPlan() ;
        LOConst const122 = new LOConst(innerPlan22, genNewOperatorKey(), 26) ;
        const122.setType(DataType.INTEGER);
        innerPlan22.add(const122) ;

        // Create Cogroup
        ArrayList<LogicalOperator> inputs = new ArrayList<LogicalOperator>() ;
        inputs.add(load1) ;
        inputs.add(load2) ;

        MultiMap<LogicalOperator, LogicalPlan> maps
                            = new MultiMap<LogicalOperator, LogicalPlan>() ;
        maps.put(load1, innerPlan11);
        maps.put(load1, innerPlan21);
        maps.put(load2, innerPlan12);
        maps.put(load2, innerPlan22);

        boolean[] isInner = new boolean[inputs.size()] ;
        for (int i=0; i < isInner.length ; i++) {
            isInner[i] = false ;
        }

        LOCogroup cogroup1 = new LOCogroup(plan,
                                           genNewOperatorKey(),
                                           maps,
                                           isInner) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(load2) ;
        plan.add(cogroup1) ;

        plan.connect(load1, cogroup1);
        plan.connect(load2, cogroup1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        TypeCheckingTestUtil.printMessageCollector(collector) ;
        TypeCheckingTestUtil.printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check outer schema
        Schema endResultSchema = cogroup1.getSchema() ;

        // Tuple group column
        assertEquals(endResultSchema.getField(0).type, DataType.TUPLE) ;
        assertEquals(endResultSchema.getField(0).schema.getField(0).type, DataType.FLOAT) ;
        assertEquals(endResultSchema.getField(0).schema.getField(1).type, DataType.DOUBLE);

        assertEquals(endResultSchema.getField(1).type, DataType.BAG) ;
        assertEquals(endResultSchema.getField(2).type, DataType.BAG) ;

        // check inner schema1
        assertEquals(endResultSchema.getField(1).schema, null);
        assertEquals(endResultSchema.getField(2).schema, null);

        // check group by col end result
        assertEquals(innerPlan11.getSingleLeafPlanOutputType(), DataType.FLOAT) ;
        assertEquals(innerPlan21.getSingleLeafPlanOutputType(), DataType.DOUBLE) ;
        assertEquals(innerPlan12.getSingleLeafPlanOutputType(), DataType.FLOAT) ;
        assertEquals(innerPlan22.getSingleLeafPlanOutputType(), DataType.DOUBLE) ;
    }



    // Positive test
    @Test
    public void testForEachGenerate1() throws Throwable {

        printCurrentMethodName() ;

        LogicalPlan plan = new LogicalPlan() ;
        LOLoad load1 = genDummyLOLoad(plan) ;

        // set schemas
        load1.setEnforcedSchema(null) ;

        // Create expression inner plan #1
        LogicalPlan innerPlan1 = new LogicalPlan() ;
        LOProject project11 = new LOProject(innerPlan1, genNewOperatorKey(), load1, 0) ;
        project11.setSentinel(true);
        LOConst const11 = new LOConst(innerPlan1, genNewOperatorKey(), 26F) ;
        const11.setType(DataType.FLOAT);
        LOSubtract subtract11 = new LOSubtract(innerPlan1,
                                                genNewOperatorKey()) ;

        innerPlan1.add(project11) ;
        innerPlan1.add(const11) ;
        innerPlan1.add(subtract11) ;

        innerPlan1.connect(project11, subtract11);
        innerPlan1.connect(const11, subtract11) ;

        // Create expression inner plan #2
        LogicalPlan innerPlan2 = new LogicalPlan() ;
        LOProject project21 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 0) ;
        project21.setSentinel(true);
        LOProject project22 = new LOProject(innerPlan2, genNewOperatorKey(), load1, 1) ;
        project21.setSentinel(true);
        LOAdd add21 = new LOAdd(innerPlan2,
                                genNewOperatorKey()) ;

        innerPlan2.add(project21) ;
        innerPlan2.add(project22) ;
        innerPlan2.add(add21) ;

        innerPlan2.connect(project21, add21);
        innerPlan2.connect(project22, add21);

        // List of plans
        ArrayList<LogicalPlan> generatePlans = new ArrayList<LogicalPlan>() ;
        generatePlans.add(innerPlan1);
        generatePlans.add(innerPlan2);

        // List of flatten flags
        ArrayList<Boolean> flattens = new ArrayList<Boolean>() ;
        flattens.add(true) ;
        flattens.add(false) ;

        // Create LOForEach
        LOForEach foreach1 = new LOForEach(plan, genNewOperatorKey(), generatePlans, flattens) ;

        // construct the main plan
        plan.add(load1) ;
        plan.add(foreach1) ;

        plan.connect(load1, foreach1);

        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        TypeCheckingValidator typeValidator = new TypeCheckingValidator() ;
        typeValidator.validate(plan, collector) ;
        printMessageCollector(collector) ;
        printTypeGraph(plan) ;

        if (collector.hasError()) {
            throw new AssertionError("Expect no error") ;
        }

        // check outer schema
        Schema endResultSchema = foreach1.getSchema() ;

        assertEquals(endResultSchema.getField(0).type, DataType.FLOAT) ;
        assertEquals(endResultSchema.getField(1).type, DataType.DOUBLE) ;
    }
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOLesserThan extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOLesserThan.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOLesserThan(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "LesserThan " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import static org.junit.Assert.assertEquals;

import java.io.File;
import java.io.IOException;
import java.util.Iterator;

import org.apache.pig.ExecType;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigServer;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DefaultBagFactory;
import org.apache.pig.data.DefaultTuple;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
import org.apache.pig.test.utils.GenPhyOp;
import org.apache.pig.test.utils.TestHelper;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

/**
 *  Start Plan - --4430968173902769765
 *  |
 *  |---Filter - -3398344075398026874
 *  |   |
 *  |   |---For Each - --3361918026493682288
 *  |       |
 *  |       |---Load - --7021057205133896020
 *  |
 *  |---Filter - -4449068980110713814
 *      |
 *      |---For Each - -7192652407897311774
 *          |
 *          |---Load - --3816247505117325386
 *          
 *  Tests the Start Plan operator with the above plan.
 *  The verification is done as follows:
 *  Both loads load the same file(/etc/passwd).
 *  The filters cover the input. Here the filters used
 *  are $2<=50 & $2>50
 *  The bags coming out of Start Plan is checked against
 *  the projected input bag.
 *  Since types are not supported yet, there is an explicit
 *  conversion from DataByteArray to native types for computation
 *  and back to DataByteArray for comparison with input.
 */
public class TestUnion extends junit.framework.TestCase {
    POUnion sp;
    DataBag expBag;
    MiniCluster cluster = MiniCluster.buildCluster();
    PigContext pc = new PigContext();
    @Before
    public void setUp() throws Exception {
        pc.connect();
        GenPhyOp.setPc(pc);
        POLoad ld1 = GenPhyOp.topLoadOp();
        String curDir = System.getProperty("user.dir");
        String inpDir = curDir + File.separatorChar + "test/org/apache/pig/test/data/InputFiles/";
        FileSpec fSpec = new FileSpec("file:"+ inpDir +"passwd", new FuncSpec(PigStorage.class.getName() , new String[]{":"}));
        ld1.setLFile(fSpec);
        
        POLoad ld2 = GenPhyOp.topLoadOp();
        ld2.setLFile(fSpec);
        
        POFilter fl1 = GenPhyOp.topFilterOpWithProj(1, 50, GenPhyOp.LTE);
        
        POFilter fl2 = GenPhyOp.topFilterOpWithProj(1, 50, GenPhyOp.GT);
        
        int[] flds = {0,2};
        Tuple sample = new DefaultTuple();
        sample.append(new String("S"));
        sample.append(new String("x"));
        sample.append(new Integer("10"));
        sample.append(new Integer("20"));
        sample.append(new String("S"));
        sample.append(new String("x"));
        sample.append(new String("S"));
        sample.append(new String("x"));

        POForEach fe1 = GenPhyOp.topForEachOPWithPlan(flds , sample);
        
        POForEach fe2 = GenPhyOp.topForEachOPWithPlan(flds , sample);
        
        sp = GenPhyOp.topUnionOp();
        
        PhysicalPlan plan = new PhysicalPlan();
        
        plan.add(ld1);
        plan.add(ld2);
        plan.add(fl1);
        plan.add(fl2);
        plan.add(fe1);
        plan.add(fe2);
        plan.add(sp);
        
        plan.connect(ld1, fe1);
        plan.connect(fe1, fl1);
        plan.connect(ld2, fe2);
        plan.connect(fe2, fl2);
        plan.connect(fl1, sp);
        plan.connect(fl2, sp);
        
        /*PlanPrinter ppp = new PlanPrinter(plan);
        ppp.visit();*/
        
        
        POLoad ld3 = GenPhyOp.topLoadOp();
        ld3.setLFile(fSpec);
        DataBag fullBag = DefaultBagFactory.getInstance().newDefaultBag();
        Tuple t=null;
        for(Result res=ld3.getNext(t);res.returnStatus!=POStatus.STATUS_EOP;res=ld3.getNext(t)){
            fullBag.add((Tuple)res.result);
        }

        int[] fields = {0,2};
        expBag = TestHelper.projectBag(fullBag, fields);
    }

    @After
    public void tearDown() throws Exception {
    }
    
    private Tuple castToDBA(Tuple in) throws ExecException{
        Tuple res = new DefaultTuple();
        for (int i=0;i<in.size();i++) {
            DataByteArray dba = new DataByteArray(in.get(i).toString());
            res.append(dba);
        }
        return res;
    }

    @Test
    public void testGetNextTuple() throws ExecException, IOException {
        Tuple t = null;
        DataBag outBag = DefaultBagFactory.getInstance().newDefaultBag();
        for(Result res=sp.getNext(t);res.returnStatus!=POStatus.STATUS_EOP;res=sp.getNext(t)){
            outBag.add(castToDBA((Tuple)res.result));
        }
        assertEquals(true, TestHelper.compareBags(expBag, outBag));
    }

    // Test the case when POUnion is one of the roots in a map reduce
    // plan and the input to it can be null
    // This can happen when we have
    // a plan like below
    // POUnion
    // |
    // |--POLocalRearrange
    // |    |
    // |    |-POUnion (root 2)--> This union's getNext() can lead the code here
    // |
    // |--POLocalRearrange (root 1)
    
    // The inner POUnion above is a root in the plan which has 2 roots.
    // So these 2 roots would have input coming from different input
    // sources (dfs files). So certain maps would be working on input only
    // meant for "root 1" above and some maps would work on input
    // meant only for "root 2". In the former case, "root 2" would
    // neither get input attached to it nor does it have predecessors
    @Test
    public void testGetNextNullInput() throws Exception {
        Util.createInputFile(cluster, "a.txt", new String[] {"1\t2\t3", "4\t5\t6"});
        Util.createInputFile(cluster, "b.txt", new String[] {"7\t8\t9", "1\t200\t300"});
        Util.createInputFile(cluster, "c.txt", new String[] {"1\t20\t30"});
        FileLocalizer.deleteTempFiles();
        PigServer pig = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pig.registerQuery("a = load 'a.txt' ;");
        pig.registerQuery("b = load 'b.txt';");
        pig.registerQuery("c = union a, b;");
        pig.registerQuery("d = load 'c.txt' ;");
        pig.registerQuery("e = cogroup c by $0 inner, d by $0 inner;");
        pig.explain("e", System.err);
        // output should be 
        // (1,{(1,2,3),(1,200,300)},{(1,20,30)})
        Tuple expectedResult = new DefaultTuple();
        expectedResult.append(new DataByteArray("1"));
        Tuple[] secondFieldContents = new DefaultTuple[2];
        secondFieldContents[0] = Util.createTuple(Util.toDataByteArrays(new String[] {"1", "2", "3"}));
        secondFieldContents[1] = Util.createTuple(Util.toDataByteArrays(new String[] {"1", "200", "300"}));
        DataBag secondField = Util.createBag(secondFieldContents);
        expectedResult.append(secondField);
        DataBag thirdField = Util.createBag(new Tuple[]{Util.createTuple(Util.toDataByteArrays(new String[]{"1", "20", "30"}))});
        expectedResult.append(thirdField);
        Iterator<Tuple> it = pig.openIterator("e");
        assertEquals(expectedResult, it.next());
        assertFalse(it.hasNext());
    }
    
    // Test schema merge in union when one of the fields is a bag
    @Test
    public void testSchemaMergeWithBag() throws Exception {
        Util.createInputFile(cluster, "input1.txt", new String[] {"dummy"});
        Util.createInputFile(cluster, "input2.txt", new String[] {"dummy"});
        PigServer pig = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        Util.registerMultiLineQuery(pig, "a = load 'input1.txt';" +
        		"b = load 'input2.txt';" +
        		"c = foreach a generate 1, {(1, 'str1')};" +
        		"d = foreach b generate 2, {(2, 'str2')};" +
        		"e = union c,d;" +
        		"");
        Iterator<Tuple> it = pig.openIterator("e");
        Object[] expected = new Object[] { Util.getPigConstant("(1, {(1, 'str1')})"),
                Util.getPigConstant("(2, {(2, 'str2')})")};
        Object[] results = new Object[2];
        int i = 0;
        while(it.hasNext()) {
            if(i == 2) {
                fail("Got more tuples than expected!");
            }
            Tuple t = it.next();
            if(t.get(0).equals(1)) {
                // this is the first tuple
                results[0] = t;
            } else {
                results[1] = t;
            }
            i++;
        }
        for (int j = 0; j < expected.length; j++) {
            assertTrue(expected[j].equals(results[j]));
        }
    }
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Set;
import java.util.TreeSet;

import org.apache.pig.impl.plan.*;
import org.apache.pig.impl.plan.optimizer.*;

import org.junit.Test;

/**
 * Test the generic operator classes (Operator, OperatorPlan,
 * PlanVisitor).  Also includes tests for optimizer framework, since that
 * can use the same generic test operators.
 */

public class TestOperatorPlan extends junit.framework.TestCase {

    private int mNextKey = 0;
    private static final String SCOPE = "RULE";
    private static NodeIdGenerator nodeIdGen = NodeIdGenerator.getGenerator();
    public static int MAX_OPTIMIZATION_ITERATIONS = 250;

    abstract class TOperator extends Operator implements Comparable {
        protected String mName;

        TOperator(String name) {
            super(new OperatorKey("", mNextKey++));
            mName = name;
        }

        public int compareTo(Object o) {
            if (!(o instanceof TOperator)) {
                return -1;
            }

            TOperator other = (TOperator)o;

            return mName.compareTo(other.mName);
        }
    }

    class SingleOperator extends TOperator {
        SingleOperator(String name) {
            super(name);
        }

        public boolean supportsMultipleInputs() {
            return false;
        }

        public boolean supportsMultipleOutputs() {
            return false;
        }

        @Override
        public void visit(PlanVisitor v) throws VisitorException {
            ((TVisitor)v).visit(this);
        }

        public String name() {
            //return this.getClass().getName() + " " + mName
            return mName;
        }

    }

    class MultiOperator extends TOperator {
        MultiOperator(String name) {
            super(name);
        }

        public boolean supportsMultipleInputs() {
            return true;
        }

        public boolean supportsMultipleOutputs() {
            return true;
        }

        public void visit(PlanVisitor v) throws VisitorException {
            ((TVisitor)v).visit(this);
        }

        public String name() {
            //return this.getClass().getName() + " " + mName;
            return mName;
        }

    }
    
    class MultiInputSingleOutputOperator extends TOperator {
        MultiInputSingleOutputOperator(String name) {
            super(name);
        }

        public boolean supportsMultipleInputs() {
            return true;
        }

        public boolean supportsMultipleOutputs() {
            return false;
        }

        @Override
        public void visit(PlanVisitor v) throws VisitorException {
            ((TVisitor)v).visit(this);
        }

        public String name() {
            //return this.getClass().getName() + " " + mName
            return mName;
        }

    }
    
    class MultiOutputSingleInputOperator extends TOperator {
        MultiOutputSingleInputOperator(String name) {
            super(name);
        }

        public boolean supportsMultipleInputs() {
            return false;
        }

        public boolean supportsMultipleOutputs() {
            return true;
        }

        @Override
        public void visit(PlanVisitor v) throws VisitorException {
            ((TVisitor)v).visit(this);
        }

        public String name() {
            //return this.getClass().getName() + " " + mName
            return mName;
        }

    }

    class TPlan extends OperatorPlan<TOperator> {

        public String display() {
            StringBuilder buf = new StringBuilder();

            buf.append("Nodes: ");
            // Guarantee a sorting
            TreeSet<TOperator> ts = new TreeSet(mOps.keySet());
            for (TOperator op : ts) {
                buf.append(op.name());
                buf.append(' ');
            }

            buf.append("FromEdges: ");
            ts = new TreeSet(mFromEdges.keySet());
            Iterator<TOperator> i = ts.iterator();
            while (i.hasNext()) {
                TOperator from = i.next();
                TreeSet<TOperator> ts2 = new TreeSet(mFromEdges.get(from));
                Iterator<TOperator> j = ts2.iterator();
                while (j.hasNext()) {
                    buf.append(from.name());
                    buf.append("->");
                    buf.append(j.next().name());
                    buf.append(' ');
                }
            }

            buf.append("ToEdges: ");
            ts = new TreeSet(mToEdges.keySet());
            i = ts.iterator();
            while (i.hasNext()) {
                TOperator from = i.next();
                TreeSet<TOperator> ts2 = new TreeSet(mToEdges.get(from));
                Iterator<TOperator> j = ts2.iterator();
                while (j.hasNext()) {
                    buf.append(from.name());
                    buf.append("->");
                    buf.append(j.next().name());
                    buf.append(' ');
                }
            }
            return buf.toString();
        }
    }

    abstract class TVisitor extends PlanVisitor<TOperator, TPlan> {
        protected StringBuilder mJournal;

        TVisitor(TPlan plan, PlanWalker<TOperator, TPlan> walker) {
            super(plan, walker);
            mJournal = new StringBuilder();
        }

        public void visit(SingleOperator so) throws VisitorException {
            mJournal.append(so.name());
            mJournal.append(' ');
        }

        public void visit(MultiOperator mo) throws VisitorException {
            mJournal.append(mo.name());
            mJournal.append(' ');
        }
        
        public void visit(MultiInputSingleOutputOperator miso) throws VisitorException {
            mJournal.append(miso.name());
            mJournal.append(' ');
        }
        
        public void visit(MultiOutputSingleInputOperator mosi) throws VisitorException {
            mJournal.append(mosi.name());
            mJournal.append(' ');
        }

        public String getJournal() {
            return mJournal.toString();
        }
    }

    class TDepthVisitor extends TVisitor {

        TDepthVisitor(TPlan plan) {
            super(plan, new DepthFirstWalker(plan));
        }
    }

    class TDependVisitor extends TVisitor {

        TDependVisitor(TPlan plan) {
            super(plan, new DependencyOrderWalker(plan));
        }
    }

    static class TOptimizer extends PlanOptimizer<TOperator, TPlan> {

        public TOptimizer(TPlan plan) {
            super(plan, TestOperatorPlan.MAX_OPTIMIZATION_ITERATIONS);
        }

        public void addRule(Rule rule) {
            mRules.add(rule);
        }
    }

    class AlwaysTransform extends Transformer<TOperator, TPlan> {
        public boolean mTransformed = false;
        private int mNumChecks = 0;

        AlwaysTransform(TPlan plan) {
            super(plan, new DepthFirstWalker<TOperator, TPlan>(plan));
        }

        public boolean check(List<TOperator> nodes) {
            ++mNumChecks;
            return true;
        }

        public void transform(List<TOperator> nodes) {
            mTransformed = true;
        }
        
        public int getNumberOfChecks() {
            return mNumChecks;
        }
    }

    class NeverTransform extends Transformer<TOperator, TPlan> {
        public boolean mTransformed = false;
        private int mNumChecks = 0;

        NeverTransform(TPlan plan) {
            super(plan, new DepthFirstWalker<TOperator, TPlan>(plan));
        }

        public boolean check(List<TOperator> nodes) {
            ++mNumChecks;
            return false;
        }

        public void transform(List<TOperator> nodes) {
            mTransformed = true;
        }
        
        public int getNumberOfChecks() {
            return mNumChecks;
        }
    }

    @Test
    public void testAddRemove() throws Exception {
        // Test that we can add and remove nodes from the plan.  Also test
        // that we can fetch the nodes by operator key, by operator, by
        // roots, by leaves, that they have no predecessors and no
        // successors.

        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        for (int i = 0; i < 3; i++) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }

        // All should be roots, as none are connected
        List<TOperator> roots = plan.getRoots();
        for (int i = 0; i < 3; i++) {
            assertTrue("Roots should contain operator " + i,
                roots.contains(ops[i]));
        }

        // All should be leaves, as none are connected
        List<TOperator> leaves = plan.getLeaves();
        for (int i = 0; i < 3; i++) {
            assertTrue("Leaves should contain operator " + i,
                leaves.contains(ops[i]));
        }

        // Each operator should have no successors or predecessors.
        assertNull(plan.getSuccessors(ops[1]));
        assertNull(plan.getPredecessors(ops[1]));

        // Make sure we find them all when we iterate through them.
        Set<TOperator> s = new HashSet<TOperator>();
        Iterator<TOperator> j = plan.iterator();
        while (j.hasNext()) {
            s.add(j.next());
        }

        for (int i = 0; i < 3; i++) {
            assertTrue("Iterator should contain operator " + i,
                s.contains(ops[i]));
        }

        // Test that we can find an operator by its key.
        TOperator op = plan.getOperator(new OperatorKey("", 1));
        assertEquals("Expected to get back ops[1]", ops[1], op);

        // Test that we can get an operator key by its operator
        OperatorKey opkey = new OperatorKey("", 1);
        assertTrue("Expected to get back key for ops[1]",
            opkey.equals(plan.getOperatorKey(ops[1])));

        // Test that we can remove operators
        plan.remove(ops[2]);

        assertEquals("Should only have two roots now.", 2,
            plan.getRoots().size());
        assertEquals("Should only have two leaves now.", 2,
            plan.getLeaves().size());

        j = plan.iterator();
        int k;
        for (k = 0; j.hasNext(); k++) j.next();
        assertEquals("Iterator should only return two now", 2, k);

        // Remove all operators
        plan.remove(ops[0]);
        plan.remove(ops[1]);

        assertEquals("Should only have no roots now.", 0,
            plan.getRoots().size());
        assertEquals("Should only have no leaves now.", 0,
            plan.getLeaves().size());

        j = plan.iterator();
        assertFalse("Iterator should return nothing now", j.hasNext());
    }

    @Test
    public void testInsertBetween() throws Exception {
        // Test that insertBetween works.

        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        for (int i = 0; i < 3; i++) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }

        // Connect 0 to 2
        plan.connect(ops[0], ops[2]);

        Collection p = plan.getPredecessors(ops[0]);
        assertNull(p);
        p = plan.getSuccessors(ops[0]);
        assertEquals(1, p.size());
        Iterator i = p.iterator();
        assertEquals(ops[2], i.next());

        p = plan.getPredecessors(ops[1]);
        assertNull(p);
        p = plan.getSuccessors(ops[1]);
        assertNull(p);

        p = plan.getPredecessors(ops[2]);
        assertEquals(1, p.size());
        i = p.iterator();
        assertEquals(ops[0], i.next());
        p = plan.getSuccessors(ops[2]);
        assertNull(p);

        // Insert 1 in between 0 and 2
        plan.insertBetween(ops[0], ops[1], ops[2]);

        p = plan.getPredecessors(ops[0]);
        assertNull(p);
        p = plan.getSuccessors(ops[0]);
        assertEquals(1, p.size());
        i = p.iterator();
        assertEquals(ops[1], i.next());

        p = plan.getPredecessors(ops[1]);
        assertEquals(1, p.size());
        i = p.iterator();
        assertEquals(ops[0], i.next());
        p = plan.getSuccessors(ops[1]);
        assertEquals(1, p.size());
        i = p.iterator();
        assertEquals(ops[2], i.next());

        p = plan.getPredecessors(ops[2]);
        assertEquals(1, p.size());
        i = p.iterator();
        assertEquals(ops[1], i.next());
        p = plan.getSuccessors(ops[2]);
        assertNull(p);
    }

    @Test
    public void testInsertBetweenNegative() throws Exception {
        // Test that insertBetween throws errors when it should.

        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[4];
        for (int i = 0; i < 4; i++) {
            ops[i] = new MultiOperator(Integer.toString(i));
            plan.add(ops[i]);
        }

        plan.connect(ops[0], ops[1]);

        boolean caughtIt = false;
        try {
            plan.insertBetween(ops[0], ops[3], ops[2]);
        } catch (PlanException pe) {
            caughtIt = true;
        }
        assertTrue(caughtIt);
    }

    @Test
    public void testLinearGraph() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[5];
        for (int i = 0; i < 5; i++) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
            if (i > 0) plan.connect(ops[i - 1], ops[i]);
        }

        // Test that connecting a node not yet in the plan is detected.
        TOperator bogus = new SingleOperator("X");
        boolean sawError = false;
        try {
            plan.connect(ops[2], bogus);
        } catch (PlanException ioe) {
            assertEquals("Attempt to connect operator X which is not in "
                + "the plan.", ioe.getMessage());
            sawError = true;
        }
        assertTrue("Should have caught an error when we tried to connect a "
            + "node that was not in the plan", sawError);

        // Get roots should just return ops[0]
        List<TOperator> roots = plan.getRoots();
        assertEquals(1, roots.size());
        assertEquals(roots.get(0), ops[0]);

        // Get leaves should just return ops[4]
        List<TOperator> leaves = plan.getLeaves();
        assertEquals(1, leaves.size());
        assertEquals(leaves.get(0), ops[4]);

        // Test that connecting another input to SingleOperator gives
        // error.
        plan.add(bogus);
        sawError = false;
        try {
            plan.connect(bogus, ops[1]);
        } catch (PlanException ioe) {
            assertEquals("Attempt to give operator of type " +
                "org.apache.pig.test.TestOperatorPlan$SingleOperator " +
                "multiple inputs.  This operator does "
                + "not support multiple inputs.", ioe.getMessage());
            sawError = true;
        }
        assertTrue("Should have caught an error when we tried to connect a "
            + "second input to a Single", sawError);

        // Test that connecting another output to SingleOperator gives
        // error.
        sawError = false;
        try {
            plan.connect(ops[0], bogus);
        } catch (PlanException ioe) {
            assertEquals("Attempt to give operator of type " +
                "org.apache.pig.test.TestOperatorPlan$SingleOperator " +
                "multiple outputs.  This operator does "
                + "not support multiple outputs.", ioe.getMessage());
            sawError = true;
        }
        assertTrue("Should have caught an error when we tried to connect a "
            + "second output to a " +
            "org.apache.pig.test.TestOperatorPlan$SingleOperator", sawError);
        plan.remove(bogus);

        // Successor for ops[1] should be ops[2]
        Collection s = plan.getSuccessors(ops[1]);
        assertEquals(1, s.size());
        Iterator i = s.iterator();
        assertEquals(ops[2], i.next());

        // Predecessor for ops[1] should be ops[0]
        Collection p = plan.getPredecessors(ops[1]);
        assertEquals(1, p.size());
        i = p.iterator();
        assertEquals(ops[0], i.next());

        assertEquals("Nodes: 0 1 2 3 4 FromEdges: 0->1 1->2 2->3 3->4 ToEdges: 1->0 2->1 3->2 4->3 ", plan.display());

        // Visit it depth first
        TVisitor visitor = new TDepthVisitor(plan);
        visitor.visit();
        assertEquals("0 1 2 3 4 ", visitor.getJournal());

        // Visit it dependency order
        visitor = new TDependVisitor(plan);
        visitor.visit();
        assertEquals("0 1 2 3 4 ", visitor.getJournal());

        // Test disconnect
        plan.disconnect(ops[2], ops[3]);
        assertEquals("Nodes: 0 1 2 3 4 FromEdges: 0->1 1->2 3->4 ToEdges: 1->0 2->1 4->3 ", plan.display());

        // Test remove
        plan.remove(ops[1]);
        assertEquals("Nodes: 0 2 3 4 FromEdges: 3->4 ToEdges: 4->3 ", plan.display());
    }

    @Test
    public void testDAG() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        for (int i = 0; i < 6; i++) {
            ops[i] = new MultiOperator(Integer.toString(i));
            plan.add(ops[i]);
        }
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        plan.connect(ops[2], ops[3]);
        plan.connect(ops[3], ops[4]);
        plan.connect(ops[3], ops[5]);

        // Get roots should return ops[0] and ops[1]
        List<TOperator> roots = plan.getRoots();
        assertEquals(2, roots.size());
        assertTrue(roots.contains(ops[0]));
        assertTrue(roots.contains(ops[1]));

        // Get leaves should return ops[4] and ops[5]
        List<TOperator> leaves = plan.getLeaves();
        assertEquals(2, leaves.size());
        assertTrue(leaves.contains(ops[4]));
        assertTrue(leaves.contains(ops[5]));

        // Successor for ops[3] should be ops[4] and ops[5]
        List<TOperator> s = new ArrayList<TOperator>(plan.getSuccessors(ops[3]));
        assertEquals(2, s.size());
        assertTrue(s.contains(ops[4]));
        assertTrue(s.contains(ops[5]));
        
        // Predecessor for ops[2] should be ops[0] and ops[1]
        s = new ArrayList<TOperator>(plan.getPredecessors(ops[2]));
        assertEquals(2, s.size());
        assertTrue(s.contains(ops[0]));
        assertTrue(s.contains(ops[1]));

        assertEquals("Nodes: 0 1 2 3 4 5 FromEdges: 0->2 1->2 2->3 3->4 3->5 ToEdges: 2->0 2->1 3->2 4->3 5->3 ", plan.display());

        // Visit it depth first
        TVisitor visitor = new TDepthVisitor(plan);
        visitor.visit();
        // There are a number of valid patterns, make sure we found one of
        // them.
        String result = visitor.getJournal();
        assertTrue(result.equals("1 2 3 4 5 0 ") ||
            result.equals("1 2 3 5 4 0 ") || result.equals("0 2 3 4 5 1 ")
            || result.equals("0 2 3 5 4 1 "));

        // Visit it dependency order
        visitor = new TDependVisitor(plan);
        visitor.visit();
        result = visitor.getJournal();
        assertTrue(result.equals("0 1 2 3 4 5 ") ||
            result.equals("0 1 2 3 5 4 "));

        // Test disconnect
        plan.disconnect(ops[2], ops[3]);
        assertEquals("Nodes: 0 1 2 3 4 5 FromEdges: 0->2 1->2 3->4 3->5 ToEdges: 2->0 2->1 4->3 5->3 ", plan.display());

        // Test remove
        plan.remove(ops[2]);
        assertEquals("Nodes: 0 1 3 4 5 FromEdges: 3->4 3->5 ToEdges: 4->3 5->3 ", plan.display());
    }

    // Test that we don't match when nodes don't match pattern.  Will give
    // a pattern of S->S->M and a plan of S->M->S.
    @Test
    public void testOptimizerDifferentNodes() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        ops[2] = new SingleOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);
        
        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();
        assertFalse(transformer.mTransformed);
    }

    // Test that we don't match when edges don't match pattern.  Will give
    // a pattern of S->S->M and a plan of S->S M.
    @Test
    public void testOptimizerDifferentEdges() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        
        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);
        optimizer.optimize();
        assertFalse(transformer.mTransformed);
    }

    // Test that we match when appropriate.  Will give
    // a pattern of S->S->M and a plan of S->S->M.
    @Test
    public void testOptimizerMatches() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);
        
        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();

        assertTrue(transformer.mTransformed);
    }

    // Test that we match when the pattern says any.  Will give
    // a pattern of any and a plan of S->S->M.
    @Test
    public void testOptimizerMatchesAny() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class,
                RuleOperator.NodeType.ANY_NODE,
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);

        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();
        assertTrue(transformer.mTransformed);
    }

    // Test that we match when the whole plan doesn't match.  Will give
    // a pattern of S->S->M and a plan of S->S->S->M.
    @Test
    public void testOptimizerMatchesPart() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[4];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new SingleOperator("3");
        plan.add(ops[2]);
        ops[3] = new MultiOperator("4");
        plan.add(ops[3]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);
        plan.connect(ops[2], ops[3]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();
        assertTrue(transformer.mTransformed);
    }

    // Test that we match when a node is optional and the optional node is
    // present.  Will give
    // a pattern of S->S->M (with second S optional) and a plan of S->S->M.
    @Test
    public void testOptimizerOptionalMatches() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();
        assertTrue(transformer.mTransformed);
    }

    // Test that we do not match when a node is missing.  Will give
    // a pattern of S->S->M and a plan of S->M.
    @Test
    public void testOptimizerOptionalMissing() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[2];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        plan.connect(ops[0], ops[1]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform transformer = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();
        assertFalse(transformer.mTransformed);
    }

    // Test that even if we match, if check returns false then the optimization
    // is not done.
    @Test
    public void testCheck() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        NeverTransform transformer = new NeverTransform(plan);
        Rule<TOperator, TPlan> r =
            new Rule<TOperator, TPlan>(rulePlan, transformer, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r);

        optimizer.optimize();
        assertFalse(transformer.mTransformed);
    }

    @Test
    public void testReplace() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        ops[3] = new MultiOperator("4");
        plan.add(ops[3]);
        ops[4] = new MultiOperator("5");
        plan.add(ops[4]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        plan.connect(ops[2], ops[3]);
        plan.connect(ops[2], ops[4]);
        ops[5] = new MultiOperator("6");
        plan.replace(ops[2], ops[5]);

        assertEquals("Nodes: 1 2 4 5 6 FromEdges: 1->6 2->6 6->4 6->5 ToEdges: 4->6 5->6 6->1 6->2 ", plan.display());
    }

    @Test
    public void testReplaceNoConnections() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[4];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        ops[3] = new MultiOperator("4");
        plan.replace(ops[1], ops[3]);

        assertEquals("Nodes: 1 3 4 FromEdges: 1->3 ToEdges: 3->1 ", plan.display());
    }
    
    // Input and pattern are both
    // S   S
    //  \ /
    //   M
    // Test that we match
    @Test
    public void testMultiInputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, multiOperator_1);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
    }
    
    // Pattern
    // S   M
    //  \ /
    //   M
    // Input has the roots swapped
    // M   S
    //  \ /
    //   M
    // Test that we match
    @Test
    public void testIsomorphicMultiInputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(multiOperator_1);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_1, multiOperator_2);
        rulePlan.connect(singleOperator_1, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
    }
    
    // Input and pattern are both
    // M
    // ||
    // M
    // Test that we match
    @Test
    public void testMultiInputMultiOutputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[2];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[0], ops[1]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(multiOperator_1);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_1, multiOperator_2);
        rulePlan.connect(multiOperator_1, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");
        
        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
    }

    // Input and pattern are both
    //  M
    // / \
    // S  S
    // Test that we match
    @Test
    public void testMultiOutputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new SingleOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[0], ops[2]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(multiOperator_1);
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.connect(multiOperator_1, singleOperator_1);
        rulePlan.connect(multiOperator_1, singleOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");
        
        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
    }
    
    // Pattern
    //  M
    // / \
    // S  S
    // Input
    //  M
    //  |
    //  S
    // Test that we don't match
    @Test
    public void testNegativeMultiOutputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[2];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        plan.connect(ops[0], ops[1]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(multiOperator_1);
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.connect(multiOperator_1, singleOperator_1);
        rulePlan.connect(multiOperator_1, singleOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");
        
        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertFalse(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == 0); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == 0); //default max iterations
    }
    
    // Pattern
    //  M
    //  |
    //  M
    // Input
    //  M
    // ||
    //  M
    // Test that we don't match
    @Test
    public void testNegativeMultiOutputPattern1() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[2];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new MultiOperator("2");
        plan.add(ops[1]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[0], ops[1]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(multiOperator_1);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_1, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");
        
        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertFalse(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == 0); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == 0); //default max iterations
    }
    
    // Pattern
    // S   S
    //  \ /
    //   M
    // Input
    // S   S    S   S
    //  \ /      \ /
    //   M        M
    // Test that we match multiple instances in the disconnected graph
    @Test
    public void testMultipleMultiInputPatternInDisconnectedGraph() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        
        ops[3] = new SingleOperator("4");
        plan.add(ops[3]);
        ops[4] = new SingleOperator("5");
        plan.add(ops[4]);
        ops[5] = new MultiOperator("6");
        plan.add(ops[5]);
        plan.connect(ops[3], ops[5]);
        plan.connect(ops[4], ops[5]);


        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, multiOperator_1);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == (2* MAX_OPTIMIZATION_ITERATIONS)); 
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == (2 * MAX_OPTIMIZATION_ITERATIONS));
    }

    // Pattern is
    // S   S
    //  \ /
    //   M
    // Input
    // S   S    S   S
    //  \ /      \ /
    //   M        M
    //    \      /
    //       M
    // Test that we match multiple instances in a connected graph
    @Test
    public void testMultipleMultiInputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[7];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        
        ops[3] = new SingleOperator("4");
        plan.add(ops[3]);
        ops[4] = new SingleOperator("5");
        plan.add(ops[4]);
        ops[5] = new MultiOperator("6");
        plan.add(ops[5]);
        plan.connect(ops[3], ops[5]);
        plan.connect(ops[4], ops[5]);

        ops[6] = new MultiOperator("7");
        plan.add(ops[6]);
        plan.connect(ops[2], ops[6]);
        plan.connect(ops[5], ops[6]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, multiOperator_1);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == (2 * MAX_OPTIMIZATION_ITERATIONS));
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == (2 * MAX_OPTIMIZATION_ITERATIONS));
    }

    // Pattern is
    // S   S
    //  \ /
    //   M
    // Test that we match only one instance in a connected graph
    @Test
    public void testSingleMultiInputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        
        ops[3] = new SingleOperator("4");
        plan.add(ops[3]);
        ops[4] = new MultiOperator("5");
        plan.add(ops[4]);
        plan.connect(ops[3], ops[4]);

        ops[5] = new MultiOperator("6");
        plan.add(ops[5]);
        plan.connect(ops[4], ops[5]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, multiOperator_1);
        rulePlan.connect(singleOperator_2, multiOperator_1);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS);
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS);
    }
    
    // Input and pattern are both
    //  M
    // / \
    // S  S
    // \ /
    //  M
    // Test that we match
    @Test
    public void testDiamondPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[4];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new SingleOperator("3");
        plan.add(ops[2]);
        ops[3] = new MultiOperator("4");
        plan.add(ops[3]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[3]);
        plan.connect(ops[2], ops[3]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(multiOperator_1);
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_1, singleOperator_1);
        rulePlan.connect(multiOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_1, multiOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");
        
        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
    }

    // Pattern 
    //  M
    // / \
    // S  S
    // \ /
    //  M
    // Input has an additional edge from the bottom of the diamond
    //  M
    // / \
    // S  S
    // \ /
    //  M
    //  |
    //  S
    // Test that we match
    @Test
    public void testDiamondWithEdgePattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[5];
        ops[0] = new MultiOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new SingleOperator("3");
        plan.add(ops[2]);
        ops[3] = new MultiOperator("4");
        plan.add(ops[3]);
        ops[4] = new SingleOperator("5");
        plan.add(ops[4]);
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[3]);
        plan.connect(ops[2], ops[3]);
        plan.connect(ops[3], ops[4]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(multiOperator_1);
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_1, singleOperator_1);
        rulePlan.connect(multiOperator_1, singleOperator_2);
        rulePlan.connect(singleOperator_1, multiOperator_2);
        rulePlan.connect(singleOperator_2, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");
        
        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS); //default max iterations
    }
    
    // Input and Pattern is
    // S   S
    //  \ /
    //   M
    //   |
    //   M
    //  / \
    // S   S
    // Test that we match once
    @Test
    public void testComplexInputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        
        ops[3] = new SingleOperator("4");
        plan.add(ops[3]);
        ops[4] = new SingleOperator("5");
        plan.add(ops[4]);
        ops[5] = new MultiOperator("6");
        plan.add(ops[5]);
        plan.connect(ops[5], ops[3]);
        plan.connect(ops[5], ops[4]);

        plan.connect(ops[2], ops[5]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, multiOperator_1);
        rulePlan.connect(singleOperator_2, multiOperator_1);
        
        RuleOperator singleOperator_3 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_4 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_3);
        rulePlan.add(singleOperator_4);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_2, singleOperator_3);
        rulePlan.connect(multiOperator_2, singleOperator_4);
        
        rulePlan.connect(multiOperator_1, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertTrue(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS);
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == MAX_OPTIMIZATION_ITERATIONS);
    }
    
    // Pattern is
    // S   S
    //  \ /
    //   M
    //   ||
    //   M
    //  / \
    // S   S
    // Input is
    // S   S
    //  \ /
    //   M
    //   |
    //   M
    //  / \
    // S   S
    // Test that we don't match
    @Test
    public void testNegativeComplexInputPattern() throws Exception {
        // Build a plan
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        
        ops[3] = new SingleOperator("4");
        plan.add(ops[3]);
        ops[4] = new SingleOperator("5");
        plan.add(ops[4]);
        ops[5] = new MultiOperator("6");
        plan.add(ops[5]);
        plan.connect(ops[5], ops[3]);
        plan.connect(ops[5], ops[4]);

        plan.connect(ops[2], ops[5]);

        // Create our rule
        RulePlan rulePlan = new RulePlan();
        RuleOperator singleOperator_1 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_2 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_1 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_1);
        rulePlan.add(singleOperator_2);
        rulePlan.add(multiOperator_1);
        rulePlan.connect(singleOperator_1, multiOperator_1);
        rulePlan.connect(singleOperator_2, multiOperator_1);
        
        RuleOperator singleOperator_3 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator singleOperator_4 = new RuleOperator(SingleOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        RuleOperator multiOperator_2 = new RuleOperator(MultiOperator.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(singleOperator_3);
        rulePlan.add(singleOperator_4);
        rulePlan.add(multiOperator_2);
        rulePlan.connect(multiOperator_2, singleOperator_3);
        rulePlan.connect(multiOperator_2, singleOperator_4);
        
        rulePlan.connect(multiOperator_1, multiOperator_2);
        rulePlan.connect(multiOperator_1, multiOperator_2);

        AlwaysTransform alwaysTransform = new AlwaysTransform(plan);
        Rule<TOperator, TPlan> r1 =
            new Rule<TOperator, TPlan>(rulePlan, alwaysTransform, "TestRule");

        NeverTransform neverTransform = new NeverTransform(plan);
        Rule<TOperator, TPlan> r2 =
            new Rule<TOperator, TPlan>(rulePlan, neverTransform, "TestRule");

        TOptimizer optimizer = new TOptimizer(plan);
        optimizer.addRule(r1);
        optimizer.addRule(r2);

        optimizer.optimize();
        assertFalse(alwaysTransform.mTransformed);
        assertTrue(alwaysTransform.getNumberOfChecks() == 0);
        assertFalse(neverTransform.mTransformed);
        assertTrue(neverTransform.getNumberOfChecks() == 0);
    }
    
    //Swap two roots in a graph. Both the roots are disconnected
    //and are the only nodes in the graph
    @Test
    public void testSwapRootsInDisconnectedGraph() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[2];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }
        
        plan.swap(ops[0], ops[1]);
        
        List<TOperator> roots = (ArrayList<TOperator>)plan.getRoots();
        for(int i = 0; i < roots.size(); ++i) {
            assertEquals(roots.get(i), ops[i]);
        }
    }
    
    //Swap two nodes in a graph.
    //Input
    //S1->S2
    //Ouput
    //S2->S1
    //Swap again
    //Output
    //S1->S2
    @Test
    public void testSimpleSwap() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[2];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }
        
        plan.connect(ops[0], ops[1]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();
        
        plan.swap(ops[0], ops[1]);
        
        //planPrinter.visit();
        
        List<TOperator> roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[1]);
        
        List<TOperator> rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[0]);
        
        List<TOperator> leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[0]);
        
        List<TOperator> leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[1]);
        
        plan.swap(ops[0], ops[1]);
        
        //planPrinter.visit();
        
        roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[0]);
        
        rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[1]);
        
        leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[1]);
        
        leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[0]);
    }
    
    //Swap two nodes in a graph.
    //Swap S1 and S3
    //Input
    //S1->S2->S3
    //Intermediate Output
    //S3->S2->S1
    //Output
    //S1->S2->S3
    @Test
    public void testSimpleSwap2() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }
        
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();
        
        plan.swap(ops[0], ops[2]);

        //planPrinter.visit();
        
        List<TOperator> roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[2]);
        
        List<TOperator> rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[1]);
        
        List<TOperator> leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[0]);
        
        List<TOperator> leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[1]);
        
        plan.swap(ops[0], ops[2]);
        
        //planPrinter.visit();
        
        roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[0]);
        
        rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[1]);
        
        leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[2]);
        
        leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[1]);
    }
    
    //Swap two nodes in a graph and then swap it back again.
    //Swap S2 and S3
    //Input
    //S1->S2->S3
    //Intermediate Output
    //S1->S3->S2
    //Output
    //S1->S2->S3
    @Test
    public void testSimpleSwap3() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }
        
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        plan.swap(ops[1], ops[2]);
        
        //planPrinter.visit();

        List<TOperator> roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[0]);
        
        List<TOperator> rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[2]);
        
        List<TOperator> leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[1]);
        
        List<TOperator> leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[2]);
        
        plan.swap(ops[1], ops[2]);
        
        //planPrinter.visit();

        roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[0]);
        
        rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[1]);
        
        leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[2]);
        
        leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[1]);
    }
    
    //Swap two nodes in a graph and then swap it back again.
    //Swap S1 and S2
    //Input
    //S1->S2->S3
    //Intermediate Output
    //S2->S1->S3
    //Output
    //S1->S2->S3
    @Test
    public void testSimpleSwap4() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[3];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
            plan.add(ops[i]);
        }
        
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        plan.swap(ops[0], ops[1]);

        //planPrinter.visit();
        
        List<TOperator> roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[1]);
        
        List<TOperator> rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[0]);
        
        List<TOperator> leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[2]);
        
        List<TOperator> leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[0]);
        
        plan.swap(ops[0], ops[1]);

        //planPrinter.visit();
        
        roots = (ArrayList<TOperator>)plan.getRoots();
        assertEquals(roots.get(0), ops[0]);
        
        rootSuccessors = plan.getSuccessors(roots.get(0));
        assertEquals(rootSuccessors.get(0), ops[1]);
        
        leaves = (ArrayList<TOperator>)plan.getLeaves();
        assertEquals(leaves.get(0), ops[2]);
        
        leafPredecessors = plan.getPredecessors(leaves.get(0));
        assertEquals(leafPredecessors.get(0), ops[1]);
    }
    
    //Swap non-existent nodes in a graph and check for exceptions
    //Swap S1 and S4
    //Swap S4 and S1
    //Swap S5 and S4
    //Swap S1 and null
    //Swap null and S1
    //Swap null and null
    //Input
    //S1->S2->S3 S4 S5
    @Test
    public void testNegativeSimpleSwap() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[5];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
        }

        for(int i = 0; i < ops.length - 2; ++i) {
            plan.add(ops[i]);
        }

        
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        try {
            plan.swap(ops[0], ops[3]);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }
        
        try {
            plan.swap(ops[3], ops[0]);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }

        try {
            plan.swap(ops[4], ops[3]);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }
        
        try {
            plan.swap(ops[0], null);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1092);
        }
        
        try {
            plan.swap(null, ops[0]);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1092);
        }
        
        try {
            plan.swap(null, null);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1092);
        }

    }
    
    //Swap nodes that have multiple inputs and multiple outs in a graph and check for exceptions
    //Input
    // S1   S2
    //  \ /
    //   M1
    //   |
    //   M2
    //  / \
    // S3   S4
    //Swap S1 and M1
    //Swap M1 and S1
    //Swap M1 and M2
    //Swap M2 and M1
    //Swap M2 and S3
    //Swap S3 and M2
    @Test
    public void testNegativeSimpleSwap1() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[6];
        ops[0] = new SingleOperator("1");
        plan.add(ops[0]);
        ops[1] = new SingleOperator("2");
        plan.add(ops[1]);
        ops[2] = new MultiOperator("3");
        plan.add(ops[2]);
        plan.connect(ops[0], ops[2]);
        plan.connect(ops[1], ops[2]);
        
        ops[3] = new SingleOperator("4");
        plan.add(ops[3]);
        ops[4] = new SingleOperator("5");
        plan.add(ops[4]);
        ops[5] = new MultiOperator("6");
        plan.add(ops[5]);
        plan.connect(ops[5], ops[3]);
        plan.connect(ops[5], ops[4]);

        plan.connect(ops[2], ops[5]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        try {
            plan.swap(ops[0], ops[2]);
            fail("Expected exception for multi-input operator.");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1093);
        }
        
        try {
            plan.swap(ops[2], ops[0]);
            fail("Expected exception for multi-input operator.");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1093);
        }

        try {
            plan.swap(ops[2], ops[5]);
            fail("Expected exception for multi-input operator.");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1093);
        }
        
        try {
            plan.swap(ops[5], ops[2]);
            fail("Expected exception for multi-output operator.");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1093);
        }
        
        try {
            plan.swap(ops[5], ops[3]);
            fail("Expected exception for multi-output operator.");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1093);
        }
        
        try {
            plan.swap(ops[3], ops[5]);
            fail("Expected exception for multi-output operator.");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1093);
        }

    }
    
    //Push M11 before M10's inputs - 0 through 3
    //Input
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //      /  |  \
    //     S5  M11  S6
    //      /  |  \
    //     S7  S8  S9
    //Output when pushed before 1st input
    //       S2
    //       |
    //  S1   M11  S3  S4
    //   \   |   |  /
    //       M10
    //   / / |  \ \
    // S5 S7 S8 S9 S6
    @Test
    public void testpushBefore() throws Exception {
        for(int index = 0; index < 4; index++) {
            TPlan plan = new TPlan();
            TOperator[] ops = new TOperator[11];
            
            for(int i = 0; i < ops.length - 2; ++i) {
                ops[i] = new SingleOperator(Integer.toString(i+1));
                plan.add(ops[i]);
            }
            
            ops[9] = new MultiOperator("10");
            plan.add(ops[9]);
            
            ops[10] = new MultiOperator("11");
            plan.add(ops[10]);
            
            plan.connect(ops[0], ops[9]);
            plan.connect(ops[1], ops[9]);
            plan.connect(ops[2], ops[9]);
            plan.connect(ops[3], ops[9]);
            plan.connect(ops[9], ops[4]);
            plan.connect(ops[9], ops[10]);
            plan.connect(ops[9], ops[5]);
            
            plan.connect(ops[10], ops[6]);
            plan.connect(ops[10], ops[7]);
            plan.connect(ops[10], ops[8]);
            
            //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
            //planPrinter.visit();
    
            plan.pushBefore(ops[9], ops[10], index) ;
            
            //planPrinter.visit();
            
            Set<TOperator> rootSet = new HashSet<TOperator>();
            rootSet.add(ops[0]);
            rootSet.add(ops[1]);
            rootSet.add(ops[2]);
            rootSet.add(ops[3]);
            
            Set<TOperator> expectedRootSet = new HashSet<TOperator>(plan.getRoots());
            
            rootSet.retainAll(expectedRootSet);
            assertTrue(rootSet.size() == 4);
            
            Set<TOperator> leafSet = new HashSet<TOperator>();
            leafSet.add(ops[4]);
            leafSet.add(ops[5]);
            leafSet.add(ops[6]);
            leafSet.add(ops[7]);
            leafSet.add(ops[8]);
            
            Set<TOperator> expectedLeafSet = new HashSet<TOperator>(plan.getLeaves());
            
            leafSet.retainAll(expectedLeafSet);
            assertTrue(leafSet.size() == 5);
            
            List<TOperator> m10Predecessors = plan.getPredecessors(ops[9]);            
            assertTrue(m10Predecessors.get(index) == ops[10]);
            
            List<TOperator> m11Predecessors = plan.getPredecessors(ops[10]);
            assertTrue(m11Predecessors.get(0) == ops[index]);
        }
    }
    
    //Push S5 and S6 before M10's input
    //Input
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //      /  |  \
    //     S5  M11  S6
    //      /  |  \
    //     S7  S8  S9
    //Output when pushed before 1st input
    //       S2
    //       |
    //  S1   S5  S3  S4
    //   \   |   |  /
    //        M10
    //       /   \
    //      M11   S6
    //    /  |  \
    //   S7  S8  S9
    @Test
    public void testpushBefore2() throws Exception {
        for(int outerIndex = 0; outerIndex < 2; outerIndex++) {
            for(int index = 0; index < 2; index++) {
                TPlan plan = new TPlan();
                TOperator[] ops = new TOperator[11];
                
                for(int i = 0; i < ops.length - 2; ++i) {
                    ops[i] = new SingleOperator(Integer.toString(i+1));
                    plan.add(ops[i]);
                }
                
                ops[9] = new MultiOperator("10");
                plan.add(ops[9]);
                
                ops[10] = new MultiOperator("11");
                plan.add(ops[10]);
                
                plan.connect(ops[0], ops[9]);
                plan.connect(ops[1], ops[9]);
                plan.connect(ops[2], ops[9]);
                plan.connect(ops[3], ops[9]);
                plan.connect(ops[9], ops[4]);
                plan.connect(ops[9], ops[10]);
                plan.connect(ops[9], ops[5]);
                
                plan.connect(ops[10], ops[6]);
                plan.connect(ops[10], ops[7]);
                plan.connect(ops[10], ops[8]);
                
                //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
                //planPrinter.visit();
        
                int secondNodeIndex = outerIndex + 4;
                plan.pushBefore(ops[9], ops[secondNodeIndex], index) ;
                
                //planPrinter.visit();
                
                Set<TOperator> rootSet = new HashSet<TOperator>();
                rootSet.add(ops[0]);
                rootSet.add(ops[1]);
                rootSet.add(ops[2]);
                rootSet.add(ops[3]);
                
                Set<TOperator> expectedRootSet = new HashSet<TOperator>(plan.getRoots());
                
                rootSet.retainAll(expectedRootSet);
                assertTrue(rootSet.size() == 4);
                
                Set<TOperator> leafSet = new HashSet<TOperator>();
                for(int leafIndex = 4; leafIndex < 9; ++leafIndex) {
                    if(leafIndex != secondNodeIndex) {
                        leafSet.add(ops[leafIndex]);
                    }
                }
                
                Set<TOperator> expectedLeafSet = new HashSet<TOperator>(plan.getLeaves());
                
                leafSet.retainAll(expectedLeafSet);
                assertTrue(leafSet.size() == 4);
                
                List<TOperator> outerIndexNodePredecessors = plan.getPredecessors(ops[secondNodeIndex]);            
                assertTrue(outerIndexNodePredecessors.get(0) == ops[index]);
                
                List<TOperator> m10Predecessors = plan.getPredecessors(ops[9]);            
                assertTrue(m10Predecessors.get(index) == ops[secondNodeIndex]);
            }
        }
    }

    //Push non-existent nodes in a graph and check for exceptions
    //Push S1 after S4
    //Push S4 after S1
    //Push S5 after S4
    //Push S1 after null
    //Push null after S1
    //Push null after null
    //Input
    //S1->S2->S3 S4 S5
    @Test
    public void testNegativePushBefore() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[5];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
        }

        for(int i = 0; i < ops.length - 2; ++i) {
            plan.add(ops[i]);
        }

        
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        try {
            plan.pushBefore(ops[0], ops[3], 0);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }
        
        try {
            plan.pushBefore(ops[3], ops[0], 0);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }

        try {
            plan.pushBefore(ops[4], ops[3], 0);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }
        
        try {
            plan.pushBefore(ops[0], null, 0);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1085);
        }
        
        try {
            plan.pushBefore(null, ops[0], 0);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1085);
        }
        
        try {
            plan.pushBefore(null, null, 0);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1085);
        }

    }

    //Negative test cases
    //Input
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //      /  |  \
    //     S5  M11  S6
    //      /  |  \
    //     S7  S8  S9
    @Test
    public void testNegativePushBefore2() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[11];
        
        for(int i = 0; i < ops.length - 2; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i+1));
            plan.add(ops[i]);
        }
        
        ops[9] = new MultiOperator("10");
        plan.add(ops[9]);
        
        ops[10] = new MultiOperator("11");
        plan.add(ops[10]);
        
        plan.connect(ops[0], ops[9]);
        plan.connect(ops[1], ops[9]);
        plan.connect(ops[2], ops[9]);
        plan.connect(ops[3], ops[9]);
        plan.connect(ops[9], ops[4]);
        plan.connect(ops[9], ops[10]);
        plan.connect(ops[9], ops[5]);
        
        plan.connect(ops[10], ops[6]);
        plan.connect(ops[10], ops[7]);
        plan.connect(ops[10], ops[8]);
        
        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        try {
            plan.pushBefore(ops[0], ops[9], 0) ;
            fail("Expected exception for first operator having null predecessors");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1086);
        }
        
        try {
            plan.pushBefore(ops[10], ops[6], 0) ;
            fail("Expected exception for first operator having one predecessor");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1086);
        }
        
        try {
            plan.pushBefore(ops[9], ops[10], 4) ;
            fail("Expected exception for inputNum exceeding number of predecessors");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1087);
        }
        
        try {
            plan.pushBefore(ops[9], ops[0], 0) ;
            fail("Expected exception for second operator having null predecessors");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1088);
        }

        try {
            plan.pushBefore(ops[9], ops[9], 0) ;
            fail("Expected exception for second operator having more than one predecessor");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1088);
        }
        
        try {
            plan.pushBefore(ops[9], ops[8], 0) ;
            fail("Expected exception for second operator not being a successor of first operator");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1089);
        }
        
        plan.disconnect(ops[9], ops[4]);
        plan.disconnect(ops[9], ops[5]);
        
        MultiInputSingleOutputOperator miso = new MultiInputSingleOutputOperator("12");
        
        plan.replace(ops[9], miso);
        
        try {
            plan.pushBefore(miso, ops[10], 0) ;
            fail("Expected exception for trying to connect multiple outputs to the first operator");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1091);
        }
        
    }

    //Push M10 after M11's outputs - 0 through 2
    //Input
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //    S5   |  S6
    //     \   |  /
    //        M11
    //      /  |  \
    //     S7  S8  S9
    //Output when pushed after 1st output
    //  S5 S1  S2  S3  S4 S6
    //   \   \  \  /  /  /
    //           M10
    //         /  |  \
    //        S7  M11  S9
    //            |
    //            S8

    @Test
    public void testpushAfter() throws Exception {
        for(int index = 0; index < 3; index++) {
            TPlan plan = new TPlan();
            TOperator[] ops = new TOperator[11];
            
            for(int i = 0; i < ops.length - 2; ++i) {
                ops[i] = new SingleOperator(Integer.toString(i+1));
                plan.add(ops[i]);
            }
            
            ops[9] = new MultiOperator("10");
            plan.add(ops[9]);
            
            ops[10] = new MultiOperator("11");
            plan.add(ops[10]);
            
            plan.connect(ops[0], ops[9]);
            plan.connect(ops[1], ops[9]);
            plan.connect(ops[2], ops[9]);
            plan.connect(ops[3], ops[9]);
            plan.connect(ops[9], ops[10]);
            plan.connect(ops[4], ops[10]);
            plan.connect(ops[5], ops[10]);
            
            plan.connect(ops[10], ops[6]);
            plan.connect(ops[10], ops[7]);
            plan.connect(ops[10], ops[8]);
            
            //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
            //planPrinter.visit();
    
            plan.pushAfter(ops[10], ops[9], index) ;
            
            //planPrinter.visit();
            
            Set<TOperator> rootSet = new HashSet<TOperator>();
            rootSet.add(ops[0]);
            rootSet.add(ops[1]);
            rootSet.add(ops[2]);
            rootSet.add(ops[3]);
            rootSet.add(ops[4]);
            rootSet.add(ops[5]);
            
            Set<TOperator> expectedRootSet = new HashSet<TOperator>(plan.getRoots());
            
            rootSet.retainAll(expectedRootSet);
            assertTrue(rootSet.size() == 6);
            
            Set<TOperator> leafSet = new HashSet<TOperator>();
            leafSet.add(ops[6]);
            leafSet.add(ops[7]);
            leafSet.add(ops[8]);
            
            Set<TOperator> expectedLeafSet = new HashSet<TOperator>(plan.getLeaves());
            
            leafSet.retainAll(expectedLeafSet);
            assertTrue(leafSet.size() == 3);
            
            List<TOperator> m10Successors = plan.getSuccessors(ops[9]);            
            assertTrue(m10Successors.get(0) == ops[index + 6]);
            
            List<TOperator> m11Successors = plan.getSuccessors(ops[10]);
            assertTrue(m11Successors.get(index) == ops[9]);
        }
    }

    //Push S5 and S6 after M11's outputs - 0 through 2
    //Input
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //    S5   |  S6
    //     \   |  /
    //        M11
    //      /  |  \
    //     S7  S8  S9
    //Output when S5 is pushed after 1st output
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //         |  S6
    //         |  /
    //        M11
    //      /  |  \
    //     S7  S5  S9
    //         |
    //         S8

    @Test
    public void testpushAfter1() throws Exception {
        for(int outerIndex = 0; outerIndex < 2; outerIndex++) {
            for(int index = 0; index < 3; index++) {
                TPlan plan = new TPlan();
                TOperator[] ops = new TOperator[11];
                
                for(int i = 0; i < ops.length - 2; ++i) {
                    ops[i] = new SingleOperator(Integer.toString(i+1));
                    plan.add(ops[i]);
                }
                
                ops[9] = new MultiOperator("10");
                plan.add(ops[9]);
                
                ops[10] = new MultiOperator("11");
                plan.add(ops[10]);
                
                plan.connect(ops[0], ops[9]);
                plan.connect(ops[1], ops[9]);
                plan.connect(ops[2], ops[9]);
                plan.connect(ops[3], ops[9]);
                plan.connect(ops[9], ops[10]);
                plan.connect(ops[4], ops[10]);
                plan.connect(ops[5], ops[10]);
                
                plan.connect(ops[10], ops[6]);
                plan.connect(ops[10], ops[7]);
                plan.connect(ops[10], ops[8]);
                
                //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
                //planPrinter.visit();
        
                int secondNodeIndex = outerIndex + 4;
                plan.pushAfter(ops[10], ops[secondNodeIndex], index) ;
                
                //planPrinter.visit();
                
                Set<TOperator> rootSet = new HashSet<TOperator>();
                rootSet.add(ops[0]);
                rootSet.add(ops[1]);
                rootSet.add(ops[2]);
                rootSet.add(ops[3]);

                for(int rootIndex = 0; rootIndex < 6; ++rootIndex) {
                    if(rootIndex != secondNodeIndex) {
                        rootSet.add(ops[rootIndex]);
                    }
                }

                Set<TOperator> expectedRootSet = new HashSet<TOperator>(plan.getRoots());
                
                rootSet.retainAll(expectedRootSet);
                assertTrue(rootSet.size() == 5);

                Set<TOperator> leafSet = new HashSet<TOperator>();
                leafSet.add(ops[6]);
                leafSet.add(ops[7]);
                leafSet.add(ops[8]);
                
                Set<TOperator> expectedLeafSet = new HashSet<TOperator>(plan.getLeaves());
                
                leafSet.retainAll(expectedLeafSet);
                assertTrue(leafSet.size() == 3);
                
                List<TOperator> outerIndexNodeSuccessors = plan.getSuccessors(ops[secondNodeIndex]);
                assertTrue(outerIndexNodeSuccessors.get(0) == ops[index + 6]);
                
                List<TOperator> m11Successors = plan.getSuccessors(ops[10]);            
                assertTrue(m11Successors.get(index) == ops[secondNodeIndex]);
            }
        }
    }
    
    //Push non-existent nodes in a graph and check for exceptions
    //Push S1 after S4
    //Push S4 after S1
    //Push S5 after S4
    //Push S1 after null
    //Push null after S1
    //Push null after null
    //Input
    //S1->S2->S3 S4 S5
    @Test
    public void testNegativePushAfter() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[5];
        
        for(int i = 0; i < ops.length; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i));
        }

        for(int i = 0; i < ops.length - 2; ++i) {
            plan.add(ops[i]);
        }

        
        plan.connect(ops[0], ops[1]);
        plan.connect(ops[1], ops[2]);

        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        try {
            plan.pushAfter(ops[0], ops[3], 0);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }
        
        try {
            plan.pushAfter(ops[3], ops[0], 0);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }

        try {
            plan.pushAfter(ops[4], ops[3], 0);
            fail("Expected exception for node not in plan.");
        } catch (PlanException pe) {
            assertTrue(pe.getMessage().contains("not in the plan"));
        }
        
        try {
            plan.pushAfter(ops[0], null, 0);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1085);
        }
        
        try {
            plan.pushAfter(null, ops[0], 0);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1085);
        }
        
        try {
            plan.pushAfter(null, null, 0);
            fail("Expected exception for having null as one of the inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1085);
        }

    }

    //Negative test cases
    //Input
    //  S1   S2  S3  S4
    //   \   |   |  /
    //        M10
    //    S5   |  S6
    //     \   |  /
    //        M11
    //      /  |  \
    //     S7  S8  S9
    @Test
    public void testNegativePushAfter2() throws Exception {
        TPlan plan = new TPlan();
        TOperator[] ops = new TOperator[11];
        
        for(int i = 0; i < ops.length - 2; ++i) {
            ops[i] = new SingleOperator(Integer.toString(i+1));
            plan.add(ops[i]);
        }
        
        ops[9] = new MultiOperator("10");
        plan.add(ops[9]);
        
        ops[10] = new MultiOperator("11");
        plan.add(ops[10]);
        
        plan.connect(ops[0], ops[9]);
        plan.connect(ops[1], ops[9]);
        plan.connect(ops[2], ops[9]);
        plan.connect(ops[3], ops[9]);
        plan.connect(ops[9], ops[10]);
        plan.connect(ops[4], ops[10]);
        plan.connect(ops[5], ops[10]);
        
        plan.connect(ops[10], ops[6]);
        plan.connect(ops[10], ops[7]);
        plan.connect(ops[10], ops[8]);
        
        //PlanPrinter<TOperator, TPlan> planPrinter = new PlanPrinter<TOperator, TPlan>(System.err, plan);
        //planPrinter.visit();

        try {
            plan.pushAfter(ops[6], ops[9], 0) ;
            fail("Expected exception for first operator having null successors");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1086);
        }
        
        try {
            plan.pushAfter(ops[0], ops[9], 0) ;
            fail("Expected exception for first operator having no inputs");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1088);
        }
        
        try {
            plan.pushAfter(ops[9], ops[6], 0) ;
            fail("Expected exception for first operator having one successor");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1086);
        }
        
        try {
            plan.pushAfter(ops[6], ops[10], 0) ;
            fail("Expected exception for first operator having one successors");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1086);
        }

        
        try {
            plan.pushAfter(ops[10], ops[9], 4) ;
            fail("Expected exception for outputNum exceeding the number of outputs of first operator");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1087);
        }

        try {
            plan.pushAfter(ops[10], ops[6], 0) ;
            fail("Expected exception for second operator having null successors");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1088);
        }
        
        try {
            plan.pushAfter(ops[10], ops[10], 0) ;
            fail("Expected exception for second operator having more than one successor");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1088);
        }
        
        try {
            plan.pushAfter(ops[10], ops[0], 0) ;
            fail("Expected exception for second operator not being a predecessor of first operator");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1089);
        }
        
        plan.disconnect(ops[4], ops[10]);
        plan.disconnect(ops[5], ops[10]);
        
        MultiOutputSingleInputOperator mosi = new MultiOutputSingleInputOperator("12");
        
        plan.replace(ops[10], mosi);
        
        try {
            plan.pushAfter(mosi, ops[9], 0) ;
            fail("Expected exception for trying to connect multiple inputs to the first operator");
        } catch (PlanException pe) {
            assertTrue(pe.getErrorCode() == 1091);
        }
        
    }

}


/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOMultiply extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOMultiply.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOMultiply(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.mergeType(getLhsOperand().getType(), getRhsOperand().getType()));
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Multiply " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.plan;

import java.io.IOException;
import java.io.OutputStream;
import java.io.PrintStream;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.pig.PigException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;


//import org.apache.commons.collections.map.MultiValueMap;

/**
 * A generic graphing class for use by LogicalPlan, PhysicalPlan, etc.  One
 * important aspect of this package is that it guarantees that once a graph is
 * constructed, manipulations on that graph will maintain the ordering of
 * inputs and outputs for a given node.  That is, if a node has two inputs, 0
 * and 1, it is guaranteed that everytime it asks for its inputs, it will
 * receive them in the same order.  This allows operators that need to
 * distinguish their inputs (such as binary operators that need to know left
 * from right) to work without needing to store their inputs themselves.  This
 * is an extra burden on the graph package and not in line with the way graphs
 * are generally understood mathematically.  But it greatly reducing the need
 * for graph manipulators (such as the validators and optimizers) to
 * understand the internals of various nodes.
 */
public abstract class OperatorPlan<E extends Operator> implements Iterable<E>, Serializable, Cloneable {
    protected Map<E, OperatorKey> mOps;
    protected Map<OperatorKey, E> mKeys;
    protected MultiMap<E, E> mFromEdges;
    protected MultiMap<E, E> mToEdges;

    private List<E> mRoots;
    private List<E> mLeaves;
    protected static Log log = LogFactory.getLog(OperatorPlan.class);
    
    public OperatorPlan() {
        mRoots = new ArrayList<E>();
        mLeaves = new ArrayList<E>();
        mOps = new HashMap<E, OperatorKey>();
        mKeys = new HashMap<OperatorKey, E>();
        mFromEdges = new MultiMap<E, E>();
        mToEdges = new MultiMap<E, E>();
    }

    /**
     * Get a list of all nodes in the graph that are roots.  A root is defined to
     * be a node that has no input.
     */
    public List<E> getRoots() {
        if (mRoots.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) {
                if (mToEdges.get(op) == null) {
                    mRoots.add(op);
                }
            }
        }
        return mRoots;
    }

    /**
     * Get a list of all nodes in the graph that are leaves.  A leaf is defined to
     * be a node that has no output.
     */
    public List<E> getLeaves() {
        if (mLeaves.size() == 0 && mOps.size() > 0) {
            for (E op : mOps.keySet()) {
                if (mFromEdges.get(op) == null) {
                    mLeaves.add(op);
                }
            }
        }
        return mLeaves;
    }

    /**
     * Given an operator, find its OperatorKey.
     * @param op Logical operator.
     * @return associated OperatorKey
     */
    public OperatorKey getOperatorKey(E op) {
        return mOps.get(op);
    }

    /**
     * Given an OperatorKey, find the associated operator.
     * @param opKey OperatorKey
     * @return associated operator.
     */
    public E getOperator(OperatorKey opKey) {
        return mKeys.get(opKey);
    }

    /**
     * Get the map of operator key and associated operators
     * @return map of operator key and operators.
     */
    public Map<OperatorKey, E> getKeys() {
        return mKeys;
    }

    /**
     * Insert an operator into the plan.  This only inserts it as a node in
     * the graph, it does not connect it to any other operators.  That should
     * be done as a separate step using connect.
     * @param op Operator to add to the plan.
     */
    public void add(E op) {
        markDirty();
        mOps.put(op, op.getOperatorKey());
        mKeys.put(op.getOperatorKey(), op);
    }

    /**
     * Create an edge between two nodes.  The direction of the edge implies data
     * flow.
     * @param from Operator data will flow from.
     * @param to Operator data will flow to.
     * @throws PlanException if this edge will create multiple inputs for an
     * operator that does not support multiple inputs or create multiple outputs
     * for an operator that does not support multiple outputs.
     */
    public void connect(E from, E to) throws PlanException {
        markDirty();

        // Check that both nodes are in the plan.
        checkInPlan(from);
        checkInPlan(to);

        // Check to see if the from operator already has outputs, and if so
        // whether it supports multiple outputs.
        if (mFromEdges.get(from) != null &&
                !from.supportsMultipleOutputs()) {
            PlanException pe = new PlanException("Attempt to give operator of type " +
                from.getClass().getName() + " multiple outputs.  This operator does "
                + "not support multiple outputs.");
            log.error(pe.getMessage());
            throw pe;
        }

        // Check to see if the to operator already has inputs, and if so
        // whether it supports multiple inputs.
        if (mToEdges.get(to) != null &&
                !to.supportsMultipleInputs()) {
            PlanException pe =  new PlanException("Attempt to give operator of type " +
                to.getClass().getName() + " multiple inputs.  This operator does "
                + "not support multiple inputs.");
            log.error(pe.getMessage());
            throw pe;
        }
        mFromEdges.put(from, to);
        mToEdges.put(to, from);
    }

    /**
     * Remove an edge from between two nodes. 
     * Use {@link org.apache.pig.impl.plan.OperatorPlan#insertBetween(Operator, Operator, Operator)} 
     * if disconnect is used in the process of inserting a new node between two nodes 
     * by calling disconnect followed by a connect.
     * @param from Operator data would flow from.
     * @param to Operator data would flow to.
     * @return true if the nodes were connected according to the specified data
     * flow, false otherwise.
     */
    public boolean disconnect(E from, E to) {
        markDirty();
        
        boolean sawNull = false;
        if (mFromEdges.remove(from, to) == null) sawNull = true;
        if (mToEdges.remove(to, from) == null) sawNull = true;

        return !sawNull;
    }

    /**
     * Remove an operator from the plan.  Any edges that the node has will
     * be removed as well.
     * @param op Operator to remove.
     */
    public void remove(E op) {
        markDirty();

        removeEdges(op, mFromEdges, mToEdges);
        removeEdges(op, mToEdges, mFromEdges);

        // Remove the operator from nodes
        mOps.remove(op);
        mKeys.remove(op.getOperatorKey());
    }

    /**
     * Trim everything below a given operator.  The specified operator will
     * NOT be removed.
     * @param op Operator to trim everything after.
     */
    public void trimBelow(E op) {
        trimBelow(getSuccessors(op));
    }

    private void trimBelow(List<E> ops) {
        if (ops != null) {
            // Make a copy because we'll be messing with the underlying list.
            List<E> copy = new ArrayList<E>(ops);
            for (E op : copy) {
                trimBelow(getSuccessors(op));
                remove(op);
            }
        }
    }

    /**
     * Trim everything above a given operator.  The specified operator will
     * NOT be removed.
     * @param op Operator to trim everything before.
     */
    public void trimAbove(E op) {
        trimAbove(getPredecessors(op));
    }

    private void trimAbove(List<E> ops) {
        if (ops != null) {
            // Make a copy because we'll be messing with the underlying list.
            List<E> copy = new ArrayList<E>(ops);
            for (E op : copy) {
                trimAbove(getPredecessors(op));
                remove(op);
            }
        }
    }


    /**
     * Find all of the nodes that have edges to the indicated node from
     * themselves.
     * @param op Node to look to
     * @return Collection of nodes.
     */
    public List<E> getPredecessors(E op) {
        return (List<E>)mToEdges.get(op);
    }


    /**
     * Find all of the nodes that have edges from the indicated node to
     * themselves.
     * @param op Node to look from
     * @return Collection of nodes.
     */
    public List<E> getSuccessors(E op) {
        return (List<E>)mFromEdges.get(op);
    }

    public Iterator<E> iterator() { 
        return mOps.keySet().iterator();
    }

    private void markDirty() {
        mRoots.clear();
        mLeaves.clear();
    }

    private void removeEdges(E op,
                             MultiMap<E, E> fromMap,
                             MultiMap<E, E> toMap) {
        // Find all of the from edges, as I have to remove all the associated to
        // edges.  Need to make a copy so we can delete from the map without
        // screwing up our iterator.
        Collection c = fromMap.get(op);
        if (c == null) return;

        ArrayList al = new ArrayList(c);
        Iterator i = al.iterator();
        while (i.hasNext()) {
            E to = (E)i.next();
            toMap.remove(to, op);
            fromMap.remove(op, to);
        }
    }

    private void checkInPlan(E op) throws PlanException {
        if (mOps.get(op) == null) {
            PlanException pe = new PlanException("Attempt to connect operator " +
                op.name() + " which is not in the plan.");
            log.error(pe.getMessage());
            throw pe;
        }
    }
    
    /**
     * Merges the operators in the incoming operPlan with
     * this plan's operators. By merging I mean just making
     * a combined graph with each one as a component
     * It doesn't support merging of shared plans
     * @param inpPlan
     * @return this pointer
     * @throws PlanException
     */
    public OperatorPlan<E> merge(OperatorPlan<E> inpPlan) throws PlanException {
       return doMerge(inpPlan, false);
    }

    /**
     * Merges the operators in the incoming plan with this plan's operators.
     * The plans can have shared components. 
     *
     * @param inpPlan
     * @return this pointer
     * @throws PlanException
     */
    public OperatorPlan<E> mergeSharedPlan(OperatorPlan<E> inpPlan) throws PlanException {
        return doMerge(inpPlan, true);
    }

    private OperatorPlan<E> doMerge(OperatorPlan<E> inpPlan, boolean allowSharedPlan) throws PlanException {
        Map<E, OperatorKey> inpOps = inpPlan.mOps;
        Set<E> curOpsKeySet = mOps.keySet();
        for (Map.Entry<E, OperatorKey> mapEnt : inpOps.entrySet()) {
            if (curOpsKeySet.contains(mapEnt.getKey())) {
                if (!allowSharedPlan) {
                    PlanException pe = new PlanException(
                        "There are operators that are shared across the plans. Merge of "
                            + "mutually exclusive plans is the only supported merge.");
                    log.error(pe.getMessage());
                    throw pe;
                }
            } else {
                mOps.put(mapEnt.getKey(), mapEnt.getValue());
            }
        }

        Map<OperatorKey, E> inpKeys = inpPlan.mKeys;
        Set<OperatorKey> curOKKeySet = mKeys.keySet();
        for (Map.Entry<OperatorKey, E> mapEnt : inpKeys.entrySet()) {
            if (curOKKeySet.contains(mapEnt.getKey())) {
                if (!allowSharedPlan) {
                    PlanException pe = new PlanException(
                        "There are operators that are shared across the plans. Merge of "
                            + "mutually exclusive plans is the only supported merge.");
                    log.error(pe.getMessage());
                    throw pe;
                }
            } else {
                mKeys.put(mapEnt.getKey(), mapEnt.getValue());
            }
        }

        MultiMap<E, E> inpFromEdges = inpPlan.mFromEdges;
        Set<E> curFEKeySet = mFromEdges.keySet();
        for (E fromEdg : inpFromEdges.keySet()) {
            if (curFEKeySet.contains(fromEdg) && !allowSharedPlan) {
            	PlanException pe = new PlanException(
                        "There are operators that are shared across the plans. Merge of "
                            + "mutually exclusive plans is the only supported merge.");
                log.error(pe.getMessage());
                throw pe;
            }
            
            for (E e : inpFromEdges.get(fromEdg)) {
                if (mFromEdges.get(fromEdg) == null || !mFromEdges.get(fromEdg).contains(e)) {
                    mFromEdges.put(fromEdg, e);
                }
            }
        }

        MultiMap<E, E> inpToEdges = inpPlan.mToEdges;
        Set<E> curTEKeySet = mToEdges.keySet();
        for (E toEdg : inpToEdges.keySet()) {
            if (curTEKeySet.contains(toEdg) && !allowSharedPlan) {  
                PlanException pe = new PlanException(
                    "There are operators that are shared across the plans. Merge of "
                        + "mutually exclusive plans is the only supported merge.");
                log.error(pe.getMessage());
                throw pe;                
            }
            
            for (E e : inpToEdges.get(toEdg)) {
                if (mToEdges.get(toEdg) == null || !mToEdges.get(toEdg).contains(e)) {
                    mToEdges.put(toEdg, e);
                }
            }
        }

        markDirty();
        return this;
    }


    /**
     * Utility method heavily used in the MRCompiler
     * Adds the leaf operator to the plan and connects
     * all existing leaves to the new leaf
     * @param leaf
     * @throws PlanException 
     */
    public void addAsLeaf(E leaf) throws PlanException {
        List<E> ret = new ArrayList<E>();
        for (E operator : getLeaves()) {
            ret.add(operator);
        }
        add(leaf);
        for (E oper : ret) {
            connect(oper, leaf);
        }
    }
    
    public boolean isSingleLeafPlan() {
        List<E> tmpList = getLeaves() ;
        return tmpList.size() == 1 ;
    }

    public int size() {
        return mKeys.size() ;
    }

    /**
     * Given two connected nodes add another node between them.
     * 'newNode' will be placed in same position in predecessor list as 'before' (old node).
     * @param after Node to insert this node after
     * @param newNode new node to insert.  This node must have already been
     * added to the plan.
     * @param before Node to insert this node before
     * @throws PlanException if it encounters trouble disconnecting or
     * connecting nodes.
     */
    public void insertBetween(
            E after,
            E newNode,
            E before) throws PlanException {
        checkInPlan(newNode);
        if (!replaceNode(after, newNode, before, mFromEdges) || !replaceNode(before, newNode, after, mToEdges)) {
            PlanException pe = new PlanException("Attempt to insert between two nodes " +
                "that were not connected.");
            log.error(pe.getMessage());
            throw pe;
        }
        mFromEdges.put(newNode, before);
        mToEdges.put(newNode, after);
    }

    // replaces (src -> dst) entry in multiMap with (src -> replacement)
    private boolean replaceNode(E src, E replacement, E dst, MultiMap<E, E> multiMap) {
        if(multiMap == null) return false;
        
        if(src == null) return false;
      
        List<E> nodes = (ArrayList<E>)multiMap.get(src);
        if (nodes == null) {
            //we need to add replacement to the multimap as long as replacement != null
            if(replacement == null) {
                return false;
            } else if (dst == null) {
                ArrayList<E> replacementNodes = new ArrayList<E>();
                replacementNodes.add(replacement);
                multiMap.put(src, replacementNodes);
                return true;
            } else {
                return false;
            }
        }
        
        if(dst == null) return false;
        
        boolean replaced = false;
        ArrayList<E> replacementNodes = new ArrayList<E>();
        for(int i = 0; i < nodes.size(); ++i) {
            E to = nodes.get(i);
            if(to.equals(dst)) {
                replaced = true;
                if(replacement != null) {
                    replacementNodes.add(replacement);
                }
            } else {
                replacementNodes.add(to);
            }
        }
        
        if(replaced) {
            multiMap.removeKey(src);
            if(replacementNodes.size() > 0) {
                multiMap.put(src, replacementNodes);
            }
        }
        return replaced;
    }

    /**
     * Replace an existing node in the graph with a new node.  The new node
     * will be connected to all the nodes the old node was.  The old node will
     * be removed.
     * @param oldNode Node to be replaced
     * @param newNode Node to add in place of oldNode
     * @throws PlanException
     */
    public void replace(E oldNode, E newNode) throws PlanException {
        checkInPlan(oldNode);
        add(newNode);
        mToEdges = generateNewMap(oldNode, newNode, mToEdges);
        mFromEdges = generateNewMap(oldNode, newNode, mFromEdges);
        remove(oldNode);
    }

    private MultiMap<E, E> generateNewMap(
            E oldNode,
            E newNode,
            MultiMap<E, E> mm) {
        // First, replace the key
        Collection<E> targets = mm.get(oldNode);
        if (targets != null) {
            mm.removeKey(oldNode);
            mm.put(newNode, targets);
        }

        // We can't just do a remove and add in the map because of our
        // guarantee of not changing orders.  So we need to walk the lists and
        // put the new node in the same slot as the old.

        // Walk all the other keys and replace any references to the oldNode
        // in their targets.
        MultiMap<E, E> newMap = new MultiMap<E, E>(mm.size());
        for (E key : mm.keySet()) {
            Collection<E> c = mm.get(key);
            ArrayList<E> al = new ArrayList<E>(c);
            for (int i = 0; i < al.size(); i++) {
                if (al.get(i) == oldNode) al.set(i, newNode);
            }
            newMap.put(key, al);
        }
        return newMap;
    }

    /**
     * Remove a node in a way that connects the node's predecessor (if any)
     * with the node's successor (if any).  This function does not handle the
     * case where the node has multiple predecessors or successors.
     * @param node Node to be removed
     * @throws PlanException if the node has more than one predecessor or
     * successor.
     */
    public void removeAndReconnect(E node) throws PlanException {
        List<E> preds = getPredecessors(node);
        E pred = null;
        if (preds != null) {
            if (preds.size() > 1) {
                PlanException pe = new PlanException("Attempt to remove " +
                    " and reconnect for node with multiple predecessors.");
                log.error(pe.getMessage());
                throw pe;
            }
            pred = preds.get(0);
            disconnect(pred, node);
        }

        List<E> succs = getSuccessors(node);
        E succ = null;
        if (succs != null) {
            if (succs.size() > 1) {
                PlanException pe = new PlanException("Attempt to remove " +
                    " and reconnect for node with multiple successors.");
                log.error(pe.getMessage());
                throw pe;
            }
            succ = succs.get(0);
            disconnect(node, succ);
        }

        remove(node);
        if (pred != null && succ != null) connect(pred, succ);
    }

    private void reconnectSuccessors(E node, boolean successorRequired, boolean removeNode) throws PlanException {
        // Before:
        //    A (predecessor (only one) )
        //  / |
        // X  B(nodeB)  Y(some predecessor of a Cn)
        //  / | \     / 
        // C1 C2  C3 ... (Successors)
        // should become
        // After:
        //    ___ A     Y
        //   /  / | \  /
        //  X  C1 C2 C3 ...
        // the variable names are from above example

        E nodeB = node;
        List<E> preds = getPredecessors(nodeB);
        //checking pre-requisite conditions
        if (preds == null || preds.size() != 1) {
            Integer size = null;
            if(preds != null)
                size = preds.size();

            PlanException pe = new PlanException("Attempt to remove " +
                    " and reconnect for node with  " + size +
            " predecessors.");
            log.error(pe.getMessage());
            throw pe;
        }

        //A and C
        E nodeA = preds.get(0);
        Collection<E> nodeC = mFromEdges.get(nodeB);

        //checking pre-requisite conditions
        if(successorRequired) {
            if (nodeC == null || nodeC.size() == 0) {
                PlanException pe = new PlanException("Attempt to remove " +
                " and reconnect for node with no successors.");
                log.error(pe.getMessage());
                throw pe;
            }
        }


        // replace B in A.succesors and add B.successors(ie C) to it
        replaceAndAddSucessors(nodeA, nodeB);
        
        // for all C(succs) , replace B(node) in predecessors, with A(pred)
        if(nodeC != null) {
            for(E c: nodeC) {
                Collection<E> sPreds = mToEdges.get(c);
                ArrayList<E> newPreds = new ArrayList<E>(sPreds.size());
                for(E p: sPreds){
                    if(p == nodeB){
                        //replace
                        newPreds.add(nodeA);
                    }
                    else{
                        newPreds.add(p);
                    }
                }
                mToEdges.removeKey(c);
                mToEdges.put(c,newPreds);
                
            }
        }
        
        if(removeNode) {
            remove(nodeB);
        } else {
            //make sure that the node does not have any dangling from and to edges
            mFromEdges.removeKey(nodeB);
            mToEdges.removeKey(nodeB);
        }
    }
    
    private void reconnectPredecessors(E node, boolean predecessorRequired, boolean removeNode) throws PlanException {
        // Before:
        // C1 C2  C3 ... (Predecessors)
        //  \ |  /    \
        // X  B(nodeB)  Y(some successor of a Cn)
        //  \ |
        //    A (successor (only one) )
 

        // should become
        // After:
        //  X  C1 C2 C3 ...
        //   \  \ | /  \
        //        A     Y
        // the variable names are from above example

        E nodeB = node;
        List<E> nodeBsuccessors = getSuccessors(nodeB);
        //checking pre-requisite conditions
        if (nodeBsuccessors == null || nodeBsuccessors.size() != 1) {
            Integer size = null;
            if(nodeBsuccessors != null)
                size = nodeBsuccessors.size();

            PlanException pe = new PlanException("Attempt to remove " +
                    " and reconnect for node with  " + size +
            " successors.");
            log.error(pe.getMessage());
            throw pe;
        }

        //A and C
        E nodeA = nodeBsuccessors.get(0);
        Collection<E> nodeC = mToEdges.get(nodeB);

        //checking pre-requisite conditions
        if(predecessorRequired) {
            if (nodeC == null || nodeC.size() == 0) {
                PlanException pe = new PlanException("Attempt to remove " +
                " and reconnect for node with no predecessors.");
                log.error(pe.getMessage());
                throw pe;
            }
        }


        // replace B in A.predecessors and add B.predecessors(ie C) to it
        replaceAndAddPredecessors(nodeA, nodeB);
        
        // for all C(predecessors) , replace B(node) in successors, with A(successor)
        if(nodeC != null) {
            for(E c: nodeC) {
                Collection<E> sPreds = mFromEdges.get(c);
                ArrayList<E> newPreds = new ArrayList<E>(sPreds.size());
                for(E p: sPreds){
                    if(p == nodeB){
                        //replace
                        newPreds.add(nodeA);
                    }
                    else{
                        newPreds.add(p);
                    }
                }
                mFromEdges.removeKey(c);
                mFromEdges.put(c,newPreds);
                
            }
        }
        
        if(removeNode) {
            remove(nodeB);
        } else {
            //make sure that the node does not have any dangling from and to edges
            mFromEdges.removeKey(nodeB);
            mToEdges.removeKey(nodeB);
        }
    }
    
    // removes entry for successor in list of successors of node
    // and adds successors of successor in its place
    // @param noded - parent node whose entry for successor needs to be replaced
    // @param successor - see above
    private void replaceAndAddSucessors(E node, E successor) throws PlanException {
       Collection<E> oldSuccessors = mFromEdges.get(node);
       Collection<E> replacementSuccessors = mFromEdges.get(successor);
       ArrayList<E> newSuccessors = new ArrayList<E>();
       for(E s: oldSuccessors){
           if(s == successor){
               if(replacementSuccessors != null) {
                   newSuccessors.addAll(replacementSuccessors);
               }
           }else{
               newSuccessors.add(s);
           }
       }
       mFromEdges.removeKey(node);
       mFromEdges.put(node,newSuccessors);
    }    

    // removes entry  for predecessor in list of predecessors of node, 
    // and adds predecessors of predecessor in its place
    // @param node - parent node whose entry for predecessor needs to be replaced
    // @param predecessor - see above
    private void replaceAndAddPredecessors(E node, E predecessor) throws PlanException {
       Collection<E> oldPredecessors = mToEdges.get(node);
       Collection<E> replacementPredecessors = mToEdges.get(predecessor);
       ArrayList<E> newPredecessors = new ArrayList<E>();
       for(E p: oldPredecessors){
           if(p == predecessor){
               if(replacementPredecessors != null) {
                   newPredecessors.addAll(replacementPredecessors);
               }
           }else{
               newPredecessors.add(p);
           }
       }
       mToEdges.removeKey(node);
       mToEdges.put(node,newPredecessors);
    }
    
    /**
     * Remove a node in a way that connects the node's predecessor (if any)
     * with the node's successors (if any).  This function handles the
     * case where the node has *one* predecessor and one or more successors.
     * It replaces the predecessor in same position as node was in
     * each of the successors predecessor list(getPredecessors()), to 
     * preserve input ordering 
     * for eg, it is used to remove redundant project(*) from plan
     * which will have only one predecessor,but can have multiple success
     * @param node Node to be removed
     * @throws PlanException if the node has more than one predecessor
     */
    public void removeAndReconnectMultiSucc(E node) throws PlanException {
        reconnectSuccessors(node, true, true);
    }
    

    
    public void dump(PrintStream ps) {
        ps.println("Ops");
        for (E op : mOps.keySet()) {
            ps.println(op.name());
        }
        ps.println("from edges");
        for (E op : mFromEdges.keySet()) {
            for (E to : mFromEdges.get(op)) {
                ps.println(op.name() + " -> " + to.name());
            }
        }
        ps.println("to edges");
        for (E op : mToEdges.keySet()) {
            for (E to : mToEdges.get(op)) {
                ps.println(op.name() + " -> " + to.name());
            }
        }
    }
    
    public void explain(
            OutputStream out,
            PrintStream ps) throws VisitorException, IOException {
        PlanPrinter pp = new PlanPrinter(ps, this);
        pp.print(out);
    }

    /**
     * Swap two operators in a plan.  Both of the operators must have single
     * inputs and single outputs.
     * @param first operator
     * @param second operator
     * @throws PlanException if either operator is not single input and output.
     */
    public void swap(E first, E second) throws PlanException {
        E firstNode = first;
        E secondNode = second;
        
        if(firstNode == null) {
            int errCode = 1092;
            String msg = "First operator in swap is null. Cannot swap null operators.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        if(secondNode == null) {
            int errCode = 1092;
            String msg = "Second operator in swap is null. Cannot swap null operators.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        checkInPlan(firstNode);
        checkInPlan(secondNode);
        
        List<E> firstNodePredecessors = (ArrayList<E>)mToEdges.get(firstNode);
        
        if(firstNodePredecessors != null && firstNodePredecessors.size() > 1) {
            int errCode = 1093;
            String msg = "Swap supports swap of operators with at most one input."
                            + " Found first operator with " + firstNodePredecessors.size() + " inputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> firstNodeSuccessors = (ArrayList<E>)mFromEdges.get(firstNode);
        
        if(firstNodeSuccessors != null && firstNodeSuccessors.size() > 1) {
            int errCode = 1093;
            String msg = "Swap supports swap of operators with at most one output."
                + " Found first operator with " + firstNodeSuccessors.size() + " outputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> secondNodePredecessors = (ArrayList<E>)mToEdges.get(secondNode);
        
        if(secondNodePredecessors != null && secondNodePredecessors.size() > 1) {
            int errCode = 1093;
            String msg = "Swap supports swap of operators with at most one input."
                + " Found second operator with " + secondNodePredecessors.size() + " inputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> secondNodeSuccessors = (ArrayList<E>)mFromEdges.get(secondNode);
        
        if(secondNodeSuccessors != null && secondNodeSuccessors.size() > 1) {
            int errCode = 1093;
            String msg = "Swap supports swap of operators with at most one output."
                + " Found second operator with " + secondNodeSuccessors.size() + " outputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        E firstNodePredecessor = null;
        E firstNodeSuccessor = null;
        E secondNodePredecessor = null;
        E secondNodeSuccessor = null;
        
        if(firstNodePredecessors != null) {
            firstNodePredecessor = firstNodePredecessors.get(0);
        }
        
        if(firstNodeSuccessors != null) {
            firstNodeSuccessor = firstNodeSuccessors.get(0);
        }

        if(secondNodePredecessors != null) {
            secondNodePredecessor = secondNodePredecessors.get(0);
        }
        
        if(secondNodeSuccessors != null) {
            secondNodeSuccessor = secondNodeSuccessors.get(0);
        }
        
        boolean immediateNodes = false;
        
        if((firstNodeSuccessor == secondNode) && (secondNodePredecessor == firstNode)) {
            immediateNodes = true;
        } else if ((secondNodeSuccessor == firstNode) && (firstNodePredecessor == secondNode)) {
            immediateNodes = true;
            //swap the firstNode and secondNode
            E tmpNode = firstNode;
            firstNode = secondNode;
            secondNode = tmpNode;
            
            //swap the predecessor and successor nodes
            tmpNode = firstNodePredecessor;
            firstNodePredecessor = secondNodePredecessor;
            secondNodePredecessor = tmpNode;
            
            tmpNode = firstNodeSuccessor;
            firstNodeSuccessor = secondNodeSuccessor;
            secondNodeSuccessor = tmpNode;
        }

        if(immediateNodes) {
            //Replace the predecessors and successors of first and second in their respective edge lists       
            replaceNode(firstNode, secondNodeSuccessor, firstNodeSuccessor, mFromEdges);
            replaceNode(firstNode, secondNode, firstNodePredecessor, mToEdges);
            replaceNode(secondNode, firstNode, secondNodeSuccessor, mFromEdges);
            replaceNode(secondNode, firstNodePredecessor, secondNodePredecessor, mToEdges);
        } else {
            //Replace the predecessors and successors of first and second in their respective edge lists       
            replaceNode(firstNode, secondNodeSuccessor, firstNodeSuccessor, mFromEdges);
            replaceNode(firstNode, secondNodePredecessor, firstNodePredecessor, mToEdges);
            replaceNode(secondNode, firstNodeSuccessor, secondNodeSuccessor, mFromEdges);
            replaceNode(secondNode, firstNodePredecessor, secondNodePredecessor, mToEdges);
        }

        //Replace first with second in the edges list for first's predecessor and successor        
        replaceNode(firstNodePredecessor, secondNode, firstNode, mFromEdges);
        replaceNode(firstNodeSuccessor, secondNode, firstNode, mToEdges);
        
        //Replace second with first in the edges list for second's predecessor and successor
        replaceNode(secondNodePredecessor, firstNode, secondNode, mFromEdges);
        replaceNode(secondNodeSuccessor, firstNode, secondNode, mToEdges);
        
        markDirty();
    }

    /**
     * Push one operator in front of another.  This function is for use when
     * the first operator has multiple inputs.  The caller can specify
     * which input of the first operator the second operator should be pushed to.
     * @param first operator, assumed to have multiple inputs.
     * @param second operator, will be pushed in front of first
     * @param inputNum indicates which input of the first operator the second
     * operator will be pushed onto.  Numbered from 0.
     * @throws PlanException if inputNum does not exist for first operator
     */
    public void pushBefore(E first, E second, int inputNum) throws PlanException {
        E firstNode = first;
        E secondNode = second;
        
        if(firstNode == null) {
            int errCode = 1085;
            String msg = "First operator in pushBefore is null. Cannot pushBefore null operators.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        if(secondNode == null) {
            int errCode = 1085;
            String msg = "Second operator in pushBefore is null. Cannot pushBefore null operators.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        checkInPlan(firstNode);
        checkInPlan(secondNode);
        
        List<E> firstNodePredecessors = (ArrayList<E>)mToEdges.get(firstNode);
        
        if(firstNodePredecessors == null || firstNodePredecessors.size() <= 1) {
            int size = (firstNodePredecessors == null ? 0 : firstNodePredecessors.size());
            int errCode = 1086;
            String msg = "First operator in pushBefore should have multiple inputs."
                            + " Found first operator with " + size + " inputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        if(inputNum >= firstNodePredecessors.size()) {
            int errCode = 1087;
            String msg = "The inputNum " + inputNum + " should be lesser than the number of inputs of the first operator."
                            + " Found first operator with " + firstNodePredecessors.size() + " inputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> firstNodeSuccessors = (ArrayList<E>)mFromEdges.get(firstNode);
        
        if(firstNodeSuccessors == null) {
            int errCode = 1088;
            String msg = "First operator in pushBefore should have at least one output."
                + " Found first operator with no outputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> secondNodePredecessors = (ArrayList<E>)mToEdges.get(secondNode);
        
        if(secondNodePredecessors == null || secondNodePredecessors.size() > 1) {
            int size = (secondNodePredecessors == null ? 0 : secondNodePredecessors.size());
            int errCode = 1088;
            String msg = "Second operator in pushBefore should have one input."
                + " Found second operator with " + size + " inputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> secondNodeSuccessors = (ArrayList<E>)mFromEdges.get(secondNode);
        
        //check for multiple edges from first to second
        int edgesFromFirstToSecond = 0;
        for(E node: firstNodeSuccessors) {
            if(node == secondNode) {
                ++edgesFromFirstToSecond;
            }
        }
        
        if(edgesFromFirstToSecond == 0) {
            int errCode = 1089;
            String msg = "Second operator in pushBefore should be the successor of the First operator.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        } else if (edgesFromFirstToSecond > 1) {
            int errCode = 1090;
            String msg = "Second operator can have at most one incoming edge from First operator."
                + " Found " + edgesFromFirstToSecond + " edges.";
            throw new PlanException(msg, errCode, PigException.INPUT);            
        }
        
        //check if E (i.e., firstNode) can support multiple outputs before we short-circuit
        
        if(!firstNode.supportsMultipleOutputs()) {
            int numSecondNodeSuccessors = (secondNodeSuccessors == null? 0 : secondNodeSuccessors.size());
            if((firstNodeSuccessors.size() > 0) || (numSecondNodeSuccessors > 0)) {
                int errCode = 1091;
                String msg = "First operator does not support multiple outputs."
                    + " On completing the pushBefore operation First operator will end up with "
                    + (firstNodeSuccessors.size() + numSecondNodeSuccessors) + " edges.";
                throw new PlanException(msg, errCode, PigException.INPUT);
            }
        }
        
        //Assume that we have a graph which is like
        //   A   B   C   D
        //   \   |   |  /
        //         E
        //      /  |  \
        //     F   G   H
        //      /  |  \
        //     I   J   K
        //
        //Now pushBefore(E, G, 1)
        //This can be done using the following sequence of transformations
        //1. Promote G's successors as E's successors using reconnectSuccessors(G)
        //2. Insert G between B and E using insertBetween(B, G, E)
        //The graphs after each step
        //Step 1 - Note that G is standing alone
        //   A   B   C   D   G
        //   \   |   |  /
        //         E
        //   /  /  |  \  \
        //  F  I   J   K  H  
        //Step 2
        //       B
        //       |
        //   A   G   C   D
        //   \   |   |  /
        //         E
        //   /  /  |  \  \
        //  F  I   J   K  H           
        
        reconnectSuccessors(secondNode, false, false);
        insertBetween(firstNodePredecessors.get(inputNum), secondNode, firstNode);

        markDirty();
        return;
    }

    /**
     * Push one operator after another.  This function is for use when the second
     * operator has multiple outputs.  The caller can specify which output of the
     * second operator the first operator should be pushed to.
     * @param first operator, assumed to have multiple outputs
     * @param second operator, will be pushed after the first operator
     * @param outputNum indicates which output of the first operator the second 
     * operator will be pushed onto.  Numbered from 0.
     * @throws PlanException if outputNum does not exist for first operator
     */
    public void pushAfter(E first, E second, int outputNum) throws PlanException {
        E firstNode = first;
        E secondNode = second;
        
        if(firstNode == null) {
            int errCode = 1085;
            String msg = "First operator in pushAfter is null. Cannot pushBefore null operators.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        if(secondNode == null) {
            int errCode = 1085;
            String msg = "Second operator in pushAfter is null. Cannot pushBefore null operators.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        checkInPlan(firstNode);
        checkInPlan(secondNode);
        
        List<E> firstNodePredecessors = (ArrayList<E>)mToEdges.get(firstNode);

        if(firstNodePredecessors == null) {
            int errCode = 1088;
            String msg = "First operator in pushAfter should have at least one input."
                + " Found first operator with no inputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }        

        List<E> firstNodeSuccessors = (ArrayList<E>)mFromEdges.get(firstNode);
        
        if(firstNodeSuccessors == null || firstNodeSuccessors.size() <= 1) {
            int size = (firstNodeSuccessors == null ? 0 : firstNodeSuccessors.size());
            int errCode = 1086;
            String msg = "First operator in pushAfter should have multiple outputs."
                            + " Found first operator with " + size + " outputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        if(outputNum >= firstNodeSuccessors.size()) {
            int errCode = 1087;
            String msg = "The outputNum " + outputNum + " should be lesser than the number of outputs of the first operator."
                            + " Found first operator with " + firstNodeSuccessors.size() + " outputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        
        List<E> secondNodePredecessors = (ArrayList<E>)mToEdges.get(secondNode);
        
        List<E> secondNodeSuccessors = (ArrayList<E>)mFromEdges.get(secondNode);

        if(secondNodeSuccessors == null || secondNodeSuccessors.size() > 1) {
            int size = (secondNodeSuccessors == null ? 0 : secondNodeSuccessors.size());
            int errCode = 1088;
            String msg = "Second operator in pushAfter should have one output."
                + " Found second operator with " + size + " outputs.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        }
        

        //check for multiple edges from second to first
        int edgesFromSecondToFirst = 0;
        for(E node: secondNodeSuccessors) {
            if(node == firstNode) {
                ++edgesFromSecondToFirst;
            }
        }
        
        if(edgesFromSecondToFirst == 0) {
            int errCode = 1089;
            String msg = "Second operator in pushAfter should be the predecessor of the First operator.";
            throw new PlanException(msg, errCode, PigException.INPUT);
        } else if (edgesFromSecondToFirst > 1) {
            int errCode = 1090;
            String msg = "Second operator can have at most one outgoing edge from First operator."
                + " Found " + edgesFromSecondToFirst + " edges.";
            throw new PlanException(msg, errCode, PigException.INPUT);            
        }
        
        //check if E (i.e., firstNode) can support multiple outputs before we short-circuit
        
        if(!firstNode.supportsMultipleInputs()) {
            int numSecondNodePredecessors = (secondNodePredecessors == null? 0 : secondNodePredecessors.size());
            if((firstNodePredecessors.size() > 0) || (numSecondNodePredecessors > 0)) {
                int errCode = 1091;
                String msg = "First operator does not support multiple inputs."
                    + " On completing the pushAfter operation First operator will end up with "
                    + (firstNodePredecessors.size() + numSecondNodePredecessors) + " edges.";
                throw new PlanException(msg, errCode, PigException.INPUT);
            }
        }
        
        //Assume that we have a graph which is like
        //   A   B   C   D
        //   \   |   |  /
        //         E
        //         |
        //         G
        //      /  |  \
        //     I   J   K
        //
        //Now pushAfter(G, E, 1)
        //This can be done using the following sequence of transformations
        //1. Promote E's predecessors as G's predecessors using reconnectPredecessors(E)
        //2. Insert E between G and J using insertBetween(G, E, J)
        //The graphs after each step
        //Step 1 - Note that E is standing alone
        //   A   B   C   D   E
        //   \   |   |  /
        //         G
        //      /  |  \
        //     I   J   K  
        //Step 2
        //   A   B   C   D 
        //   \   |   |  /
        //         G
        //      /  |  \
        //     I   E   K
        //         |
        //         J
        
        reconnectPredecessors(secondNode, false, false);
        insertBetween(firstNode, secondNode, firstNodeSuccessors.get(outputNum));
        
        markDirty();
        return;

    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import static org.apache.pig.ExecType.MAPREDUCE;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Random;

import org.junit.Before;
import org.junit.Test;

import org.apache.pig.EvalFunc;
import org.apache.pig.ExecType;
import org.apache.pig.PigServer;
import org.apache.pig.data.*;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.test.utils.Identity;
import org.apache.pig.builtin.BinStorage;

import junit.framework.TestCase;

public class TestEvalPipeline2 extends TestCase {
    
    MiniCluster cluster = MiniCluster.buildCluster();
    private PigServer pigServer;

    TupleFactory mTf = TupleFactory.getInstance();
    BagFactory mBf = BagFactory.getInstance();
    
    @Before
    @Override
    public void setUp() throws Exception{
        FileLocalizer.setR(new Random());
        pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
//        pigServer = new PigServer(ExecType.LOCAL);
    }
    
    
    @Test
    public void testUdfInputOrder() throws IOException {
        String[] input = {
                "(123)",
                "((123)",
                "(123123123123)",
                "(asdf)"
        };
        
        Util.createInputFile(cluster, "table_udfInp", input);
        pigServer.registerQuery("a = load 'table_udfInp' as (i:int);");
        pigServer.registerQuery("b = foreach a {dec = 'hello'; str1 = " +  Identity.class.getName() + 
                    "(dec,'abc','def');" + 
                    "generate dec,str1; };");
        Iterator<Tuple> it = pigServer.openIterator("b");
        
        Tuple tup=null;

        //tuple 1 
        tup = it.next();
        Tuple out = (Tuple)tup.get(1);

        assertEquals( out.get(0).toString(), "hello");
        assertEquals(out.get(1).toString(), "abc");
        assertEquals(out.get(2).toString(), "def");
        
        Util.deleteFile(cluster, "table_udfInp");
    }
 

    @Test
    public void testUDFwithStarInput() throws Exception {
        int LOOP_COUNT = 10;
        File tmpFile = File.createTempFile("test", "txt");
        PrintStream ps = new PrintStream(new FileOutputStream(tmpFile));
        Random r = new Random();
        for(int i = 0; i < LOOP_COUNT; i++) {
            for(int j=0;j<LOOP_COUNT;j+=2){
                ps.println(i+"\t"+j);
                ps.println(i+"\t"+j);
            }
        }
        ps.close();

        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "';");
        pigServer.registerQuery("B = group A by $0;");
        String query = "C = foreach B {"
        + "generate " + Identity.class.getName() +"(*);"
        + "};";

        pigServer.registerQuery(query);
        Iterator<Tuple> iter = pigServer.openIterator("C");
        if(!iter.hasNext()) fail("No output found");
        int numIdentity = 0;
        while(iter.hasNext()){
            Tuple tuple = iter.next();
            Tuple t = (Tuple)tuple.get(0);
            assertEquals(DataByteArray.class, t.get(0).getClass());
            int group = Integer.parseInt(new String(((DataByteArray)t.get(0)).get()));
            assertEquals(numIdentity, group);
            assertTrue(t.get(1) instanceof DataBag);
            DataBag bag = (DataBag)t.get(1);
            assertEquals(10, bag.size());
            assertEquals(2, t.size());
            ++numIdentity;
        }
        assertEquals(LOOP_COUNT, numIdentity);

    }
    @Test
    public void testBinStorageByteArrayCastsSimple() throws IOException {
        // Test for PIG-544 fix
        // Tries to read data in BinStorage bytearrays as other pig types,
        // should return null if the conversion fails.
        // This test case does not use a practical example , it just tests
        // if the conversion happens when minimum conditions for conversion
        // such as expected number of bytes are met.
        String[] input = {
                    "asdf\t12\t1.1\t231\t234", 
                    "sa\t1231\t123.4\t12345678\t1234.567",
                    "asdff\t1232123\t1.45345\t123456789\t123456789.9"
                    };
        
        Util.createInputFile(cluster, "table_bs_ac", input);

        // test with BinStorage
        pigServer.registerQuery("a = load 'table_bs_ac';");
        String output = "/pig/out/TestEvalPipeline2_BinStorageByteArrayCasts";
        pigServer.deleteFile(output);
        pigServer.store("a", output, BinStorage.class.getName());

        pigServer.registerQuery("b = load '" + output + "' using BinStorage() "
                + "as (name: int, age: int, gpa: float, lage: long, dgpa: double);");
        
        Iterator<Tuple> it = pigServer.openIterator("b");
        
        Tuple tup=null;
        
        // I have separately verified only few of the successful conversions,
        // assuming the rest are correct.
        // It is primarily testing if null is being returned when conversions
        // are expected to fail
        
        //tuple 1 
        tup = it.next();

        
        //1634952294 is integer whose  binary represtation is same as that of "asdf"
        // other columns are returning null because they have less than num of bytes
        //expected for the corresponding numeric type's binary respresentation.
        assertTrue( (Integer)tup.get(0) == 1634952294); 
        assertTrue(tup.get(1) == null);
        assertTrue(tup.get(2) == null);
        assertTrue(tup.get(3) == null);
        assertTrue(tup.get(4) == null);
        
        //tuple 2 
        tup = it.next();
        assertTrue(tup.get(0) == null);
        assertTrue( (Integer)tup.get(1) == 825373489);
        assertTrue( (Float)tup.get(2) == 2.5931501E-9F);
        assertTrue( (Long)tup.get(3) == 3544952156018063160L);
        assertTrue( (Double)tup.get(4) == 1.030084341992388E-71);
        
        //tuple 3
        tup = it.next();
        // when byte array is larger than required num of bytes for given number type
        // it uses the required bytes from beginging of byte array for conversion
        // for example 1634952294 corresponds to first 4 byptes of binary string correspnding to
        // asdff
        assertTrue((Integer)tup.get(0) == 1634952294);
        assertTrue( (Integer)tup.get(1) == 825373490);
        assertTrue( (Float)tup.get(2) == 2.5350009E-9F);
        assertTrue( (Long)tup.get(3) == 3544952156018063160L);
        assertTrue( (Double)tup.get(4) == 1.0300843656201408E-71);
        
        Util.deleteFile(cluster, "table");
    }
    @Test
    public void testBinStorageByteArrayCastsComplexBag() throws IOException {
        // Test for PIG-544 fix
        
        // Tries to read data in BinStorage bytearrays as other pig bags,
        // should return null if the conversion fails.
        
        String[] input = {
                "{(asdf)}",
                "{(2344)}",
                "{(2344}",
                "{(323423423423434)}",
                "{(323423423423434L)}",
                "{(asdff)}"
        };
        
        Util.createInputFile(cluster, "table_bs_ac_clx", input);

        // test with BinStorage
        pigServer.registerQuery("a = load 'table_bs_ac_clx' as (f1);");
        pigServer.registerQuery("b = foreach a generate (bag{tuple(int)})f1;");
        
        Iterator<Tuple> it = pigServer.openIterator("b");
        
        Tuple tup=null;

        //tuple 1 
        tup = it.next();
        assertTrue(tup.get(0) != null);
        
        //tuple 2 
        tup = it.next();
        assertTrue(tup.get(0) != null);
        
        //tuple 3 - malformed
        tup = it.next();
        assertTrue(tup.get(0) == null);

        //tuple 4 - integer exceeds size limit
        tup = it.next();
        assertTrue(tup.get(0) == null);

        //tuple 5 
        tup = it.next();
        assertTrue(tup.get(0) != null);

        //tuple 6
        tup = it.next();
        assertTrue(tup.get(0) != null);
        
        Util.deleteFile(cluster, "table_bs_ac_clx");
    }
    @Test
    public void testBinStorageByteArrayCastsComplexTuple() throws IOException {
        // Test for PIG-544 fix
        
        // Tries to read data in BinStorage bytearrays as other pig bags,
        // should return null if the conversion fails.
        
        String[] input = {
                "(123)",
                "((123)",
                "(123123123123)",
                "(asdf)"
        };
        
        Util.createInputFile(cluster, "table_bs_ac_clxt", input);

        // test with BinStorage
        pigServer.registerQuery("a = load 'table_bs_ac_clxt' as (t:tuple(t:tuple(i:int)));");
        Iterator<Tuple> it = pigServer.openIterator("a");
        
        Tuple tup=null;

        //tuple 1 
        tup = it.next();
        assertTrue(tup.get(0) != null);
        
        //tuple 2 -malformed tuple
        tup = it.next();
        assertTrue(tup.get(0) == null);
        
        //tuple 3 - integer exceeds size limit
        tup = it.next();
        assertTrue(tup.get(0) == null);

        //tuple 5 
        tup = it.next();
        assertTrue(tup.get(0) != null);

        Util.deleteFile(cluster, "table_bs_ac_clxt");
    }
    
    @Test
    public void testPigStorageWithCtrlChars() throws Exception {
        String[] inputData = { "hello\u0001world", "good\u0001morning", "nice\u0001day" };
        Util.createInputFile(cluster, "testPigStorageWithCtrlCharsInput.txt", inputData);
        String script = "a = load 'testPigStorageWithCtrlCharsInput.txt' using PigStorage('\u0001');" +
        		"b = foreach a generate $0, CONCAT($0, '\u0005'), $1; " +
        		"store b into 'testPigStorageWithCtrlCharsOutput.txt' using PigStorage('\u0001');" +
        		"c = load 'testPigStorageWithCtrlCharsOutput.txt' using PigStorage('\u0001') as (f1:chararray, f2:chararray, f3:chararray);";
        Util.registerMultiLineQuery(pigServer, script);
        Iterator<Tuple> it  = pigServer.openIterator("c");
        HashMap<String, Tuple> expectedResults = new HashMap<String, Tuple>();
        expectedResults.put("hello", (Tuple) Util.getPigConstant("('hello','hello\u0005','world')"));
        expectedResults.put("good", (Tuple) Util.getPigConstant("('good','good\u0005','morning')"));
        expectedResults.put("nice", (Tuple) Util.getPigConstant("('nice','nice\u0005','day')"));
        HashMap<String, Boolean> seen = new HashMap<String, Boolean>();
        int numRows = 0;
        while(it.hasNext()) {
            Tuple t = it.next();
            String firstCol = (String) t.get(0);
            assertFalse(seen.containsKey(firstCol));
            seen.put(firstCol, true);
            assertEquals(expectedResults.get(firstCol), t);
            numRows++;
        }
        assertEquals(3, numRows);
        
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.io.IOException;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.pig.ExecType;
import org.apache.pig.LoadFunc;
import org.apache.pig.PigException;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.impl.util.WrappedIOException;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.SchemaMergeException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOLoad extends LogicalOperator {
    private static final long serialVersionUID = 2L;
    protected boolean splittable = true;

    private FileSpec mInputFileSpec;
    transient private LoadFunc mLoadFunc;
    private String mSchemaFile;
    private Schema mEnforcedSchema = null ;
    transient private DataStorage mStorage;
    private ExecType mExecType;
    private static Log log = LogFactory.getLog(LOLoad.class);
    private Schema mDeterminedSchema = null;

    /**
     * @param plan
     *            LogicalPlan this operator is a part of.
     * @param key
     *            OperatorKey for this operator
     * @param inputFileSpec
     *            the file to be loaded *
     * @param execType
     *            the execution mode @see org.apache.pig.ExecType
     * @param storage
     *            the underlying storage
     * @param splittable
     *            if the input file is splittable (.gz is not)
     *            
     * 
     */
    public LOLoad(LogicalPlan plan, OperatorKey key, FileSpec inputFileSpec,
            ExecType execType, DataStorage storage, boolean splittable) throws IOException {
        super(plan, key);
        mInputFileSpec = inputFileSpec;
        //mSchemaFile = schemaFile;
        // schemaFile is the input file since we are trying
        // to deduce the schema by looking at the input file
        mSchemaFile = inputFileSpec.getFileName();
        mStorage = storage;
        mExecType = execType;
        this.splittable = splittable;

         try { 
             mLoadFunc = (LoadFunc)
                  PigContext.instantiateFuncFromSpec(inputFileSpec.getFuncSpec()); 
        }catch (ClassCastException cce) {
            log.error(inputFileSpec.getFuncSpec() + " should implement the LoadFunc interface.");
            throw WrappedIOException.wrap(cce);
        }
         catch (Exception e){ 
             throw WrappedIOException.wrap(e);
        }
    }

    public FileSpec getInputFile() {
        return mInputFileSpec;
    }
    
    
    public void setInputFile(FileSpec inputFileSpec) throws IOException {
       try { 
            mLoadFunc = (LoadFunc)
                 PigContext.instantiateFuncFromSpec(inputFileSpec.getFuncSpec()); 
       }catch (ClassCastException cce) {
           log.error(inputFileSpec.getFuncSpec() + " should implement the LoadFunc interface.");
           IOException ioe = new IOException(cce.getMessage()); 
           ioe.setStackTrace(cce.getStackTrace());
           throw ioe;
       }
        catch (Exception e){ 
           IOException ioe = new IOException(e.getMessage()); 
           ioe.setStackTrace(e.getStackTrace());
           throw ioe; 
       }
        mInputFileSpec = inputFileSpec;
    }

    public String getSchemaFile() {
        return mSchemaFile;
    }

    public LoadFunc getLoadFunc() {
        return mLoadFunc;
    }

    @Override
    public String name() {
        return "Load " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public Schema getSchema() throws FrontendException {
        if (!mIsSchemaComputed) {
            // get the schema of the load function
            try {
                if (mEnforcedSchema != null) {
                    mSchema = mEnforcedSchema ;
                    return mSchema ;
                }

                if(null == mDeterminedSchema) {
                    mSchema = mLoadFunc.determineSchema(mSchemaFile, mExecType, mStorage);
                    mDeterminedSchema  = mSchema;
                }
                mIsSchemaComputed = true;
            } catch (IOException ioe) {
                int errCode = 1018;
                String msg = "Problem determining schema during load";
                FrontendException fee = new FrontendException(msg, errCode, PigException.INPUT, false, null, ioe);
                mIsSchemaComputed = false;
                mSchema = null;
                throw fee;
            }
        }
        return mSchema;
    }
    
    /* (non-Javadoc)
     * @see org.apache.pig.impl.logicalLayer.LogicalOperator#setSchema(org.apache.pig.impl.logicalLayer.schema.Schema)
     */
    @Override
    public void setSchema(Schema schema) throws FrontendException {
        // In general, operators don't generate their schema until they're
        // asked, so ask them to do it.
        try {
            getSchema();
        } catch (FrontendException ioe) {
            // It's fine, it just means we don't have a schema yet.
        }
        if (mSchema == null) {
            log.debug("Operator schema is null; Setting it to new schema");
            mSchema = schema;
        } else {
            log.debug("Reconciling schema");
            log.debug("mSchema: " + mSchema + " schema: " + schema);
            try {
                mSchema = mSchema.mergePrefixSchema(schema, true, true);
            } catch (SchemaMergeException e) {
                int errCode = 1019;
                String msg = "Unable to merge schemas";
                throw new FrontendException(msg, errCode, PigException.INPUT, false, null, e);
            }
        }
    }
    

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    public Schema getEnforcedSchema() {
        return mEnforcedSchema;
    }

    /***
     * Set this when user enforces schema
     * @param enforcedSchema
     */
    public void setEnforcedSchema(Schema enforcedSchema) {
        this.mEnforcedSchema = enforcedSchema;
    }

    public boolean isSplittable() {
        return splittable;
    }

    @Override
    public byte getType() {
        return DataType.BAG ;
    }

    /**
     * @return the DeterminedSchema
     */
    public Schema getDeterminedSchema() {
        return mDeterminedSchema;
    }
    
    @Override
    public ProjectionMap getProjectionMap() {
        Schema outputSchema;
        
        try {
            outputSchema = getSchema();
        } catch (FrontendException fee) {
            return null;
        }
        
        if(outputSchema == null) {
            return null;
        }
        
        Schema inputSchema = null;        
        
        List<LogicalOperator> predecessors = (ArrayList<LogicalOperator>)mPlan.getPredecessors(this);
        if(predecessors != null) {
            try {
                inputSchema = predecessors.get(0).getSchema();
            } catch (FrontendException fee) {
                return null;
            }
        } else {
            try {
                inputSchema = mLoadFunc.determineSchema(mSchemaFile, mExecType, mStorage);
            } catch (IOException ioe) {
                return null;
            }
        }
        
        if(inputSchema == null) {
            return null;
        }
        
        if(Schema.equals(inputSchema, outputSchema, false, true)) {
            //there is a one is to one mapping between input and output schemas
            return new ProjectionMap(false);
        } else {
            MultiMap<Integer, Pair<Integer, Integer>> mapFields = new MultiMap<Integer, Pair<Integer, Integer>>();
            //compute the mapping assuming its a prefix projection
            for(int i = 0; i < inputSchema.size(); ++i) {
                mapFields.put(i, new Pair<Integer, Integer>(0, i));
            }
            return new ProjectionMap(mapFields, null, null);
        }
    }

    @Override
    public List<RequiredFields> getRequiredFields() {
        List<RequiredFields> requiredFields = new ArrayList<RequiredFields>();
        requiredFields.add(new RequiredFields(false, true));
        return requiredFields;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import java.util.List;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

/**
 * This abstract class represents the logical Unary Expression Operator The
 * unary operator has an operand and an operator. The format of the expression
 * is operator operand. The operator is implicit and not recorded in the class
 */
public abstract class UnaryExpressionOperator extends ExpressionOperator {
    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(UnaryExpressionOperator.class);

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     * @param rp
     *            degree of requested parallelism with which to execute this
     *            node.
     */
    public UnaryExpressionOperator(LogicalPlan plan, OperatorKey k, int rp) {
        super(plan, k, rp);
    }

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public UnaryExpressionOperator(LogicalPlan plan, OperatorKey k) {
        super(plan, k);

    }
    
    public ExpressionOperator getOperand() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(0);
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.ExpressionOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        UnaryExpressionOperator unExOpClone = (UnaryExpressionOperator)super.clone();
        return unExOpClone;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.tools.grunt;

import java.io.File;
import java.io.BufferedReader;
import java.io.ByteArrayOutputStream;
import java.io.PrintStream;
import java.io.IOException;
import java.io.FileOutputStream;

import jline.ConsoleReader;
import jline.Completor;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.PigServer;
import org.apache.pig.impl.PigContext;
import org.apache.pig.tools.grunt.GruntParser;
import org.apache.pig.tools.grunt.PigCompletor;
import org.apache.pig.tools.grunt.PigCompletorAliases;
import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.tools.pigscript.parser.*;
import org.apache.pig.impl.logicalLayer.parser.TokenMgrError;
import org.apache.pig.impl.util.LogUtils;

public class Grunt 
{
    private final Log log = LogFactory.getLog(getClass());
    
    BufferedReader in;
    PigServer pig;
    GruntParser parser;    

    public Grunt(BufferedReader in, PigContext pigContext) throws ExecException
    {
        this.in = in;
        this.pig = new PigServer(pigContext);
        
        if (in != null)
        {
            parser = new GruntParser(in);
            parser.setParams(pig);    
        }
    }

    public void setConsoleReader(ConsoleReader c)
    {
        c.addCompletor(new PigCompletorAliases(pig));
        c.addCompletor(new PigCompletor());
        parser.setConsoleReader(c);
    }

    public void run() {        
        boolean verbose = "true".equalsIgnoreCase(pig.getPigContext().getProperties().getProperty("verbose"));
        while(true) {
            try {
                parser.setInteractive(true);
                parser.parseStopOnError();
                break;                            
            } catch(Throwable t) {
                LogUtils.writeLog(t, pig.getPigContext().getProperties().getProperty("pig.logfile"), log, verbose);
                parser.ReInit(in);
            }
        }
    }

    public int[] exec() throws Throwable {
        boolean verbose = "true".equalsIgnoreCase(pig.getPigContext().getProperties().getProperty("verbose"));
        try {
            parser.setInteractive(false);
            return parser.parseStopOnError();
        } catch (Throwable t) {
            LogUtils.writeLog(t, pig.getPigContext().getProperties().getProperty("pig.logfile"), log, verbose);
            throw (t);
        }
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer.optimizer;

import java.util.Set;

import org.apache.pig.ExecType;
import org.apache.pig.impl.logicalLayer.LOLimit;
import org.apache.pig.impl.logicalLayer.LOLoad;
import org.apache.pig.impl.logicalLayer.LOPrinter;
import org.apache.pig.impl.logicalLayer.LOStream;
import org.apache.pig.impl.logicalLayer.LogicalOperator;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.plan.optimizer.*;

/**
 * An optimizer for logical plans.
 */
public class LogicalOptimizer extends
        PlanOptimizer<LogicalOperator, LogicalPlan> {

    private static final String SCOPE = "RULE";
    private static NodeIdGenerator nodeIdGen = NodeIdGenerator.getGenerator();
    
    private Set<String> mRulesOff = null;

    public LogicalOptimizer(LogicalPlan plan) {
        this(plan, ExecType.MAPREDUCE);
    }

    public LogicalOptimizer(LogicalPlan plan, ExecType mode) {
        super(plan);
        runOptimizations(plan, mode);
    }
    
    public LogicalOptimizer(LogicalPlan plan, ExecType mode, Set<String> turnOffRules) {
        super(plan);
        mRulesOff = turnOffRules;
        runOptimizations(plan, mode);
    }

    private void runOptimizations(LogicalPlan plan, ExecType mode) {
        RulePlan rulePlan;

        // List of rules for the logical optimizer

        // This one has to be first, as the type cast inserter expects the
        // load to only have one output.
        // Find any places in the plan that have an implicit split and make
        // it explicit. Since the RuleMatcher doesn't handle trees properly,
        // we cheat and say that we match any node. Then we'll do the actual
        // test in the transformers check method.
        
        boolean turnAllRulesOff = false;
        if (mRulesOff != null) {
            for (String rule : mRulesOff) {
                if ("all".equalsIgnoreCase(rule)) {
                    turnAllRulesOff = true;
                    break;
                }
            }
        }
        
        rulePlan = new RulePlan();
        RuleOperator anyLogicalOperator = new RuleOperator(LogicalOperator.class, RuleOperator.NodeType.ANY_NODE, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(anyLogicalOperator);
        mRules.add(new Rule<LogicalOperator, LogicalPlan>(rulePlan,
                new ImplicitSplitInserter(plan), "ImplicitSplitInserter"));

        // Add type casting to plans where the schema has been declared (by
        // user, data, or data catalog).
        rulePlan = new RulePlan();
        RuleOperator loLoad = new RuleOperator(LOLoad.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(loLoad);
        mRules.add(new Rule<LogicalOperator, LogicalPlan>(rulePlan,
                new TypeCastInserter(plan, LOLoad.class.getName()), "LoadTypeCastInserter"));

        // Add type casting to plans where the schema has been declared by
        // user in a statement with stream operator.
        rulePlan = new RulePlan();
        RuleOperator loStream= new RuleOperator(LOStream.class, 
                new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
        rulePlan.add(loStream);
        mRules.add(new Rule<LogicalOperator, LogicalPlan>(rulePlan, new TypeCastInserter(plan,
                LOStream.class.getName()), "StreamTypeCastInserter"));

        // Optimize when LOAD precedes STREAM and the loader class
        // is the same as the serializer for the STREAM.
        // Similarly optimize when STREAM is followed by store and the
        // deserializer class is same as the Storage class.
        if(!turnAllRulesOff) {
            Rule rule = new Rule<LogicalOperator, LogicalPlan>(rulePlan, new StreamOptimizer(plan,
                    LOStream.class.getName()), "StreamOptimizer");
            checkAndAddRule(rule);
        }

        // Push up limit where ever possible.
        if(!turnAllRulesOff) {
            rulePlan = new RulePlan();
            RuleOperator loLimit = new RuleOperator(LOLimit.class, 
                    new OperatorKey(SCOPE, nodeIdGen.getNextNodeId(SCOPE)));
            rulePlan.add(loLimit);
            Rule rule = new Rule<LogicalOperator, LogicalPlan>(rulePlan,
                    new OpLimitOptimizer(plan, mode), "LimitOptimizer");
            checkAndAddRule(rule);
        }
        
    }

    private void checkAndAddRule(Rule rule) {
        if(mRulesOff != null) {
            for(String ruleOff: mRulesOff) {
                String ruleName = rule.getRuleName();
                if(ruleName == null) continue;
                if(ruleName.equalsIgnoreCase(ruleOff)) return;
            }
        }
        mRules.add(rule);
    }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import java.util.List;

import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

/**
 * This abstract class represents the logical Binary Expression Operator
 * The binary operator has two operands and an operator. The format of
 * the expression is lhs_operand operator rhs_operand. The operator name
 * is assumed and can be inferred by the class name 
 */

public abstract class BinaryExpressionOperator extends ExpressionOperator {
    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(BinaryExpressionOperator.class);

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     * @param rp
     *            degree of requested parallelism with which to execute this
     *            node.
     */
    public BinaryExpressionOperator(LogicalPlan plan, OperatorKey k, int rp) {
        super(plan, k, rp);
    }

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public BinaryExpressionOperator(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }
    
    public ExpressionOperator getLhsOperand() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(0);
    }

    public ExpressionOperator getRhsOperand() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(1);
    }
        
    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }
    
    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.ExpressionOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        BinaryExpressionOperator binExOpClone = (BinaryExpressionOperator)super.clone();
        return binExOpClone;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.plan.optimizer;

import java.util.ArrayList;
import java.util.List;

import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorPlan;
import org.apache.pig.impl.plan.VisitorException;

/******************************************************************************
 * A class to optimize plans.  This class need not be subclassed for a
 * particular type of plan.  It can be instantiated with a set of Rules and
 * then optimize called.
 *
 */

public abstract class PlanOptimizer<O extends Operator, P extends OperatorPlan<O>> {
    
    protected List<Rule> mRules;
    protected P mPlan;
    protected int mMaxIterations;

    /**
     * @param plan Plan to optimize
     */
    protected PlanOptimizer(P plan) {
        this(plan, 500);
    }

    /**
     * @param plan Plan to optimize
     * @param iterations maximum number of optimization iterations
     */
    protected PlanOptimizer(P plan, int iterations) {
        mRules = new ArrayList<Rule>();
        mPlan = plan;
        if(iterations < 0) {
            mMaxIterations = 1000;
        } else {
            mMaxIterations = iterations;
        }
    }
    
    /**
     * Run the optimizer.  This method attempts to match each of the Rules
     * against the plan.  If a Rule matches, it then calls the check
     * method of the associated Transformer to give the it a chance to
     * check whether it really wants to do the optimization.  If that
     * returns true as well, then Transformer.transform is called. 
     * @throws OptimizerException
     */
    public final void optimize() throws OptimizerException {
        boolean sawMatch = false;
        int numIterations = 0;
        do {
            sawMatch = false;
            for (Rule rule : mRules) {
                RuleMatcher matcher = new RuleMatcher();
                if (matcher.match(rule)) {
                    // It matches the pattern.  Now check if the transformer
                    // approves as well.
                    List<List<O>> matches = matcher.getAllMatches();
                    for (List<O> match:matches)
                    {
    	                if (rule.getTransformer().check(match)) {
    	                    // The transformer approves.
    	                    sawMatch = true;
    	                    rule.getTransformer().transform(match);
    	                }
                    }
                }
            }
        } while(sawMatch && ++numIterations < mMaxIterations);
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.test;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.PrintWriter;
import java.util.Map;
import java.util.Random;

import junit.framework.TestCase;

import org.apache.hadoop.fs.Path;
import org.apache.pig.ExecType;
import org.apache.pig.PigServer;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.tools.pigstats.PigStats;
import org.junit.Test;

public class TestCounters extends TestCase {
    String file = "input.txt";

    MiniCluster cluster = MiniCluster.buildCluster();

    final int MAX = 100*1000;
    Random r = new Random();

    @Test
    public void testMapOnly() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        for(int i = 0; i < MAX; i++) {
            int t = r.nextInt(100);
            pw.println(t);
            if(t > 50)
                count ++;
        }
        pw.close();
        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = filter a by $0 > 50;");
        pigServer.registerQuery("c = foreach b generate $0 - 50;");
        PigStats pigStats = pigServer.store("c", "output_map_only").getStatistics();

        //PigStats pigStats = pigServer.getPigStats();
        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        
        //counting the no. of bytes in the output file
        //long filesize = cluster.getFileSystem().getFileStatus(new Path("output_map_only")).getLen();
        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output_map_only", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();
        
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output_map_only"), true);

        System.out.println("============================================");
        System.out.println("Test case Map Only");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map.Entry<String, Map<String, String>> e = stats.entrySet().iterator().next();

        //System.out.println("Job Name : " + e.getKey());

        Map<String, String> jobStats = e.getValue();

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        assertNull(jobStats.get("PIG_STATS_REDUCE_PLAN"));
        assertNull(jobStats.get("PIG_STATS_COMBINE_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_MAP_PLAN"));
        assertEquals(0, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        assertEquals(0, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));

        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());

    }

    @Test
    public void testMapOnlyBinStorage() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        for(int i = 0; i < MAX; i++) {
            int t = r.nextInt(100);
            pw.println(t);
            if(t > 50)
                count ++;
        }
        pw.close();
        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = filter a by $0 > 50;");
        pigServer.registerQuery("c = foreach b generate $0 - 50;");
        //pigServer.store("c", "output_map_only");
        PigStats pigStats = pigServer.store("c", "output_map_only", "BinStorage").getStatistics();
        
        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output_map_only", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();

        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output_map_only"), true);

        System.out.println("============================================");
        System.out.println("Test case Map Only");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map.Entry<String, Map<String, String>> e = stats.entrySet().iterator().next();

        //System.out.println("Job Name : " + e.getKey());

        Map<String, String> jobStats = e.getValue();

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        assertNull(jobStats.get("PIG_STATS_REDUCE_PLAN"));
        assertNull(jobStats.get("PIG_STATS_COMBINE_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_MAP_PLAN"));
        assertEquals(0, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        assertEquals(0, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));

        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());
    }

    @Test
    public void testMapReduceOnly() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        int [] nos = new int[10];
        for(int i = 0; i < 10; i++)
            nos[i] = 0;

        for(int i = 0; i < MAX; i++) {
            int index = r.nextInt(10);
            int value = r.nextInt(100);
            nos[index] += value;
            pw.println(index + "\t" + value);
        }
        pw.close();

        for(int i = 0; i < 10; i++) 
            if(nos[i] > 0)
                count ++;

        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = group a by $0;");
        pigServer.registerQuery("c = foreach b generate group;");
        PigStats pigStats = pigServer.store("c", "output").getStatistics();
        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();

        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output"), true);

        System.out.println("============================================");
        System.out.println("Test case MapReduce");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map.Entry<String, Map<String, String>> e = stats.entrySet().iterator().next();

        Map<String, String> jobStats = e.getValue();

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        System.out.println("Reduce input records : " + jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        System.out.println("Reduce output records : " + jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));

        assertNull(jobStats.get("PIG_STATS_COMBINE_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_MAP_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_REDUCE_PLAN"));

        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());
    }

    @Test
    public void testMapReduceOnlyBinStorage() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        int [] nos = new int[10];
        for(int i = 0; i < 10; i++)
            nos[i] = 0;

        for(int i = 0; i < MAX; i++) {
            int index = r.nextInt(10);
            int value = r.nextInt(100);
            nos[index] += value;
            pw.println(index + "\t" + value);
        }
        pw.close();

        for(int i = 0; i < 10; i++) 
            if(nos[i] > 0)
                count ++;

        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = group a by $0;");
        pigServer.registerQuery("c = foreach b generate group;");
        PigStats pigStats = pigServer.store("c", "output", "BinStorage").getStatistics();

        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();
        
        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output"), true);

        System.out.println("============================================");
        System.out.println("Test case MapReduce");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map.Entry<String, Map<String, String>> e = stats.entrySet().iterator().next();

        Map<String, String> jobStats = e.getValue();

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        System.out.println("Reduce input records : " + jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        System.out.println("Reduce output records : " + jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));

        assertNull(jobStats.get("PIG_STATS_COMBINE_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_MAP_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_REDUCE_PLAN"));
        
        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());

    }

    @Test
    public void testMapCombineReduce() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        int [] nos = new int[10];
        for(int i = 0; i < 10; i++)
            nos[i] = 0;

        for(int i = 0; i < MAX; i++) {
            int index = r.nextInt(10);
            int value = r.nextInt(100);
            nos[index] += value;
            pw.println(index + "\t" + value);
        }
        pw.close();

        for(int i = 0; i < 10; i++) 
            if(nos[i] > 0)
                count ++;

        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = group a by $0;");
        pigServer.registerQuery("c = foreach b generate group, SUM(a.$1);");
        PigStats pigStats = pigServer.store("c", "output").getStatistics();

        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();
        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output"), true);

        System.out.println("============================================");
        System.out.println("Test case MapCombineReduce");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map.Entry<String, Map<String, String>> e = stats.entrySet().iterator().next();

        Map<String, String> jobStats = e.getValue();

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        System.out.println("Reduce input records : " + jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        System.out.println("Reduce output records : " + jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));

        assertNotNull(jobStats.get("PIG_STATS_COMBINE_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_MAP_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_REDUCE_PLAN"));

        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());
    }

    @Test
    public void testMapCombineReduceBinStorage() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        int [] nos = new int[10];
        for(int i = 0; i < 10; i++)
            nos[i] = 0;

        for(int i = 0; i < MAX; i++) {
            int index = r.nextInt(10);
            int value = r.nextInt(100);
            nos[index] += value;
            pw.println(index + "\t" + value);
        }
        pw.close();

        for(int i = 0; i < 10; i++) 
            if(nos[i] > 0)
                count ++;

        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = group a by $0;");
        pigServer.registerQuery("c = foreach b generate group, SUM(a.$1);");
        PigStats pigStats = pigServer.store("c", "output", "BinStorage").getStatistics();

        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();
        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output"), true);

        System.out.println("============================================");
        System.out.println("Test case MapCombineReduce");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map.Entry<String, Map<String, String>> e = stats.entrySet().iterator().next();

        Map<String, String> jobStats = e.getValue();

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        System.out.println("Reduce input records : " + jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        System.out.println("Reduce output records : " + jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));

        assertNotNull(jobStats.get("PIG_STATS_COMBINE_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_MAP_PLAN"));
        assertNotNull(jobStats.get("PIG_STATS_REDUCE_PLAN"));

        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());
    }

    @Test
    public void testMultipleMRJobs() throws IOException, ExecException {
        int count = 0;
        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        int [] nos = new int[10];
        for(int i = 0; i < 10; i++)
            nos[i] = 0;

        for(int i = 0; i < MAX; i++) {
            int index = r.nextInt(10);
            int value = r.nextInt(100);
            nos[index] += value;
            pw.println(index + "\t" + value);
        }
        pw.close();

        for(int i = 0; i < 10; i++) 
            if(nos[i] > 0)
                count ++;

        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pigServer.registerQuery("a = load '" + file + "';");
        pigServer.registerQuery("b = order a by $0;");
        pigServer.registerQuery("c = group b by $0;");
        pigServer.registerQuery("d = foreach c generate group, SUM(b.$1);");
        PigStats pigStats = pigServer.store("d", "output").getStatistics();
        
        InputStream is = FileLocalizer.open(FileLocalizer.fullPath("output", pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();
        Map<String, Map<String, String>> stats = pigStats.getPigStats();
        cluster.getFileSystem().delete(new Path(file), true);
        cluster.getFileSystem().delete(new Path("output"), true);
        
        System.out.println("============================================");
        System.out.println("Test case MultipleMRJobs");
        System.out.println("============================================");
        System.out.println("MRPlan : \n" + pigStats.getMRPlan());
        for(Map.Entry<String, Map<String, String>> entry : stats.entrySet()) {
            System.out.println("============================================");
            System.out.println("Job : " + entry.getKey());
            for(Map.Entry<String, String> e1 : entry.getValue().entrySet()) {
                System.out.println(" - " + e1.getKey() + " : \n" + e1.getValue());
            }
            System.out.println("============================================");
        }

        Map<String, String> jobStats = stats.get(pigStats.getRootJobIDs().get(0));

        System.out.println("Map input records : " + jobStats.get("PIG_STATS_MAP_INPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_INPUT_RECORDS")));
        System.out.println("Map output records : " + jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS"));
        assertEquals(MAX, Integer.parseInt(jobStats.get("PIG_STATS_MAP_OUTPUT_RECORDS")));
        System.out.println("Reduce input records : " + jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_INPUT_RECORDS")));
        System.out.println("Reduce output records : " + jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS"));
        assertEquals(count, Integer.parseInt(jobStats.get("PIG_STATS_REDUCE_OUTPUT_RECORDS")));
        
        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());

    }
    
    @Test
    public void testLocal() throws IOException, ExecException {
        int count = 0;
        //PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, file));
        File file = File.createTempFile("data", ".txt");
        PrintWriter pw = new PrintWriter(new FileOutputStream(file));
        int [] nos = new int[10];
        for(int i = 0; i < 10; i++)
            nos[i] = 0;

        for(int i = 0; i < MAX; i++) {
            int index = r.nextInt(10);
            int value = r.nextInt(100);
            nos[index] += value;
            pw.println(index + "\t" + value);
        }
        pw.close();

        for(int i = 0; i < 10; i++) 
            if(nos[i] > 0)
                count ++;

        File out = File.createTempFile("output", ".txt");
        out.delete();
        PigServer pigServer = new PigServer("local");
        pigServer.registerQuery("a = load '" + Util.encodeEscape(file.toString()) + "';");
        pigServer.registerQuery("b = order a by $0;");
        pigServer.registerQuery("c = group b by $0;");
        pigServer.registerQuery("d = foreach c generate group, SUM(b.$1);");
        PigStats pigStats = pigServer.store("d", out.getAbsolutePath()).getStatistics();
        InputStream is = FileLocalizer.open(FileLocalizer.fullPath(out.getAbsolutePath(), pigServer.getPigContext()), ExecType.MAPREDUCE, pigServer.getPigContext().getDfs());
        long filesize = 0;
        while(is.read() != -1) filesize++;
        
        is.close();
        out.delete();
        
        //Map<String, Map<String, String>> stats = pigStats.getPigStats();
        
        assertEquals(count, pigStats.getRecordsWritten());
        assertEquals(filesize, pigStats.getBytesWritten());

    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer.validators;

import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Hashtable;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Stack;
import java.util.HashSet;
import java.util.TreeMap;


import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.LoadFunc;
import org.apache.pig.Algebraic;
import org.apache.pig.PigException;
import org.apache.pig.PigWarning;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.logicalLayer.ExpressionOperator;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.LOConst;
import org.apache.pig.impl.logicalLayer.LOUserFunc;
import org.apache.pig.impl.logicalLayer.LogicalOperator;
import org.apache.pig.impl.logicalLayer.LogicalPlan;

import org.apache.pig.impl.logicalLayer.* ;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.SchemaMergeException;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType ;
import org.apache.pig.impl.plan.*;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.data.DataType;
import org.apache.pig.impl.streaming.StreamingCommand;
import org.apache.pig.impl.streaming.StreamingCommand.Handle;
import org.apache.pig.impl.streaming.StreamingCommand.HandleSpec;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

/**
 * Visitor for type checking. For simplicity of the first implementation,
 * we throw exception immediately once something doesn't look alright.
 * This is not quite smart e.g. if the plan has another unrelated branch.
 *
 */
public class TypeCheckingVisitor extends LOVisitor {

    private static final int INF = -1;

    private static final Log log = LogFactory.getLog(TypeCheckingVisitor.class);

    private CompilationMessageCollector msgCollector = null ;

    private boolean strictMode = false ;
    
    public static MultiMap<Byte, Byte> castLookup = new MultiMap<Byte, Byte>();
    static{
        //Ordering here decides the score for the best fit function.
        //Do not change the order. Conversions to a smaller type is preferred
        //over conversion to a bigger type where ordering of types is:
        //INTEGER, LONG, FLOAT, DOUBLE, CHARARRAY, TUPLE, BAG, MAP
        //from small to big
//        castLookup.put(DataType.BOOLEAN, DataType.INTEGER);
//        castLookup.put(DataType.BOOLEAN, DataType.LONG);
//        castLookup.put(DataType.BOOLEAN, DataType.FLOAT);
//        castLookup.put(DataType.BOOLEAN, DataType.DOUBLE);
//        castLookup.put(DataType.BOOLEAN, DataType.CHARARRAY);
        castLookup.put(DataType.INTEGER, DataType.LONG);
        castLookup.put(DataType.INTEGER, DataType.FLOAT);
        castLookup.put(DataType.INTEGER, DataType.DOUBLE);
//        castLookup.put(DataType.INTEGER, DataType.CHARARRAY);
        castLookup.put(DataType.LONG, DataType.FLOAT);
        castLookup.put(DataType.LONG, DataType.DOUBLE);
//        castLookup.put(DataType.LONG, DataType.CHARARRAY);
        castLookup.put(DataType.FLOAT, DataType.DOUBLE);
//        castLookup.put(DataType.FLOAT, DataType.CHARARRAY);
//        castLookup.put(DataType.DOUBLE, DataType.CHARARRAY);
//        castLookup.put(DataType.BYTEARRAY, DataType.BOOLEAN);
        castLookup.put(DataType.BYTEARRAY, DataType.INTEGER);
        castLookup.put(DataType.BYTEARRAY, DataType.LONG);
        castLookup.put(DataType.BYTEARRAY, DataType.FLOAT);
        castLookup.put(DataType.BYTEARRAY, DataType.DOUBLE);
        castLookup.put(DataType.BYTEARRAY, DataType.CHARARRAY);
        castLookup.put(DataType.BYTEARRAY, DataType.TUPLE);
        castLookup.put(DataType.BYTEARRAY, DataType.BAG);
        castLookup.put(DataType.BYTEARRAY, DataType.MAP);
    }

    public TypeCheckingVisitor(LogicalPlan plan,
                        CompilationMessageCollector messageCollector) {
        super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(plan));
        msgCollector = messageCollector ;
    }

    // Just in case caller is lazy
    @Override
    protected void visit(ExpressionOperator eOp)
                                throws VisitorException {
        if (eOp instanceof BinaryExpressionOperator) {
            visit((BinaryExpressionOperator) eOp) ;
        }
        else if (eOp instanceof UnaryExpressionOperator) {
            visit((UnaryExpressionOperator) eOp) ;
        }
        else if (eOp instanceof LOConst) {
            visit((LOConst) eOp) ;
        }
        else if (eOp instanceof LOBinCond) {
            visit((LOBinCond) eOp) ;
        }
        else if (eOp instanceof LOCast) {
            visit((LOCast) eOp) ;
        }
        else if (eOp instanceof LORegexp) {
            visit((LORegexp) eOp) ;
        }
        else if (eOp instanceof LOUserFunc) {
            visit((LOUserFunc) eOp) ;
        }
        else if (eOp instanceof LOProject) {
            visit((LOProject) eOp) ;
        }
        else if (eOp instanceof LONegative) {
            visit((LONegative) eOp) ;
        }
        else if (eOp instanceof LONot) {
            visit((LONot) eOp) ;
        }
        else if (eOp instanceof LOMapLookup) {
            visit((LOMapLookup) eOp) ;
        }
        // TODO: Check that all operators are included here
    }


    // Just in case caller is lazy
    @Override
    protected void visit(LogicalOperator lOp)
                                throws VisitorException {
        if (lOp instanceof LOLoad) {
            visit((LOLoad) lOp) ;
        }
        else if (lOp instanceof LODistinct) {
            visit((LODistinct) lOp) ;
        }
        else if (lOp instanceof LOFilter) {
            visit((LOFilter) lOp) ;
        }
        else if (lOp instanceof LOUnion) {
            visit((LOUnion) lOp) ;
        }
        else if (lOp instanceof LOSplit) {
            visit((LOSplit) lOp) ;
        }
        else if (lOp instanceof LOSplitOutput) {
            visit((LOSplitOutput) lOp) ;
        }
        else if (lOp instanceof LOCogroup) {
            visit((LOCogroup) lOp) ;
        }
        else if (lOp instanceof LOSort) {
            visit((LOSort) lOp) ;
        }
        else if (lOp instanceof LOForEach) {
            visit((LOForEach) lOp) ;
        }
        else if (lOp instanceof LOGenerate) {
            visit((LOGenerate) lOp) ;
        }
        else if (lOp instanceof LOCross) {
            visit((LOCross) lOp) ;
        }
        // TODO: Check that all operators are included here
    }



    protected void visit(LOProject pj) throws VisitorException {
        resolveLOProjectType(pj) ;
    }

    private void resolveLOProjectType(LOProject pj) throws VisitorException {

        try {
            pj.getFieldSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1035;
            String msg = "Error getting LOProject's input schema" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    /**
     * LOConst. Type information should be associated with LOConst
     * in the parsing stage so we don't need any logic here
     */
    @Override
    protected void visit(LOConst cs)
                        throws VisitorException {

    }

    @Override
    public void visit(LOMapLookup map)
                        throws VisitorException {
        if(!DataType.isAtomic(DataType.findType(map.getLookUpKey()))) {
            int errCode = 1036;
            String msg = "Map key should be a basic type" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        map.setType(map.getValueType());

    }

    /**
     * LORegexp expects CharArray as input
     * Itself always returns Boolean
     * @param rg
     */
    @Override
    protected void visit(LORegexp rg)
            throws VisitorException {

        // We allow BYTEARRAY to be converted to CHARARRAY
        if (rg.getOperand().getType() == DataType.BYTEARRAY)
        {
            insertCastForRegexp(rg) ;
        }

        // Other than that if it's not CharArray just say goodbye
        if (rg.getOperand().getType() != DataType.CHARARRAY)
        {
            int errCode = 1037;
            String msg = "Operand of Regex can be CharArray only" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }
    }

    private void insertCastForRegexp(LORegexp rg) throws VisitorException {
        insertCast(rg, DataType.CHARARRAY, rg.getOperand());
    }

    public void visit(LOAnd binOp) throws VisitorException {
        // if lhs or rhs is null constant then cast it to boolean
        insertCastsForNullToBoolean(binOp);
        ExpressionOperator lhs = binOp.getLhsOperand();
        ExpressionOperator rhs = binOp.getRhsOperand();

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;
        Schema.FieldSchema fs = new Schema.FieldSchema(null, DataType.BOOLEAN);

        if (  (lhsType != DataType.BOOLEAN)  ||
              (rhsType != DataType.BOOLEAN)  ) {
            int errCode = 1038;
            String msg = "Operands of AND/OR can be boolean only" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    /**
     * @param binOp
     * @throws VisitorException
     */
    private void insertCastsForNullToBoolean(BinaryExpressionOperator binOp)
            throws VisitorException {
        if (binOp.getLhsOperand() instanceof LOConst
                && ((LOConst) binOp.getLhsOperand()).getValue() == null)
            insertLeftCastForBinaryOp(binOp, DataType.BOOLEAN);
        if (binOp.getRhsOperand() instanceof LOConst
                && ((LOConst) binOp.getRhsOperand()).getValue() == null)
            insertRightCastForBinaryOp(binOp, DataType.BOOLEAN);
    }

    @Override
    public void visit(LOOr binOp) throws VisitorException {
        // if lhs or rhs is null constant then cast it to boolean
        insertCastsForNullToBoolean(binOp);
        ExpressionOperator lhs = binOp.getLhsOperand();
        ExpressionOperator rhs = binOp.getRhsOperand();

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;
        Schema.FieldSchema fs = new Schema.FieldSchema(null, DataType.BOOLEAN);

        if (  (lhsType != DataType.BOOLEAN)  ||
              (rhsType != DataType.BOOLEAN)  ) {
            int errCode = 1038;
            String msg = "Operands of AND/OR can be boolean only" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LOMultiply binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
             DataType.isNumberType(rhsType) ) {

            // return the bigger type
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(rhsType)) ) {
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(lhsType)) ) {
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // Cast both operands to double
            insertLeftCastForBinaryOp(binOp, DataType.DOUBLE) ;
            insertRightCastForBinaryOp(binOp, DataType.DOUBLE) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in Multiplication Operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        try {
            binOp.regenerateFieldSchema();
        } catch (FrontendException fe) {
            int errCode = 1040;
            String msg = "Could not set Multiply field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    @Override
    public void visit(LODivide binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
             DataType.isNumberType(rhsType) ) {

            // return the bigger type
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(rhsType)) ) {
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(lhsType)) ) {
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // Cast both operands to double
            insertLeftCastForBinaryOp(binOp, DataType.DOUBLE) ;
            insertRightCastForBinaryOp(binOp, DataType.DOUBLE) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in Division Operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        try {
            binOp.regenerateFieldSchema();
        } catch (FrontendException fe) {
            int errCode = 1040;
            String msg = "Could not set Divide field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    @Override
    public void visit(LOAdd binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
             DataType.isNumberType(rhsType) ) {

            // return the bigger type
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(rhsType)) ) {
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(lhsType)) ) {
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // Cast both operands to double
            insertLeftCastForBinaryOp(binOp, DataType.DOUBLE) ;
            insertRightCastForBinaryOp(binOp, DataType.DOUBLE) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in Add Operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }
        try {
            binOp.regenerateFieldSchema();
        } catch (FrontendException fe) {
            int errCode = 1040;
            String msg = "Could not set Add field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    @Override
    public void visit(LOSubtract binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
                DataType.isNumberType(rhsType) ) {

            // return the bigger type
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                (DataType.isNumberType(rhsType)) ) {
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  (DataType.isNumberType(lhsType)) ) {
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // Cast both operands to double
            insertLeftCastForBinaryOp(binOp, DataType.DOUBLE) ;
            insertRightCastForBinaryOp(binOp, DataType.DOUBLE) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in Subtract Operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }
        try {
            binOp.regenerateFieldSchema();
        } catch (FrontendException fe) {
            int errCode = 1040;
            String msg = "Could not set Subtract field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }



    @Override
    public void visit(LOGreaterThan binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
             DataType.isNumberType(rhsType) ) {
            // If not the same type, we cast them to the same
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.CHARARRAY) &&
                  (rhsType == DataType.CHARARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) )
                ) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) )
                ) {
            // Cast byte array to the type on lhs
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in GreaterThan operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LOGreaterThanEqual binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
             DataType.isNumberType(rhsType) ) {
            // If not the same type, we cast them to the same
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.CHARARRAY) &&
                  (rhsType == DataType.CHARARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) )
                ) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) )
                ) {
            // Cast byte array to the type on lhs
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in GreaterThanEqualTo operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LOLesserThan binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;
        if ( DataType.isNumberType(lhsType) &&
                DataType.isNumberType(rhsType) ) {
            // If not the same type, we cast them to the same
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.CHARARRAY) &&
                  (rhsType == DataType.CHARARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) )
                ) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) )
                ) {
            // Cast byte array to the type on lhs
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in LesserThan operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LOLesserThanEqual binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
                DataType.isNumberType(rhsType) ) {
            // If not the same type, we cast them to the same
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }
        }
        else if ( (lhsType == DataType.CHARARRAY) &&
                  (rhsType == DataType.CHARARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) )
                ) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) )
                ) {
            // Cast byte array to the type on lhs
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in LesserThanEqualTo operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }



    @Override
    public void visit(LOEqual binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( DataType.isNumberType(lhsType) &&
                DataType.isNumberType(rhsType) ) {

            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }

        }
        else if ( (lhsType == DataType.CHARARRAY) &&
                  (rhsType == DataType.CHARARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) )
                ) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) )
                ) {
            // Cast byte array to the type on lhs
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else if ( (lhsType == DataType.TUPLE) &&
                  (rhsType == DataType.TUPLE) ) {
            // good
        }
        else if ( (lhsType == DataType.MAP) &&
                  (rhsType == DataType.MAP) ) {
            // good
        }
        // A constant null is always bytearray - so cast it
        // to rhs type
        else if (binOp.getLhsOperand() instanceof LOConst
                && ((LOConst) binOp.getLhsOperand()).getValue() == null) {
            insertLeftCastForBinaryOp(binOp, rhsType);
        } else if (binOp.getRhsOperand() instanceof LOConst
                && ((LOConst) binOp.getRhsOperand()).getValue() == null) {
            insertRightCastForBinaryOp(binOp, lhsType);
        } else {
            int errCode = 1039;
            String msg = "Incompatible types in EqualTo Operator"
                    + " left hand side:" + DataType.findTypeName(lhsType) + " right hand side:"
                    + DataType.findTypeName(rhsType);
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LONotEqual binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;


        if ( DataType.isNumberType(lhsType) &&
                DataType.isNumberType(rhsType) ) {

            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;

            // Cast smaller type to the bigger type
            if (lhsType != biggerType) {
                insertLeftCastForBinaryOp(binOp, biggerType) ;
            }
            else if (rhsType != biggerType) {
                insertRightCastForBinaryOp(binOp, biggerType) ;
            }

        }
        else if ( (lhsType == DataType.CHARARRAY) &&
                  (rhsType == DataType.CHARARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  (rhsType == DataType.BYTEARRAY) ) {
            // good
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) )
                ) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else if ( (rhsType == DataType.BYTEARRAY) &&
                  ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) )
                ) {
            // Cast byte array to the type on lhs
            insertRightCastForBinaryOp(binOp, lhsType) ;
        }
        else if ( (lhsType == DataType.TUPLE) &&
                  (rhsType == DataType.TUPLE) ) {
            // good
        }
        else if ( (lhsType == DataType.MAP) &&
                  (rhsType == DataType.MAP) ) {
            // good
        }
        // A constant null is always bytearray - so cast it
        // to rhs type
        else if (binOp.getLhsOperand() instanceof LOConst
                && ((LOConst) binOp.getLhsOperand()).getValue() == null) {
            insertLeftCastForBinaryOp(binOp, rhsType);
        } else if (binOp.getRhsOperand() instanceof LOConst
                && ((LOConst) binOp.getRhsOperand()).getValue() == null) {
            insertRightCastForBinaryOp(binOp, lhsType);
        } else {
            int errCode = 1039;
            String msg = "Incompatible types in NotEqual Operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LOMod binOp) throws VisitorException {
        ExpressionOperator lhs = binOp.getLhsOperand() ;
        ExpressionOperator rhs = binOp.getRhsOperand() ;

        byte lhsType = lhs.getType() ;
        byte rhsType = rhs.getType() ;

        if ( (lhsType == DataType.INTEGER) &&
             (rhsType == DataType.INTEGER)
           ) {
           //do nothing
        }
        else if ( (lhsType == DataType.LONG) &&
                  ( (rhsType == DataType.INTEGER) || (rhsType == DataType.LONG) )
                ) {
            if (rhsType == DataType.INTEGER) {
                insertRightCastForBinaryOp(binOp, DataType.LONG) ;
            }
        }
        else if ( (rhsType == DataType.LONG) &&
                  ( (lhsType == DataType.INTEGER) || (lhsType == DataType.LONG) )
                ) {
            if (lhsType == DataType.INTEGER) {
                insertLeftCastForBinaryOp(binOp, DataType.LONG) ;
            }
        }
        else if ( (lhsType == DataType.BYTEARRAY) &&
                  ( (rhsType == DataType.INTEGER) || (rhsType == DataType.LONG) )
                ) {
            insertLeftCastForBinaryOp(binOp, rhsType) ;
        }
        else {
            int errCode = 1039;
            String msg = "Incompatible types in Mod Operator"
                            + " left hand side:" + DataType.findTypeName(lhsType)
                            + " right hand side:" + DataType.findTypeName(rhsType) ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }
        try {
            binOp.regenerateFieldSchema();
        } catch (FrontendException fe) {
            int errCode = 1040;
            String msg = "Could not set Mod field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }


    @Override
    public void visit(LONegative uniOp) throws VisitorException {
        byte type = uniOp.getOperand().getType() ;


        if (DataType.isNumberType(type)) {
            //do nothing
        }
        else if (type == DataType.BYTEARRAY) {
            insertCastForUniOp(uniOp, DataType.DOUBLE) ;
        }
        else {
            int errCode = 1041;
            String msg = "NEG can be used with numbers or Bytearray only" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        try {
            uniOp.regenerateFieldSchema();
        } catch (FrontendException fe) {
            int errCode = 1040;
            String msg = "Could not set Negative field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }
    
    @Override
    public void visit(LONot uniOp) throws VisitorException {
        if (uniOp.getOperand() instanceof LOConst
                && ((LOConst) uniOp.getOperand()).getValue() == null) {
            insertCastForUniOp(uniOp, DataType.BOOLEAN);
        }
        byte type = uniOp.getOperand().getType();
        if (type != DataType.BOOLEAN) {
            int errCode = 1042;
            String msg = "NOT can be used with boolean only" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    @Override
    public void visit(LOIsNull uniOp) throws VisitorException {
    }

    private void insertLeftCastForBinaryOp(BinaryExpressionOperator binOp,
                                           byte toType ) throws VisitorException {
        insertCast(binOp, toType, binOp.getLhsOperand());
    }

    private void insertRightCastForBinaryOp(BinaryExpressionOperator binOp,
                                            byte toType ) throws VisitorException {
        insertCast(binOp, toType, binOp.getRhsOperand());
    }


    private void insertCast(ExpressionOperator node,
                            byte toType, ExpressionOperator predecessor) 
    throws VisitorException {
        LogicalPlan currentPlan =  (LogicalPlan) mCurrentWalker.getPlan() ;
        collectCastWarning(node, predecessor.getType(), toType);

        OperatorKey newKey = genNewOperatorKey(node);
        LOCast cast = new LOCast(currentPlan, newKey, toType) ;
        currentPlan.add(cast) ;
        try {
            currentPlan.insertBetween(predecessor, cast, node);
        }
        catch (PlanException pe) {
            int errCode = 2059;
            String msg = "Problem with inserting cast operator for " + node + " in plan.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG, pe);
        }
        this.visit(cast);
    }

    
    
    /**
     * Currently, there are two unaryOps: Neg and Not.
     */
    @Override
    protected void visit(UnaryExpressionOperator uniOp) throws VisitorException {

        byte type = uniOp.getOperand().getType() ;

        if (uniOp instanceof LONegative) {
            if (DataType.isNumberType(type)) {
                uniOp.setType(type) ;
            }
            else if (type == DataType.BYTEARRAY) {
                insertCastForUniOp(uniOp, DataType.DOUBLE) ;
                uniOp.setType(DataType.DOUBLE) ;
            }
            else {
                int errCode = 1041;
                String msg = "NEG can be used with numbers or Bytearray only" ;
                msgCollector.collect(msg, MessageType.Error);
                throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
            }
        }
        else if (uniOp instanceof LONot) {
            if (type == DataType.BOOLEAN) {
                uniOp.setType(DataType.BOOLEAN) ;
            }
            else {
                int errCode = 1042;
                String msg = "NOT can be used with boolean only" ;
                msgCollector.collect(msg, MessageType.Error);
                throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
            }
        }
        else {
            // undefined for this unknown unary operator
            int errCode = 1079;
            String msg = "Undefined type checking logic for unary operator: " + uniOp.getClass().getSimpleName();
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

    }

    private void insertCastForUniOp(UnaryExpressionOperator uniOp, byte toType) throws VisitorException {
        insertCast(uniOp, toType, uniOp.getOperand());
    }
    
    // Currently there is no input type information support in UserFunc
    // So we can just check if all inputs are not of any stupid type
    @Override
    protected void visit(LOUserFunc func) throws VisitorException {

        List<ExpressionOperator> list = func.getArguments() ;

        // If the dependency graph is right, all the inputs
        // must already know the types
        Schema s = new Schema();
        for(ExpressionOperator op: list) {
            if (!DataType.isUsableType(op.getType())) {
                int errCode = 1014;
                String msg = "Problem with input " + op + " of User-defined function: " + func;
                msgCollector.collect(msg, MessageType.Error);
                throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
            }
            try {
                s.add(op.getFieldSchema());    
            } catch (FrontendException e) {
                int errCode = 1043;
                String msg = "Unable to retrieve field schema.";
                throw new TypeCheckerException(msg, errCode, PigException.INPUT, e);
            }
            
        }

        EvalFunc<?> ef = (EvalFunc<?>) PigContext.instantiateFuncFromSpec(func.getFuncSpec());

        // If the function is algebraic and the project is just sentinel
        // (special case when we apply aggregate on flattened members)
        // then it will never match algebraic functions' schemas
        // without this

        // Assuming all aggregates has only one argument at this stage
        if(func.getArguments()!=null && func.getArguments().size()>0){
            ExpressionOperator tmpExp = func.getArguments().get(0) ;
            if ( (ef instanceof Algebraic)
                 && (tmpExp instanceof LOProject)
                 && (((LOProject)tmpExp).getSentinel())) {
    
                FieldSchema tmpField ;
    
                try {
                    // embed the schema above inside a bag
                    tmpField = new FieldSchema(null, s, DataType.BAG) ;
                }
                catch (FrontendException e) {
                    int errCode = 1023;
                    String msg = "Unable to create new field schema.";
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT, e) ;
                }
    
                s = new Schema(tmpField) ;
            }
        }
        
        // ask the EvalFunc what types of inputs it can handle
        List<FuncSpec> funcSpecs = null;
        try {
            funcSpecs = ef.getArgToFuncMapping();    
        } catch (Exception e) {
            int errCode = 1044;
            String msg = "Unable to get list of overloaded methods.";
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, e);
        }
        
        /**
         * Here is an explanation of the way the matching UDF funcspec will be chosen
         * based on actual types in the input schema.
         * First an "exact" match is tried for each of the fields in the input schema
         * with the corresponding fields in the candidate funcspecs' schemas. 
         * 
         * If exact match fails, then first a check if made if the input schema has any
         * bytearrays in it. 
         * 
         * If there are NO bytearrays in the input schema, then a best fit match is attempted
         * for the different fields. Essential a permissible cast from one type to another
         * is given a "score" based on its position in the "castLookup" table. A final
         * score for a candidate funcspec is deduced as  
         *               SUM(score_of_particular_cast*noOfCastsSoFar). 
         * If no permissible casts are possible, the score for the candidate is -1. Among 
         * the non -1 score candidates, the candidate with the lowest score is chosen. 
         * 
         * If there are bytearrays in the input schema, a modified exact match is tried. In this
         * matching, bytearrays in the input schema are not considered. As a result of
         * ignoring the bytearrays, we could get multiple candidate funcspecs which match
         * "exactly" for the other columns - if this is the case, we notify the user of
         * the ambiguity and error out. Else if all other (non byte array) fields 
         * matched exactly, then we can cast bytearray(s) to the corresponding type(s)
         * in the matched udf schema. If this modified exact match fails, the above best fit 
         * algorithm is attempted by initially coming up with scores and candidate funcSpecs 
         * (with bytearray(s) being ignored in the scoring process). Then a check is 
         * made to ensure that the positions which have bytearrays in the input schema
         * have the same type (for a given position) in the corresponding positions in
         * all the candidate funcSpecs. If this is not the case, it indicates a conflict
         * and the user is notified of the error (because we have more than
         * one choice for the destination type of the cast for the bytearray). If this is the case,
         * the candidate with the lowest score is chosen. 
         */
        
        
        
        FuncSpec matchingSpec = null;
        boolean notExactMatch = false;
        if(funcSpecs!=null && funcSpecs.size()!=0){
            //Some function mappings found. Trying to see
            //if one of them fits the input schema
            if((matchingSpec = exactMatch(funcSpecs, s, func))==null){
                //Oops, no exact match found. Trying to see if we
                //have mappings that we can fit using casts.
                notExactMatch = true;
                if(byteArrayFound(s)){
                    // try "exact" matching all other fields except the byte array 
                    // fields and if they all exact match and we have only one candidate
                    // for the byte array cast then that's the matching one!
                    if((matchingSpec = exactMatchWithByteArrays(funcSpecs, s, func))==null){
                        // "exact" match with byte arrays did not work - try best fit match
                        if((matchingSpec = bestFitMatchWithByteArrays(funcSpecs, s, func)) == null) {
                            int errCode = 1045;
                            String msg = "Could not infer the matching function for "
                                + func.getFuncSpec()
                                + " as multiple or none of them fit. Please use an explicit cast.";
                            msgCollector.collect(msg, MessageType.Error);
                            throw new TypeCheckerException(msg, errCode, PigException.INPUT);
                        }
                    }
                } else if ((matchingSpec = bestFitMatch(funcSpecs, s)) == null) {
                    // Either no byte arrays found or there are byte arrays
                    // but only one mapping exists.
                    // However, we could not find a match as there were either
                    // none fitting the input schema or it was ambiguous.
                    // Throw exception that we can't infer a fit.
                    int errCode = 1045;
                    String msg = "Could not infer the matching function for "
                            + func.getFuncSpec()
                            + " as multiple or none of them fit. Please use an explicit cast.";
                    msgCollector.collect(msg, MessageType.Error);
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT);
                }
            }
        }
        if(matchingSpec!=null){
            //Voila! We have a fitting match. Lets insert casts and make
            //it work.
            // notify the user about the match we picked if it was not
            // an exact match
            if(notExactMatch) {
                String msg = "Function " + func.getFuncSpec().getClassName() + "()" +
                             " will be called with following argument types: " +
                             matchingSpec.getInputArgsSchema() + ". If you want to use " +
                             "different input argument types, please use explicit casts.";
                msgCollector.collect(msg, MessageType.Warning, PigWarning.USING_OVERLOADED_FUNCTION);
            }
            func.setFuncSpec(matchingSpec);
            insertCastsForUDF(func, s, matchingSpec.getInputArgsSchema());
            
        }
            
        //Regenerate schema as there might be new additions
        try {
            func.regenerateFieldSchema();
        } catch (FrontendException fee) {
            int errCode = 1040;
            String msg = "Could not set UserFunc field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
        }
    }
    
    /**
     * Finds if there is an exact match between the schema supported by
     * one of the funcSpecs and the input schema s. Here first exact match
     * for all non byte array fields is first attempted and if there is
     * exactly one candidate, it is chosen (since the bytearray(s) can
     * just be cast to corresponding type(s) in the candidate)
     * @param funcSpecs - mappings provided by udf
     * @param s - input schema
     * @param func - LOUserfunc for which matching is requested
     * @return the matching spec if found else null
     * @throws VisitorException 
     */
    private FuncSpec exactMatchWithByteArrays(List<FuncSpec> funcSpecs,
            Schema s, LOUserFunc func) throws VisitorException {
        // exact match all fields except byte array fields
        // ignore byte array fields for matching
        return exactMatchHelper(funcSpecs, s, func, true);
    }

    /**
     * Finds if there is an exact match between the schema supported by
     * one of the funcSpecs and the input schema s. Here an exact match
     * for all fields is attempted.
     * @param funcSpecs - mappings provided by udf
     * @param s - input schema
     * @param func - LOUserfunc for which matching is requested
     * @return the matching spec if found else null
     * @throws VisitorException 
     */
    private FuncSpec exactMatch(List<FuncSpec> funcSpecs, Schema s,
            LOUserFunc func) throws VisitorException {
        // exact match all fields, don't ignore byte array fields
        return exactMatchHelper(funcSpecs, s, func, false);
    }

    /**
     * Tries to find the schema supported by one of funcSpecs which can
     * be obtained by inserting a set of casts to the input schema
     * @param funcSpecs - mappings provided by udf
     * @param s - input schema
     * @return the funcSpec that supports the schema that is best suited
     *          to s. The best suited schema is one that has the
     *          lowest score as returned by fitPossible().
     */
    private FuncSpec bestFitMatch(List<FuncSpec> funcSpecs, Schema s) {
        FuncSpec matchingSpec = null;
        long score = INF;
        long prevBestScore = Long.MAX_VALUE;
        long bestScore = Long.MAX_VALUE;
        for (Iterator<FuncSpec> iterator = funcSpecs.iterator(); iterator.hasNext();) {
            FuncSpec fs = iterator.next();
            score = fitPossible(s,fs.getInputArgsSchema());
            if(score!=INF && score<=bestScore){
                matchingSpec = fs;
                prevBestScore = bestScore;
                bestScore = score;
            }
        }
        if(matchingSpec!=null && bestScore!=prevBestScore)
            return matchingSpec;

        return null;
    }

    private class ScoreFuncSpecListComparator implements Comparator<Pair<Long, FuncSpec>> {

        /* (non-Javadoc)
         * @see java.util.Comparator#compare(java.lang.Object, java.lang.Object)
         */
        public int compare(Pair<Long, FuncSpec> o1, Pair<Long, FuncSpec> o2) {
            if(o1.first < o2.first)
                return -1;
            else if (o1.first > o2.first)
                return 1;
            else
                return 0;
        }
        
    }
    
    /**
     * Tries to find the schema supported by one of funcSpecs which can be
     * obtained by inserting a set of casts to the input schema
     * 
     * @param funcSpecs -
     *            mappings provided by udf
     * @param s -
     *            input schema
     * @return the funcSpec that supports the schema that is best suited to s.
     *         The best suited schema is one that has the lowest score as
     *         returned by fitPossible().
     * @throws VisitorException
     */
    private FuncSpec bestFitMatchWithByteArrays(List<FuncSpec> funcSpecs,
            Schema s, LOUserFunc func) throws VisitorException {
		List<Pair<Long, FuncSpec>> scoreFuncSpecList = new ArrayList<Pair<Long,FuncSpec>>();
        for (Iterator<FuncSpec> iterator = funcSpecs.iterator(); iterator
                .hasNext();) {
            FuncSpec fs = iterator.next();
            long score = fitPossible(s, fs.getInputArgsSchema());
            if (score != INF) {
                scoreFuncSpecList.add(new Pair<Long, FuncSpec>(score, fs));
            }
        }

        // if no candidates found, return null
        if(scoreFuncSpecList.size() == 0)
            return null;
        
        if(scoreFuncSpecList.size() > 1) {
            // sort the candidates based on score
            Collections.sort(scoreFuncSpecList, new ScoreFuncSpecListComparator());
            
            // if there are two (or more) candidates with the same *lowest* score
            // we cannot choose one of them - notify the user
            if (scoreFuncSpecList.get(0).first == scoreFuncSpecList.get(1).first) {
                int errCode = 1046;
                String msg = "Multiple matching functions for "
                        + func.getFuncSpec() + " with input schemas: " + "("
                        + scoreFuncSpecList.get(0).second.getInputArgsSchema() + ", " 
                        + scoreFuncSpecList.get(1).second.getInputArgsSchema() + "). Please use an explicit cast.";
                msgCollector.collect(msg, MessageType.Error);
                throw new TypeCheckerException(msg, errCode, PigException.INPUT);
            }
        
            // now consider the bytearray fields
            List<Integer> byteArrayPositions = getByteArrayPositions(s);
            // make sure there is only one type to "cast to" for the byte array
            // positions among the candidate funcSpecs
            Map<Integer, Pair<FuncSpec, Byte>> castToMap = new HashMap<Integer, Pair<FuncSpec, Byte>>();
            for (Iterator<Pair<Long, FuncSpec>> it = scoreFuncSpecList.iterator(); it.hasNext();) {
                FuncSpec funcSpec = it.next().second;
                Schema sch = funcSpec.getInputArgsSchema();
                for (Iterator<Integer> iter = byteArrayPositions.iterator(); iter
                        .hasNext();) {
                    Integer i = iter.next();
                    try {
                        if (!castToMap.containsKey(i)) {
                            // first candidate
                            castToMap.put(i, new Pair<FuncSpec, Byte>(funcSpec, sch
                                    .getField(i).type));
                        } else {
                            // make sure the existing type from an earlier candidate
                            // matches
                            Pair<FuncSpec, Byte> existingPair = castToMap.get(i);
                            if (sch.getField(i).type != existingPair.second) {
                                int errCode = 1046;
                                String msg = "Multiple matching functions for "
                                        + func.getFuncSpec() + " with input schema: " 
                                        + "(" + existingPair.first.getInputArgsSchema() 
                                        + ", " + funcSpec.getInputArgsSchema() 
                                        + "). Please use an explicit cast.";
                                msgCollector.collect(msg, MessageType.Error);
                                throw new TypeCheckerException(msg, errCode, PigException.INPUT);
                            }
                        }
                    } catch (FrontendException fee) {
                        int errCode = 1043;
                        String msg = "Unalbe to retrieve field schema.";
                        throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee);
                    }
                }
            }
        }
        
        // if we reached here, it means we have >= 1 candidates and these candidates
        // have the same type for position which have bytearray in the input
        // Also the candidates are stored sorted by score in a list - we can now
        // just return the first candidate (the one with the lowest score)
        return scoreFuncSpecList.get(0).second;
    }
    
    /**
     * Checks to see if any field of the input schema is a byte array
     * @param s - input schema
     * @return true if found else false
     * @throws VisitorException
     */
    private boolean byteArrayFound(Schema s) throws VisitorException {
        for(int i=0;i<s.size();i++){
            try {
                FieldSchema fs=s.getField(i);
                if(fs.type==DataType.BYTEARRAY){
                    return true;
                }
            } catch (FrontendException fee) {
                int errCode = 1043;
                String msg = "Unable to retrieve field schema.";
                throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee);
            }
        }
        return false;
    }

    /**
     * Gets the positions in the schema which are byte arrays
     * 
     * @param s -
     *            input schema
     * @throws VisitorException
     */
    private List<Integer> getByteArrayPositions(Schema s)
            throws VisitorException {
        List<Integer> result = new ArrayList<Integer>();
        for (int i = 0; i < s.size(); i++) {
            try {
                FieldSchema fs = s.getField(i);
                if (fs.type == DataType.BYTEARRAY) {
                    result.add(i);
                }
            } catch (FrontendException fee) {
                int errCode = 1043;
                String msg = "Unable to retrieve field schema.";
                throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee);            }
        }
        return result;
    }

    /**
     * Finds if there is an exact match between the schema supported by
     * one of the funcSpecs and the input schema s
     * @param funcSpecs - mappings provided by udf
     * @param s - input schema
     * @param ignoreByteArrays - flag for whether the exact match is to computed
     * after ignoring bytearray (if true) or without ignoring bytearray (if false)
     * @return the matching spec if found else null
     * @throws VisitorException 
     */
    private FuncSpec exactMatchHelper(List<FuncSpec> funcSpecs, Schema s, LOUserFunc func, boolean ignoreByteArrays) throws VisitorException {
        List<FuncSpec> matchingSpecs = new ArrayList<FuncSpec>();
        for (Iterator<FuncSpec> iterator = funcSpecs.iterator(); iterator.hasNext();) {
            FuncSpec fs = iterator.next();
            if (schemaEqualsForMatching(s, fs.getInputArgsSchema(), ignoreByteArrays)) {
                matchingSpecs.add(fs);
            }
        }
        if(matchingSpecs.size() == 0)
            return null;
        
        if(matchingSpecs.size() > 1) {
            int errCode = 1046;
            String msg = "Multiple matching functions for "
                                        + func.getFuncSpec() + " with input schema: " 
                                        + "(" + matchingSpecs.get(0).getInputArgsSchema() 
                                        + ", " + matchingSpecs.get(1).getInputArgsSchema() 
                                        + "). Please use an explicit cast.";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT);
        }
        
        // exactly one matching spec - return it
        return matchingSpecs.get(0);
    }

    /***************************************************************************
     * Compare two schemas for equality for argument matching purposes. This is
     * a more relaxed form of Schema.equals wherein first the Datatypes of the
     * field schema are checked for equality. Then if a field schema in the udf
     * schema is for a complex type AND if the inner schema is NOT null, check
     * for schema equality of the inner schemas of the UDF field schema and
     * input field schema
     * 
     * @param inputSchema
     * @param udfSchema
     * @param ignoreByteArrays
     * @return true if FieldSchemas are equal for argument matching, false
     *         otherwise
     */
    public static boolean schemaEqualsForMatching(Schema inputSchema,
            Schema udfSchema, boolean ignoreByteArrays) {
        // If both of them are null, they are equal
        if ((inputSchema == null) && (udfSchema == null)) {
            return true;
        }

        // otherwise
        if (inputSchema == null) {
            return false;
        }

        if (udfSchema == null) {
            return false;
        }

        if (inputSchema.size() != udfSchema.size())
            return false;

        Iterator<FieldSchema> i = inputSchema.getFields().iterator();
        Iterator<FieldSchema> j = udfSchema.getFields().iterator();

        while (i.hasNext()) {

            FieldSchema inputFieldSchema = i.next();
            FieldSchema udfFieldSchema = j.next();

            if(ignoreByteArrays && inputFieldSchema.type == DataType.BYTEARRAY) {
                continue;
            }
            
            if (inputFieldSchema.type != udfFieldSchema.type) {
                return false;
            }

            // if a field schema in the udf schema is for a complex
            // type AND if the inner schema is NOT null, check for schema
            // equality of the inner schemas of the UDF field schema and
            // input field schema. If the field schema in the udf schema is
            // for a complex type AND if the inner schema IS null it means
            // the udf is applicable for all input which has the same type
            // for that field (irrespective of inner schema)
            if (DataType.isSchemaType(udfFieldSchema.type)
                    && udfFieldSchema.schema != null) {
                // Compare recursively using field schema
                if (!FieldSchema.equals(inputFieldSchema, udfFieldSchema,
                        false, true)) {
                    return false;
                }
            }

        }
        return true;
    }

    /**
     * Computes a modified version of manhattan distance between 
     * the two schemas: s1 & s2. Here the value on the same axis
     * are preferred over values that change axis as this means
     * that the number of casts required will be lesser on the same
     * axis. 
     * 
     * However, this function ceases to be a metric as the triangle
     * inequality does not hold.
     * 
     * Each schema is an s1.size() dimensional vector.
     * The ordering for each axis is as defined by castLookup. 
     * Unallowed casts are returned a dist of INFINITY.
     * @param s1
     * @param s2
     * @return
     */
    private long fitPossible(Schema s1, Schema s2) {
        if(s1==null || s2==null) return INF;
        List<FieldSchema> sFields = s1.getFields();
        List<FieldSchema> fsFields = s2.getFields();
        if(sFields.size()!=fsFields.size())
            return INF;
        long score = 0;
        int castCnt=0;
        for(int i=0;i<sFields.size();i++){
            FieldSchema sFS = sFields.get(i);

            // if we have a byte array do not include it
            // in the computation of the score - bytearray
            // fields will be looked at separately outside
            // of this function
            if (sFS.type == DataType.BYTEARRAY)
                continue;

            FieldSchema fsFS = fsFields.get(i);
            
            if(DataType.isSchemaType(sFS.type)){
                if(!FieldSchema.equals(sFS, fsFS, false, true))
                    return INF;
            }
            if(FieldSchema.equals(sFS, fsFS, true, true)) continue;
            if(!castLookup.containsKey(sFS.type))
                return INF;
            if(!(castLookup.get(sFS.type).contains(fsFS.type)))
                return INF;
            score += ((List)castLookup.get(sFS.type)).indexOf(fsFS.type) + 1;
            ++castCnt;
        }
        return score * castCnt;
    }
    
    private void insertCastsForUDF(LOUserFunc udf, Schema fromSch, Schema toSch) throws VisitorException {
        List<FieldSchema> fsLst = fromSch.getFields();
        List<FieldSchema> tsLst = toSch.getFields();
        List<ExpressionOperator> args = udf.getArguments();
        List<ExpressionOperator> newArgs = new ArrayList<ExpressionOperator>(args.size());
        int i=-1;
        for (FieldSchema fFSch : fsLst) {
            ++i;
            FieldSchema tFSch = tsLst.get(i); 
            if(fFSch.type==tFSch.type) {
                continue;
            }
            insertCast(udf, tFSch.type, args.get(i));
        }
    }

    /**
     * For Bincond, lhsOp and rhsOp must have the same output type
     * or both sides have to be number
     */
    @Override
    protected void visit(LOBinCond binCond) throws VisitorException {
             
        // high-level type checking
        if (binCond.getCond().getType() != DataType.BOOLEAN) {
            int errCode = 1047;
            String msg = "Condition in BinCond must be boolean" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }       
        
        byte lhsType = binCond.getLhsOp().getType() ;
        byte rhsType = binCond.getRhsOp().getType() ;
        
        // If both sides are number, we can convert the smaller type to the bigger type
        if (DataType.isNumberType(lhsType) && DataType.isNumberType(rhsType)) {
            byte biggerType = lhsType > rhsType ? lhsType:rhsType ;
            if (biggerType > lhsType) {
                insertLeftCastForBinCond(binCond, biggerType) ;
            }
            else if (biggerType > rhsType) {
                insertRightCastForBinCond(binCond, biggerType) ;
            }
            binCond.setType(biggerType) ;
        } 
        else if ((lhsType == DataType.BYTEARRAY)
                && ((rhsType == DataType.CHARARRAY) || (DataType
                        .isNumberType(rhsType)))) {
            // Cast byte array to the type on rhs
            insertLeftCastForBinCond(binCond, rhsType);
            binCond.setType(DataType.mergeType(lhsType, rhsType));
        } else if ((rhsType == DataType.BYTEARRAY)
                && ((lhsType == DataType.CHARARRAY) || (DataType
                        .isNumberType(lhsType)))) {
            // Cast byte array to the type on lhs
            insertRightCastForBinCond(binCond, lhsType);
            binCond.setType(DataType.mergeType(lhsType, rhsType));
        }
        // A constant null is always bytearray - so cast it
        // to rhs type
        else if (binCond.getLhsOp() instanceof LOConst
                && ((LOConst) binCond.getLhsOp()).getValue() == null) {
            insertLeftCastForBinCond(binCond, rhsType);
        } else if (binCond.getRhsOp() instanceof LOConst
                && ((LOConst) binCond.getRhsOp()).getValue() == null) {
            insertRightCastForBinCond(binCond, lhsType);
        } else if (lhsType == rhsType) {
            // Matching schemas if we're working with tuples
            if (DataType.isSchemaType(lhsType)) {            
                try {
                    if (!Schema.FieldSchema.equals(binCond.getLhsOp().getFieldSchema(), binCond.getRhsOp().getFieldSchema(), false, true)) {
                        int errCode = 1048;
                        String msg = "Two inputs of BinCond must have compatible schemas." 
                            + " left hand side: " + binCond.getLhsOp().getFieldSchema() 
                            + " right hand side: " + binCond.getRhsOp().getFieldSchema();
                        msgCollector.collect(msg, MessageType.Error) ;
                        throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                    }
                    // TODO: We may have to merge the schema here
                    //       if the previous check is not exact match
                    //       Is Schema.reconcile good enough?
                } 
                catch (FrontendException fe) {
                    int errCode = 1049;
                    String msg = "Problem during evaluaton of BinCond output type" ;
                    msgCollector.collect(msg, MessageType.Error) ;
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
                }
                binCond.setType(DataType.TUPLE) ;
            }
        
            binCond.setType(lhsType);
        }
        else {
            int errCode = 1050;
            String msg = "Unsupported input type for BinCond: left hand side: " + DataType.findTypeName(lhsType) + "; right hand side: " + DataType.findTypeName(rhsType);
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        try {
            binCond.regenerateFieldSchema();
        } catch (FrontendException fee) {
            int errCode = 1040;
            String msg = "Could not set BinCond field schema";
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
        }

    }

    private void insertLeftCastForBinCond(LOBinCond binCond, byte toType) throws VisitorException {
        insertCast(binCond, toType, binCond.getLhsOp());
    }

    private void insertRightCastForBinCond(LOBinCond binCond, byte toType) throws VisitorException {
        insertCast(binCond, toType, binCond.getRhsOp());
    }

    /**
     * For Basic Types:
     * 0) Casting to itself is always ok
     * 1) Casting from number to number is always ok 
     * 2) ByteArray to anything is ok
     * 3) (number or chararray) to (bytearray or chararray) is ok
     * For Composite Types:
     * Recursively traverse the schemas till you get a basic type
     */
    @Override
    protected void visit(LOCast cast) throws VisitorException {      
        
        byte inputType = cast.getExpression().getType(); 
        byte expectedType = cast.getType();


        if(expectedType == DataType.BYTEARRAY) {
            int errCode = 1051;
            String msg = "Cannot cast to bytearray";
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ; 
        }
        
        Schema.FieldSchema castFs;
        Schema.FieldSchema inputFs;
        try {
            castFs = cast.getFieldSchema();
            inputFs = cast.getExpression().getFieldSchema();
        } catch(FrontendException fee) {
            int errCode = 1076;
            String msg = "Problem while reading field schema of cast operator.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG, fee);
        }
        boolean castable = Schema.FieldSchema.castable(castFs, inputFs);
        if(!castable) {
            int errCode = 1052;
            String msg = "Cannot cast "
                           + DataType.findTypeName(inputType)
                           + ((DataType.isSchemaType(inputType))? " with schema " + inputFs : "")
                           + " to "
                           + DataType.findTypeName(expectedType)
                           + ((DataType.isSchemaType(expectedType))? " with schema " + castFs : "");
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ; 
        }
        
        // cast.getType() already returns the correct type so don't have to 
        // set here. This is a special case where output type is not
        // automatically determined.
        
        if(inputType == DataType.BYTEARRAY) {
            try {
                FuncSpec loadFuncSpec = getLoadFuncSpec(cast.getExpression());
                cast.setLoadFuncSpec(loadFuncSpec);
            } catch (FrontendException fee) {
                int errCode = 1053;
                String msg = "Cannot resolve load function to use for casting from " + 
                            DataType.findTypeName(inputType) + " to " +
                            DataType.findTypeName(expectedType) + ". ";
                msgCollector.collect(msg, MessageType.Error);
                throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee); 
            }
        }
    }
    
    
    /***********************************************************************/
    /*                  Relational Operators                               */                                                             
    /***********************************************************************/
    /*
        All the getType() of these operators always return BAG.
        We just have to :-
        1) Check types of inputs, inner plans
        2) Compute output schema with type information
           (At the moment, the parser does only return GetSchema with correct aliases)
        3) Insert casting if necessary

     */

    /*
        The output schema of LOUnion is the merge of all input schemas.
        Operands on left side always take precedance on aliases.

        We allow type promotion here
    */

    @Override
    protected void visit(LOUnion u) throws VisitorException {
        u.unsetSchema();
        // Have to make a copy, because as we insert operators, this list will
        // change under us.
        List<LogicalOperator> inputs = 
            new ArrayList<LogicalOperator>(u.getInputs());

        // There is no point to union only one operand
        // it should be a problem in the parser
        if (inputs.size() < 2) {
            AssertionError err =  new AssertionError("Union with Count(Operand) < 2") ;
        }

        Schema schema = null ;
        try {

            if (strictMode) {
                // Keep merging one by one just to check if there is
                // any problem with types in strict mode
                Schema tmpSchema = inputs.get(0).getSchema() ;
                for (int i=1; i< inputs.size() ;i++) {
                    // Assume the first input's aliases take precedance
                    tmpSchema = tmpSchema.merge(inputs.get(i).getSchema(), false) ;

                    // if they cannot be merged, we just give up
                    if (tmpSchema == null) {
                        int errCode = 1054;
                        String msg = "Cannot merge schemas from inputs of UNION" ;
                        msgCollector.collect(msg, MessageType.Error) ;
                        throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                    }
                }
            }

            // Compute the schema
            schema = u.getSchema() ;

        }
        catch (FrontendException fee) {
            int errCode = 1055;
            String msg = "Problem while reading schemas from inputs of Union" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
        }

        // Do cast insertion only if we are typed
        if (schema != null) {
            // Insert casting to inputs if necessary
            for (int i=0; i< inputs.size() ;i++) {
                LOForEach insertedOp
                        = insertCastForEachInBetweenIfNecessary(inputs.get(i), u, schema) ;

                // We may have to compute the schema of the input again
                // because we have just inserted
                if (insertedOp != null) {
                    if(insertedOp.getAlias()==null){
                        insertedOp.setAlias(inputs.get(i).getAlias());
                    }
                    try {
                        this.visit(insertedOp);
                    }
                    catch (FrontendException fee) {
                        int errCode = 1056;
                        String msg = "Problem while casting inputs of Union" ;
                        msgCollector.collect(msg, MessageType.Error) ;
                        throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
                    }
                }
            }
        }
    }

    @Override
    protected void visit(LOSplitOutput op) throws VisitorException {
        op.unsetSchema();
        LogicalPlan currentPlan =  mCurrentWalker.getPlan() ;

        // LOSplitOutput can only have 1 input
        List<LogicalOperator> list = currentPlan.getPredecessors(op) ;
        if (list.size() != 1) {
            int errCode = 2008;
            String msg = "LOSplitOutput cannot have more than one input. Found: " + list.size() + " input(s).";
            throw new TypeCheckerException(msg, errCode, PigException.BUG) ;
        }

        LogicalOperator input = list.get(0);
        LogicalPlan condPlan = op.getConditionPlan() ;

        // Check that the inner plan has only 1 output port
        if (!condPlan.isSingleLeafPlan()) {
            int errCode = 1057;
            String msg = "Split's inner plan can only have one output (leaf)" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }
            
        checkInnerPlan(condPlan) ;
                 
        byte innerCondType = condPlan.getLeaves().get(0).getType() ;
        if (innerCondType != DataType.BOOLEAN) {
            int errCode = 1058;
            String msg = "Split's condition must evaluate to boolean. Found: " + DataType.findTypeName(innerCondType) ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        try {
            // Compute the schema
            op.getSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1055;
            String msg = "Problem while reading"
                         + " schemas from inputs of SplitOutput" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }


    /***
     *  LODistinct, output schema should be the same as input
     * @param op
     * @throws VisitorException
     */
    
    @Override
    protected void visit(LODistinct op) throws VisitorException {
        op.unsetSchema();
        LogicalPlan currentPlan = mCurrentWalker.getPlan() ;
        List<LogicalOperator> list = currentPlan.getPredecessors(op) ;

        try {
            // Compute the schema
            op.getSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1055;
            String msg = "Problem while reading"
                         + " schemas from inputs of Distinct" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    @Override
    protected void visit(LOLimit op) throws VisitorException {
        try {
            // Compute the schema
            op.regenerateSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1055;
            String msg = "Problem while reading"
                         + " schemas from inputs of Limit" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    /***
     * Return concatenated of all fields from all input operators
     * If one of the inputs have no schema then we cannot construct
     * the output schema.
     * @param cs
     * @throws VisitorException
     */
    protected void visit(LOCross cs) throws VisitorException {
        cs.unsetSchema();
        List<LogicalOperator> inputs = cs.getInputs() ;
        List<FieldSchema> fsList = new ArrayList<FieldSchema>() ;

        try {
            // Compute the schema
            cs.getSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1055;
            String msg = "Problem while reading"
                        + " schemas from inputs of Cross" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }
    
    /***
     * The schema of sort output will be the same as sort input.
     *
     */

    protected void visit(LOSort s) throws VisitorException {
        s.unsetSchema();
        LogicalOperator input = s.getInput() ;
        
        // Type checking internal plans.
        for(int i=0;i < s.getSortColPlans().size(); i++) {
            
            LogicalPlan sortColPlan = s.getSortColPlans().get(i) ;

            // Check that the inner plan has only 1 output port
            if (!sortColPlan.isSingleLeafPlan()) {
                int errCode = 1057;
                String msg = "Sort's inner plan can only have one output (leaf)" ;
                msgCollector.collect(msg, MessageType.Error) ;
                throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
            }

            checkInnerPlan(sortColPlan) ;
            // TODO: May have to check SortFunc compatibility here in the future
                       
        }
        
        s.setType(input.getType()) ;  // This should be bag always.

        try {
            // Compute the schema
            s.getSchema() ;
        }
        catch (FrontendException fee) {
            int errCode = 1059;
            String msg = "Problem while reconciling output schema of Sort" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
        }
    }


    /***
     * The schema of filter output will be the same as filter input
     */

    @Override
    protected void visit(LOFilter filter) throws VisitorException {
        filter.unsetSchema();
        LogicalOperator input = filter.getInput() ;
        LogicalPlan comparisonPlan = filter.getComparisonPlan() ;
        
        // Check that the inner plan has only 1 output port
        if (!comparisonPlan.isSingleLeafPlan()) {
            int errCode = 1057;
            String msg = "Filter's cond plan can only have one output (leaf)" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }

        checkInnerPlan(comparisonPlan) ;
              
        byte innerCondType = comparisonPlan.getLeaves().get(0).getType() ;
        if (innerCondType != DataType.BOOLEAN) {
            int errCode = 1058;
            String msg = "Filter's condition must evaluate to boolean. Found: " + DataType.findTypeName(innerCondType);
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
        }       


        try {
            // Compute the schema
            filter.getSchema() ;
        } 
        catch (FrontendException fe) {
            int errCode = 1059;
            String msg = "Problem while reconciling output schema of Filter" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    /***
     * The schema of split output will be the same as split input
     */

    protected void visit(LOSplit split) throws VisitorException {
        // TODO: Why doesn't LOSplit have getInput() ???
        List<LogicalOperator> inputList = mPlan.getPredecessors(split) ;
        
        if (inputList.size() != 1) {            
            int errCode = 2008;
            String msg = "LOSplit cannot have more than one input. Found: " + inputList.size() + " input(s).";
            throw new TypeCheckerException(msg, errCode, PigException.BUG) ;
        }
        
        LogicalOperator input = inputList.get(0) ;
        
        try {
            // Compute the schema
            split.regenerateSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1059;
            String msg = "Problem while reconciling output schema of Split" ;
            msgCollector.collect(msg, MessageType.Error);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }
    
    /**
     * Mimics the type checking of LOCogroup
     */
    protected void visit(LOFRJoin frj) throws VisitorException {
        try {
            frj.regenerateSchema();
        } catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve Fragment Replicate Join output schema" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
        
        MultiMap<LogicalOperator, LogicalPlan> joinColPlans
                                                    = frj.getJoinColPlans() ;
        List<LogicalOperator> inputs = frj.getInputs() ;
        
        // Type checking internal plans.
        for(int i=0;i < inputs.size(); i++) {
            LogicalOperator input = inputs.get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(joinColPlans.get(input)) ;

            for(int j=0; j < innerPlans.size(); j++) {

                LogicalPlan innerPlan = innerPlans.get(j) ;
                
                // Check that the inner plan has only 1 output port
                if (!innerPlan.isSingleLeafPlan()) {
                    int errCode = 1057;
                    String msg = "Fragment Replicate Join's inner plans can only"
                                 + "have one output (leaf)" ;
                    msgCollector.collect(msg, MessageType.Error) ;
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                }

                checkInnerPlan(innerPlans.get(j)) ;
            }
        }
        
        try {

            if (!frj.isTupleJoinCol()) {
                // merge all the inner plan outputs so we know what type
                // our group column should be

                // TODO: Don't recompute schema here
                //byte groupType = schema.getField(0).type ;
                byte groupType = frj.getAtomicJoinColType() ;

                // go through all inputs again to add cast if necessary
                for(int i=0;i < inputs.size(); i++) {
                    LogicalOperator input = inputs.get(i) ;
                    List<LogicalPlan> innerPlans
                                = new ArrayList<LogicalPlan>(joinColPlans.get(input)) ;
                    // Checking innerPlan size already done above
                    byte innerType = innerPlans.get(0).getSingleLeafPlanOutputType() ;
                    if (innerType != groupType) {
                        insertAtomicCastForFRJInnerPlan(innerPlans.get(0),
                                                            frj,
                                                            groupType) ;
                    }
                }
            }
            else {

                // TODO: Don't recompute schema here
                //Schema groupBySchema = schema.getField(0).schema ;
                Schema groupBySchema = frj.getTupleJoinColSchema() ;

                // go through all inputs again to add cast if necessary
                for(int i=0;i < inputs.size(); i++) {
                    LogicalOperator input = inputs.get(i) ;
                    List<LogicalPlan> innerPlans
                                = new ArrayList<LogicalPlan>(joinColPlans.get(input)) ;
                    for(int j=0;j < innerPlans.size(); j++) {
                        LogicalPlan innerPlan = innerPlans.get(j) ;
                        byte innerType = innerPlan.getSingleLeafPlanOutputType() ;
                        byte expectedType = DataType.BYTEARRAY ;

                        if (!DataType.isAtomic(innerType) && (DataType.TUPLE != innerType)) {
                            int errCode = 1057;
                            String msg = "Fragment Replicate Join's inner plans can only"
                                         + "have one output (leaf)" ;
                            msgCollector.collect(msg, MessageType.Error) ;
                            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                        }

                        try {
                            expectedType = groupBySchema.getField(j).type ;
                        }
                        catch(FrontendException fee) {
                            int errCode = 1060;
                            String msg = "Cannot resolve Fragment Replicate Join output schema" ;
                            msgCollector.collect(msg, MessageType.Error) ;
                            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
                        }

                        if (innerType != expectedType) {
                            insertAtomicCastForFRJInnerPlan(innerPlan,
                                                                frj,
                                                                expectedType) ;
                        }
                    }
                }
            }
        }
        catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve Fragment Replicate Join output schema" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }

        try {
            Schema outputSchema = frj.regenerateSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve Fragment Replicate Join output schema" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }

    /**
     * COGroup
     * All group by cols from all inputs have to be of the
     * same type
     */
    protected void visit(LOCogroup cg) throws VisitorException {
        try {
            cg.regenerateSchema();
        } catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve COGroup output schema" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
        
        MultiMap<LogicalOperator, LogicalPlan> groupByPlans
                                                    = cg.getGroupByPlans() ;
        List<LogicalOperator> inputs = cg.getInputs() ;

        // Type checking internal plans.
        for(int i=0;i < inputs.size(); i++) {
            LogicalOperator input = inputs.get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(groupByPlans.get(input)) ;

            for(int j=0; j < innerPlans.size(); j++) {

                LogicalPlan innerPlan = innerPlans.get(j) ;
                
                // Check that the inner plan has only 1 output port
                if (!innerPlan.isSingleLeafPlan()) {
                    int errCode = 1057;
                    String msg = "COGroup's inner plans can only"
                                 + "have one output (leaf)" ;
                    msgCollector.collect(msg, MessageType.Error) ;
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                }

                checkInnerPlan(innerPlans.get(j)) ;
            }

        }

        try {

            if (!cg.isTupleGroupCol()) {
                // merge all the inner plan outputs so we know what type
                // our group column should be

                // TODO: Don't recompute schema here
                //byte groupType = schema.getField(0).type ;
                byte groupType = cg.getAtomicGroupByType() ;

                // go through all inputs again to add cast if necessary
                for(int i=0;i < inputs.size(); i++) {
                    LogicalOperator input = inputs.get(i) ;
                    List<LogicalPlan> innerPlans
                                = new ArrayList<LogicalPlan>(groupByPlans.get(input)) ;
                    // Checking innerPlan size already done above
                    byte innerType = innerPlans.get(0).getSingleLeafPlanOutputType() ;
                    if (innerType != groupType) {
                        insertAtomicCastForCOGroupInnerPlan(innerPlans.get(0),
                                                            cg,
                                                            groupType) ;
                    }
                }
            }
            else {

                // TODO: Don't recompute schema here
                //Schema groupBySchema = schema.getField(0).schema ;
                Schema groupBySchema = cg.getTupleGroupBySchema() ;

                // go through all inputs again to add cast if necessary
                for(int i=0;i < inputs.size(); i++) {
                    LogicalOperator input = inputs.get(i) ;
                    List<LogicalPlan> innerPlans
                                = new ArrayList<LogicalPlan>(groupByPlans.get(input)) ;
                    for(int j=0;j < innerPlans.size(); j++) {
                        LogicalPlan innerPlan = innerPlans.get(j) ;
                        byte innerType = innerPlan.getSingleLeafPlanOutputType() ;
                        byte expectedType = DataType.BYTEARRAY ;

                        if (!DataType.isAtomic(innerType) && (DataType.TUPLE != innerType)) {
                            int errCode = 1061;
                            String msg = "Sorry, group by complex types"
                                       + " will be supported soon" ;
                            msgCollector.collect(msg, MessageType.Error) ;
                            throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                        }

                        try {
                            expectedType = groupBySchema.getField(j).type ;
                        }
                        catch(FrontendException fee) {
                            int errCode = 1060;
                            String msg = "Cannot resolve COGroup output schema" ;
                            msgCollector.collect(msg, MessageType.Error) ;
                            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
                        }

                        if (innerType != expectedType) {
                            insertAtomicCastForCOGroupInnerPlan(innerPlan,
                                                                cg,
                                                                expectedType) ;
                        }
                    }
                }
            }
        }
        catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve COGroup output schema" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }

        // TODO: Don't recompute schema here. Remove all from here!
        // Generate output schema based on the schema generated from
        // COGroup itself

        try {
            Schema outputSchema = cg.regenerateSchema() ;
        }
        catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve COGroup output schema" ;
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }
    
    private void insertAtomicCastForFRJInnerPlan(LogicalPlan innerPlan,
            LOFRJoin frj, byte toType) throws VisitorException {
        if (!DataType.isUsableType(toType)) {
            int errCode = 1051;
            String msg = "Cannot cast to "
                + DataType.findTypeName(toType);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT);
        }

        List<LogicalOperator> leaves = innerPlan.getLeaves();
        if (leaves.size() > 1) {
            int errCode = 2060;
            String msg = "Expected one leaf. Found " + leaves.size() + " leaves.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG);
        }
        ExpressionOperator currentOutput = (ExpressionOperator) leaves.get(0);
        collectCastWarning(frj, currentOutput.getType(), toType);
        OperatorKey newKey = genNewOperatorKey(currentOutput);
        LOCast cast = new LOCast(innerPlan, newKey, toType);
        innerPlan.add(cast);
        try {
            innerPlan.connect(currentOutput, cast);
        } catch (PlanException pe) {
            int errCode = 2059;
            String msg = "Problem with inserting cast operator for fragment replicate join in plan.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG, pe);
        }
        this.visit(cast);
    }

    // This helps insert casting to atomic types in COGroup's inner plans
    // as a new leave of the plan
    private void insertAtomicCastForCOGroupInnerPlan(LogicalPlan innerPlan,
                                                     LOCogroup cg,
                                                     byte toType) throws VisitorException {
        if(!DataType.isUsableType(toType)) {
            int errCode = 1051;
            String msg = "Cannot cast to "
                + DataType.findTypeName(toType);
            throw new TypeCheckerException(msg, errCode, PigException.INPUT);
        }
        
        List<LogicalOperator> leaves = innerPlan.getLeaves() ;
        if (leaves.size() > 1) {
            int errCode = 2060;
            String msg = "Expected one leaf. Found " + leaves.size() + " leaves.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG);
        }
        ExpressionOperator currentOutput = (ExpressionOperator) leaves.get(0) ;
        collectCastWarning(cg, currentOutput.getType(), toType) ;
        OperatorKey newKey = genNewOperatorKey(currentOutput) ;
        LOCast cast = new LOCast(innerPlan, newKey, toType) ;
        innerPlan.add(cast) ;
        try {
            innerPlan.connect(currentOutput, cast) ;
        }
        catch (PlanException pe) {
            int errCode = 2059;
            String msg = "Problem with inserting cast operator for cogroup in plan.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG, pe);
        }
        this.visit(cast);
    }

    /**
     * This can be used to get the merged type of output group col
     * only when the group col is of atomic type
     * TODO: This doesn't work with group by complex type
     * @return The type of the group by
     */
    public byte getAtomicGroupByType(LOCogroup cg) throws VisitorException {
        if (cg.isTupleGroupCol()) {
            int errCode = 2061;
            String msg = "Expected single group by element but found multiple elements.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG);
        }
        byte groupType = DataType.BYTEARRAY ;
        // merge all the inner plan outputs so we know what type
        // our group column should be
        for(int i=0;i < cg.getInputs().size(); i++) {
            LogicalOperator input = cg.getInputs().get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(cg.getGroupByPlans().get(input)) ;
            if (innerPlans.size() != 1) {
                int errCode = 2062;
                String msg = "Each COGroup input has to have "
                    + "the same number of inner plans.";
                throw new TypeCheckerException(msg, errCode, PigException.BUG) ;
            }
            byte innerType = innerPlans.get(0).getSingleLeafPlanOutputType() ;
            groupType = DataType.mergeType(groupType, innerType) ;
            if (groupType == DataType.ERROR) {
                // We just warn about mismatch type in non-strict mode
                if (!strictMode) {
                    String msg = "COGroup by incompatible types results in ByteArray" ;
                    msgCollector.collect(msg, MessageType.Warning, PigWarning.GROUP_BY_INCOMPATIBLE_TYPES) ;
                    groupType = DataType.BYTEARRAY ;
                }
                // We just die if in strict mode
                else {
                    int errCode = 1062;
                    String msg = "COGroup by incompatible types" ;
                    msgCollector.collect(msg, MessageType.Error) ;
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                }
            }


        }

        return groupType ;

    }

    /*
        This implementation is based on the assumption that all the
        inputs have the same group col tuple arity.
        TODO: This doesn't work with group by complex type
     */
    public Schema getTupleGroupBySchema(LOCogroup cg) throws VisitorException {
        if (!cg.isTupleGroupCol()) {
            int errCode = 2063;
            String msg = "Expected multiple group by element but found single element.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG);
        }

        // this fsList represents all the columns in group tuple
        List<Schema.FieldSchema> fsList = new ArrayList<Schema.FieldSchema>() ;

        int outputSchemaSize = cg.getGroupByPlans().get(cg.getInputs().get(0)).size() ;

        // by default, they are all bytearray
        // for type checking, we don't care about aliases
        for(int i=0; i<outputSchemaSize; i++) {
            fsList.add(new Schema.FieldSchema(null, DataType.BYTEARRAY)) ;
        }

        // merge all the inner plan outputs so we know what type
        // our group column should be
        for(int i=0;i < cg.getInputs().size(); i++) {
            LogicalOperator input = cg.getInputs().get(i) ;
            List<LogicalPlan> innerPlans
                        = new ArrayList<LogicalPlan>(cg.getGroupByPlans().get(input)) ;

            for(int j=0;j < innerPlans.size(); j++) {
                byte innerType = innerPlans.get(j).getSingleLeafPlanOutputType() ;
                fsList.get(j).type = DataType.mergeType(fsList.get(j).type,
                                                        innerType) ;
                if (fsList.get(j).type == DataType.ERROR) {
                    // We just warn about mismatch type in non-strict mode
                    if (!strictMode) {
                        String msg = "COGroup by incompatible types results in ByteArray" ;
                        msgCollector.collect(msg, MessageType.Warning, PigWarning.GROUP_BY_INCOMPATIBLE_TYPES) ;
                        fsList.get(j).type = DataType.BYTEARRAY ;
                    }
                    // We just die if in strict mode
                    else {
                        int errCode = 1062;
                        String msg = "COGroup by incompatible types" ;
                        msgCollector.collect(msg, MessageType.Error) ;
                        throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                    }
                }
            }
        }

        return new Schema(fsList) ;
    }

    /***
     * Output schema of LOForEach is a tuple schma
     * which is the output of all inner plans
     *
     * Flatten also has to be taken care on in here
     *
     */

    protected void visit(LOForEach f) throws VisitorException {
        List<LogicalPlan> plans = f.getForEachPlans() ;
        List<Boolean> flattens = f.getFlatten() ;

        f.unsetSchema();
        try {

            // Have to resolve all inner plans before calling getSchema
            int outputSchemaIdx = 0 ;
            for(int i=0;i < plans.size(); i++) {

                LogicalPlan plan = plans.get(i) ;

                // Check that the inner plan has only 1 output port
                if (!plan.isSingleLeafPlan()) {
                    int errCode = 1057;
                    String msg = "Generate's expression plan can "
                                 + " only have one output (leaf)" ;
                    msgCollector.collect(msg, MessageType.Error) ;
                    throw new TypeCheckerException(msg, errCode, PigException.INPUT) ;
                }

                List<LogicalOperator> rootList = plan.getRoots() ;
                for(int j=0; j<rootList.size(); j++) {
                    LogicalOperator innerRoot = rootList.get(j) ;
                    // TODO: Support MAP dereference
                    if (innerRoot instanceof LOProject) {
                        resolveLOProjectType((LOProject) innerRoot) ;
                    }
                    else if (innerRoot instanceof LOConst) {
                        // it's ok because LOConst always has
                        // the right type information
                    }
                    else {
                        int errCode = 2064;
                        String msg = "Unsupported root type in "
                            +"LOForEach: " + innerRoot.getClass().getSimpleName();
                        throw new TypeCheckerException(msg, errCode, PigException.BUG) ;
                    }
                }

                checkInnerPlan(plan) ;

            }

            f.getSchema();

        } catch (VisitorException ve) {
            throw ve;
        } catch (FrontendException fe) {
            int errCode = 1060;
            String msg = "Cannot resolve ForEach output schema.";
            msgCollector.collect(msg, MessageType.Error) ;
            throw new TypeCheckerException(msg, errCode, PigException.INPUT, fe) ;
        }
    }
    
    /***
     * This does:-
     * 1) Propagate typing information from a schema to
     *    an inner plan of a relational operator
     * 2) Type checking of the inner plan
     * NOTE: This helper method only supports one source schema
     * 
     * @param innerPlan  the inner plan
     * @throws VisitorException
     */

    private void checkInnerPlan(LogicalPlan innerPlan)
                                    throws VisitorException {
        // Preparation
        int errorCount = 0 ;     
        List<LogicalOperator> rootList = innerPlan.getRoots() ;
        if (rootList.size() < 1) {
            int errCode = 2065;
            String msg = "Did not find roots of the inner plan.";
            throw new TypeCheckerException(msg, errCode, PigException.BUG) ;
        }

        /*
        Schema inputSchema = null ;
        try {
            inputSchema = srcOuterOp.getSchema() ;
        }
        catch(FrontendException fe) {
            String msg = "Cannot not get schema out of "
                         + srcOuterOp.getClass().getSimpleName() ;
            msgCollector.collect(msg, MessageType.Error);
            throw new VisitorException(msg) ;
        }
        */

        // Actual checking
        for(LogicalOperator op: rootList) {
            // TODO: Support map dereference
            if (op instanceof LOProject) {
                resolveLOProjectType((LOProject) op);
            }
            // TODO: I think we better need a sentinel connecting to LOForEach
            else if (op instanceof LOForEach) {
                op.setType(DataType.TUPLE);
                PlanWalker<LogicalOperator, LogicalPlan> walker
                        = new DependencyOrderWalker<LogicalOperator, LogicalPlan>(innerPlan) ;
                pushWalker(walker) ;
                this.visit((LOForEach) op) ;
                popWalker() ;
            }
            else if (op instanceof LOConst) {
                // don't have to do anything
            }
            else if (op instanceof LOUserFunc){
                visit((LOUserFunc)op);
            }
            else {
                int errCode = 2066;                
                String msg = "Unsupported root operator in inner plan:"
                             + op.getClass().getSimpleName() ;
                throw new TypeCheckerException(msg, errCode, PigException.BUG) ;
            }
        }
        
        // Throw an exception if we found errors
        // TODO: add it back
        /*
        if (errorCount > 0) {
            // TODO: Should indicate the field names or indexes here
            String msg =  "Some required fields in inner plan cannot be found in input" ;
            msgCollector.collect(msg, MessageType.Error);
            VisitorException vse = new VisitorException(msg) ;
            throw vse ;
        } */
        
        // Check typing of the inner plan by visiting it
        PlanWalker<LogicalOperator, LogicalPlan> walker
            = new DependencyOrderWalker<LogicalOperator, LogicalPlan>(innerPlan) ;
        pushWalker(walker) ;       
        this.visit() ; 
        popWalker() ;
        
    }

    protected void visit(LOLoad load)
                        throws VisitorException {
        // do nothing
    }

    protected void visit(LOStore store) {
        // do nothing
    }


    /***
     * For casting insertion for relational operators
     * only if it's necessary
     * Currently this only does "shallow" casting
     * @param fromOp
     * @param toOp
     * @param targetSchema array of target types
     * @return the inserted operator. null is no insertion
     */
    private LOForEach insertCastForEachInBetweenIfNecessary(LogicalOperator fromOp,
                                                       LogicalOperator toOp,
                                                       Schema targetSchema)
                                                    throws VisitorException {
        LogicalPlan currentPlan =  mCurrentWalker.getPlan() ;

        /*
        // Make sure that two operators are in the same plan
        if (fromOp.getPlan() != toOp.getPlan()) {
            throw new AssertionError("Two operators have toOp be in the same plan") ;
        }
        // Mare sure that they are in the plan we're looking at
        if (fromOp.getPlan() != toOp.getPlan()) {
            throw new AssertionError("Cannot manipulate any other plan"
                                    +" than the current one") ;
        }
        */

        // Make sure that they are adjacent and the direction
        // is from "fromOp" to "toOp"
        List<LogicalOperator> preList = currentPlan.getPredecessors(toOp) ;
        boolean found = false ;
        for(LogicalOperator tmpOp: preList) {
            // compare by reference
            if (tmpOp == fromOp) {
                found = true ;
                break ;
            }
        }

        if (!found) {
            int errCode = 1077;
            String msg = "Two operators that require a cast in between are not adjacent.";
            throw new TypeCheckerException(msg, errCode, PigException.INPUT);
        }

        // retrieve input schema to be casted
        // this will be used later
        Schema fromSchema = null ;
        try {
            fromSchema = fromOp.getSchema() ;
        }
        catch(FrontendException fe) {
            int errCode = 1055;
            String msg = "Problem while reading schema from input of " + fromOp.getClass().getSimpleName();
            throw new TypeCheckerException(msg, errCode, PigException.BUG, fe);
        }

        // make sure the supplied targetSchema has the same number of members
        // as number of output fields from "fromOp"
        if (fromSchema.size() != targetSchema.size()) {
            int errCode = 1078;
            String msg = "Schema size mismatch for casting. Input schema size: " + fromSchema.size() + ". Target schema size: " + targetSchema.size();
            throw new TypeCheckerException(msg, errCode, PigException.INPUT);
        }

        // Compose the new inner plan to be used in ForEach
        LogicalPlan foreachPlan = new LogicalPlan() ;

        // Plans inside Generate. Fields that do not need casting will only
        // have Project. Fields that need casting will have Project + Cast
        ArrayList<LogicalPlan> generatePlans = new ArrayList<LogicalPlan>() ;

        int castNeededCounter = 0 ;
        for(int i=0;i < fromSchema.size(); i++) {

            LogicalPlan genPlan = new LogicalPlan() ;
            LOProject project = new LOProject(genPlan,
                                              genNewOperatorKey(fromOp),
                                              fromOp,
                                              i) ;
            project.setSentinel(true);
            genPlan.add(project);

            // add casting if necessary by comparing target types
            // to the input schema
            FieldSchema fs = null ;
            try {
                fs = fromSchema.getField(i) ;
            }
            catch(FrontendException fee) {
                int errCode = 1063;
                String msg = "Problem while reading"
                                + " field schema from input while"
                                + " inserting cast " ;
                msgCollector.collect(msg, MessageType.Error) ;
                throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;
            }

            // This only does "shallow checking"

            byte inputFieldType ;

            try {
                inputFieldType = targetSchema.getField(i).type ;
            }
            catch (FrontendException fee) {
                int errCode = 1064;
                String msg = "Problem reading column " + i + " from schema: " + targetSchema;
                throw new TypeCheckerException(msg, errCode, PigException.INPUT, fee) ;                
            }

            if (inputFieldType != fs.type) {
                castNeededCounter++ ;
                LOCast cast = new LOCast(genPlan,
                                         genNewOperatorKey(fromOp),
                                         inputFieldType) ;
                genPlan.add(cast) ;
                try {
                    genPlan.connect(project, cast);
                }
                catch (PlanException pe) {
                    // This should never happen
                    int errCode = 2059;
                    String msg = "Problem with inserting cast operator for project in plan.";
                    throw new TypeCheckerException(msg, errCode, PigException.BUG, pe);
                }
            }

            generatePlans.add(genPlan) ;

        }

        // if we really need casting
        if (castNeededCounter > 0)  {
            // Flatten List
            // This is just cast insertion so we don't have any flatten
            ArrayList<Boolean> flattenList = new ArrayList<Boolean>() ;
            for(int i=0;i < targetSchema.size(); i++) {
                flattenList.add(new Boolean(false)) ;
            }

            // Create ForEach to be inserted
            LOForEach foreach = new LOForEach(currentPlan, genNewOperatorKey(fromOp), generatePlans, flattenList) ;

            // Manipulate the plan structure
            currentPlan.add(foreach);
            currentPlan.disconnect(fromOp, toOp) ;

            try {
                currentPlan.connect(fromOp, foreach);
                currentPlan.connect(foreach, toOp);
            }
            catch (PlanException pe) {
                int errCode = 2059;
                String msg = "Problem with inserting foeach operator for " + toOp.getClass().getSimpleName() + " in plan.";
                throw new TypeCheckerException(msg, errCode, PigException.BUG, pe);
            }

            return foreach;
            
        }
        else {
            log.debug("Tried to insert relational casting when not necessary");
            return null ;
        }
    }

    /***
     * Helper for collecting warning when casting is inserted
     * to the plan (implicit casting)
     *
     * @param op
     * @param originalType
     * @param toType
     */
    private void collectCastWarning(LogicalOperator op,
                                   byte originalType,
                                   byte toType) {
        String originalTypeName = DataType.findTypeName(originalType) ;
        String toTypeName = DataType.findTypeName(toType) ;
        String opName = op.getClass().getSimpleName() ;
        Enum kind = null;
        switch(toType) {
        case DataType.BAG:
        	kind = PigWarning.IMPLICIT_CAST_TO_BAG;
        	break;
        case DataType.CHARARRAY:
        	kind = PigWarning.IMPLICIT_CAST_TO_CHARARRAY;
        	break;
        case DataType.DOUBLE:
        	kind = PigWarning.IMPLICIT_CAST_TO_DOUBLE;
        	break;
        case DataType.FLOAT:
        	kind = PigWarning.IMPLICIT_CAST_TO_FLOAT;
        	break;
        case DataType.INTEGER:
        	kind = PigWarning.IMPLICIT_CAST_TO_INT;
        	break;
        case DataType.LONG:
        	kind = PigWarning.IMPLICIT_CAST_TO_LONG;
        	break;
        case DataType.MAP:
        	kind = PigWarning.IMPLICIT_CAST_TO_MAP;
        	break;
        case DataType.TUPLE:
        	kind = PigWarning.IMPLICIT_CAST_TO_TUPLE;
        	break;
        }
        msgCollector.collect(originalTypeName + " is implicitly cast to "
                             + toTypeName +" under " + opName + " Operator",
                             MessageType.Warning, kind) ;
    }

    /***
     * We need the neighbor to make sure that the new key is in the same scope
     */
    private OperatorKey genNewOperatorKey(LogicalOperator neighbor) {
        String scope = neighbor.getOperatorKey().getScope() ;
        long newId = NodeIdGenerator.getGenerator().getNextNodeId(scope) ;
        return new OperatorKey(scope, newId) ;
    }

    private FuncSpec getLoadFuncSpec(ExpressionOperator exOp) throws FrontendException {
        Schema.FieldSchema fs = ((ExpressionOperator)exOp).getFieldSchema();
        if(null == fs) {
            return null;
        }

        Map<String, LogicalOperator> canonicalMap = fs.getCanonicalMap();
        MultiMap<LogicalOperator, String> reverseCanonicalMap = fs.getReverseCanonicalMap();
        MultiMap<String, FuncSpec> loadFuncSpecMap = new MultiMap<String, FuncSpec>();
        
        if(canonicalMap.keySet().size() > 0) {
            for(String parentCanonicalName: canonicalMap.keySet()) {
                FuncSpec lfSpec = getLoadFuncSpec(exOp, parentCanonicalName);
                if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
            }
        } else {
            for(LogicalOperator op: reverseCanonicalMap.keySet()) {
                for(String parentCanonicalName: reverseCanonicalMap.get(op)) {
                    FuncSpec lfSpec = getLoadFuncSpec(op, parentCanonicalName);
                    if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                }
            }
        }
        if(loadFuncSpecMap.keySet().size() == 0) {
            return null;
        }
        if(loadFuncSpecMap.keySet().size() == 1) {
            String lfString = loadFuncSpecMap.keySet().iterator().next();
            return loadFuncSpecMap.get(lfString).iterator().next();
        }

        {
            int errCode = 1065;
            String msg = "Found more than one load function to use: " + loadFuncSpecMap.keySet();
            throw new FrontendException(msg, errCode, PigException.INPUT);
        }
    }

    private FuncSpec getLoadFuncSpec(LogicalOperator op, String parentCanonicalName) throws FrontendException {
        MultiMap<String, FuncSpec> loadFuncSpecMap = new MultiMap<String, FuncSpec>();
        if(op instanceof ExpressionOperator) {
            if(op instanceof LOUserFunc) {
                return null;
            }
            
            Schema.FieldSchema fs = ((ExpressionOperator)op).getFieldSchema();
            Map<String, LogicalOperator> canonicalMap = fs.getCanonicalMap();
            MultiMap<LogicalOperator, String> reverseCanonicalMap = fs.getReverseCanonicalMap();
            
            if(canonicalMap.keySet().size() > 0) {
                for(String canonicalName: canonicalMap.keySet()) {
                    FuncSpec lfSpec = getLoadFuncSpec(fs, canonicalName);
                    if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                }
            } else {
                for(LogicalOperator lop: reverseCanonicalMap.keySet()) {
                    for(String canonicalName: reverseCanonicalMap.get(lop)) {
                        FuncSpec lfSpec = getLoadFuncSpec(fs, canonicalName);
                        if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                    }
                }
            }
        } else {
            if(op instanceof LOLoad) {
                return ((LOLoad)op).getInputFile().getFuncSpec();
            } else if (op instanceof LOStream) {
                StreamingCommand command = ((LOStream)op).getStreamingCommand();
                HandleSpec streamOutputSpec = command.getOutputSpec(); 
                FuncSpec streamLoaderSpec = new FuncSpec(streamOutputSpec.getSpec());
                return streamLoaderSpec;
            } else if ((op instanceof LOFilter)
                    || (op instanceof LODistinct)
                    || (op instanceof LOSort)
                    || (op instanceof LOSplit)
                    || (op instanceof LOSplitOutput)
                    || (op instanceof LOLimit)) {
                LogicalPlan lp = op.getPlan();
                return getLoadFuncSpec(lp.getPredecessors(op).get(0), parentCanonicalName);        
            }
            
            Schema s = op.getSchema();
            if(null != s) {
                for(Schema.FieldSchema fs: s.getFields()) {
                    if(null != parentCanonicalName && (parentCanonicalName.equals(fs.canonicalName))) {
                        if(fs.getCanonicalMap().keySet().size() > 0) {
                            for(String canonicalName: fs.getCanonicalMap().keySet()) {
                                FuncSpec lfSpec = getLoadFuncSpec(fs, canonicalName);
                                if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                            }
                        } else {
                            FuncSpec lfSpec = getLoadFuncSpec(fs, null);
                            if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                        }
                    } else if (null == parentCanonicalName) {
                        FuncSpec lfSpec = getLoadFuncSpec(fs, null);
                        if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                    }
                }
            } else {
                LogicalPlan lp = op.getPlan();
                for(LogicalOperator pred: lp.getPredecessors(op)) {
                    FuncSpec lfSpec = getLoadFuncSpec(pred, parentCanonicalName);
                    if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                }
            }
        }
        if(loadFuncSpecMap.keySet().size() == 0) {
            return null;
        }
        if(loadFuncSpecMap.keySet().size() == 1) {
            String lfString = loadFuncSpecMap.keySet().iterator().next();
            return loadFuncSpecMap.get(lfString).iterator().next();
        }
    
        {
            int errCode = 1065;
            String msg = "Found more than one load function to use: " + loadFuncSpecMap.keySet();
            throw new FrontendException(msg, errCode, PigException.INPUT);
        }
    }

    private FuncSpec getLoadFuncSpec(Schema.FieldSchema fs, String parentCanonicalName) throws FrontendException {
        if(null == fs) {
            return null;
        }
        Map<String, LogicalOperator> canonicalMap = fs.getCanonicalMap();
        MultiMap<LogicalOperator, String> reverseCanonicalMap = fs.getReverseCanonicalMap();
        MultiMap<String, FuncSpec> loadFuncSpecMap = new MultiMap<String, FuncSpec>();

        if(canonicalMap.keySet().size() > 0) {
            for(String canonicalName: canonicalMap.keySet()) {
                if((null == parentCanonicalName) || (parentCanonicalName.equals(canonicalName))) {
                    FuncSpec lfSpec = getLoadFuncSpec(canonicalMap.get(canonicalName), parentCanonicalName);
                    if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                }
            }
        } else {
            for(LogicalOperator op: reverseCanonicalMap.keySet()) {
                for(String canonicalName: reverseCanonicalMap.get(op)) {
                    if((null == parentCanonicalName) || (parentCanonicalName.equals(canonicalName))) {
                        FuncSpec lfSpec = getLoadFuncSpec(op, parentCanonicalName);
                        if(null != lfSpec) loadFuncSpecMap.put(lfSpec.getClassName(), lfSpec);
                    }
                }
            }
        }
        if(loadFuncSpecMap.keySet().size() == 0) {
            return null;
        }
        if(loadFuncSpecMap.keySet().size() == 1) {
            String lfString = loadFuncSpecMap.keySet().iterator().next();
            return loadFuncSpecMap.get(lfString).iterator().next();
        }

        {
            int errCode = 1065;
            String msg = "Found more than one load function to use: " + loadFuncSpecMap.keySet();
            throw new FrontendException(msg, errCode, PigException.INPUT);
        }
    }


}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOGreaterThanEqual extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOGreaterThanEqual.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOGreaterThanEqual(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "GreaterThanEqual " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig;

import java.io.*;
import java.util.*;
import java.util.jar.*;
import java.text.ParseException;

import jline.ConsoleReader;
import jline.ConsoleReaderInputStream;
import jline.History;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import org.apache.log4j.Level;
import org.apache.log4j.PropertyConfigurator;
import org.apache.pig.ExecType;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.logicalLayer.LogicalPlanBuilder;
import org.apache.pig.impl.util.JarManager;
import org.apache.pig.impl.util.ObjectSerializer;
import org.apache.pig.impl.util.PropertiesUtil;
import org.apache.pig.tools.cmdline.CmdLineParser;
import org.apache.pig.tools.grunt.Grunt;
import org.apache.pig.impl.util.LogUtils;
import org.apache.pig.tools.timer.PerformanceTimerFactory;
import org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor;

public class Main
{

    private final static Log log = LogFactory.getLog(Main.class);
    
    private static final String LOG4J_CONF = "log4jconf";
    private static final String BRIEF = "brief";
    private static final String DEBUG = "debug";
    private static final String JAR = "jar";
    private static final String VERBOSE = "verbose";
    
    private enum ExecMode {STRING, FILE, SHELL, UNKNOWN};
                
/**
 * The Main-Class for the Pig Jar that will provide a shell and setup a classpath appropriate
 * for executing Jar files.
 * 
 * @param args
 *            -jar can be used to add additional jar files (colon separated). - will start a
 *            shell. -e will execute the rest of the command line as if it was input to the
 *            shell.
 * @throws IOException
 */
public static void main(String args[])
{
    int rc = 1;
    Properties properties = new Properties();
    PropertiesUtil.loadPropertiesFromFile(properties);
    
    boolean verbose = false;
    boolean gruntCalled = false;
    String logFileName = null;

    try {
        BufferedReader pin = null;
        boolean debug = false;
        boolean dryrun = false;
        ArrayList<String> params = new ArrayList<String>();
        ArrayList<String> paramFiles = new ArrayList<String>();
        HashSet<String> optimizerRules = new HashSet<String>();

        CmdLineParser opts = new CmdLineParser(args);
        opts.registerOpt('4', "log4jconf", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('b', "brief", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('c', "cluster", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('d', "debug", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('e', "execute", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('f', "file", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('h', "help", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('i', "version", CmdLineParser.ValueExpected.OPTIONAL);
        opts.registerOpt('j', "jar", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('l', "logfile", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('m', "param_file", CmdLineParser.ValueExpected.OPTIONAL);
        opts.registerOpt('o', "hod", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('p', "param", CmdLineParser.ValueExpected.OPTIONAL);
        opts.registerOpt('r', "dryrun", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('t', "optimizer_off", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('v', "verbose", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('w', "warning", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('x', "exectype", CmdLineParser.ValueExpected.REQUIRED);
        opts.registerOpt('F', "stop_on_failure", CmdLineParser.ValueExpected.NOT_ACCEPTED);
        opts.registerOpt('M', "no_multiquery", CmdLineParser.ValueExpected.NOT_ACCEPTED);

        ExecMode mode = ExecMode.UNKNOWN;
        String file = null;
        ExecType execType = ExecType.MAPREDUCE ;
        String execTypeString = properties.getProperty("exectype");
        if(execTypeString!=null && execTypeString.length()>0){
            execType = PigServer.parseExecType(execTypeString);
        }
        String cluster = "local";
        String clusterConfigured = properties.getProperty("cluster");
        if(clusterConfigured != null && clusterConfigured.length() > 0){
            cluster = clusterConfigured;
        }
        
        //by default warning aggregation is on
        properties.setProperty("aggregate.warning", ""+true);

        //by default multiquery optimization is on
        properties.setProperty("opt.multiquery", ""+true);

        //by default we keep going on error on the backend
        properties.setProperty("stop.on.failure", ""+false);

        char opt;
        while ((opt = opts.getNextOpt()) != CmdLineParser.EndOfOpts) {
            switch (opt) {
            case '4':
                String log4jconf = opts.getValStr();
                if(log4jconf != null){
                    properties.setProperty(LOG4J_CONF, log4jconf);
                }
                break;

            case 'b':
                properties.setProperty(BRIEF, "true");
                break;

            case 'c': 
                // Needed away to specify the cluster to run the MR job on
                // Bug 831708 - fixed
                String clusterParameter = opts.getValStr();
                if (clusterParameter != null && clusterParameter.length() > 0) {
                    cluster = clusterParameter;
                }
                break;

            case 'd':
                String logLevel = opts.getValStr();
                if (logLevel != null) {
                    properties.setProperty(DEBUG, logLevel);
                }
                debug = true;
                break;
                
            case 'e': 
                mode = ExecMode.STRING;
                break;

            case 'f':
                mode = ExecMode.FILE;
                file = opts.getValStr();
                break;

            case 'F':
                properties.setProperty("stop.on.failure", ""+true);
                break;

            case 'h':
                usage();
                return;

            case 'i':
            	System.out.println(getVersionString());
            	return;

            case 'j': 
                String jarsString = opts.getValStr();
                if(jarsString != null){
                    properties.setProperty(JAR, jarsString);
                }
                break;

            case 'l':
                //call to method that validates the path to the log file 
                //and sets up the file to store the client side log file                
                String logFileParameter = opts.getValStr();
                if (logFileParameter != null && logFileParameter.length() > 0) {
                    logFileName = validateLogFile(logFileParameter, null);
                } else {
                    logFileName = validateLogFile(logFileName, null);
                }
                properties.setProperty("pig.logfile", logFileName);
                break;

            case 'm':
                paramFiles.add(opts.getValStr());
                break;

            case 'M':
                // turns off multiquery optimization
                properties.setProperty("opt.multiquery",""+false);
                break;
                            
            case 'o': 
                // TODO sgroschupf using system properties is always a very bad idea
                String gateway = System.getProperty("ssh.gateway");
                if (gateway == null || gateway.length() == 0) {
                    properties.setProperty("hod.server", "local");
                } else {
                    properties.setProperty("hod.server", System.getProperty("ssh.gateway"));
                }
                break;

            case 'p': 
                String val = opts.getValStr();
                params.add(opts.getValStr());
                break;
                            
            case 'r': 
                // currently only used for parameter substitution
                // will be extended in the future
                dryrun = true;
                break;

            case 't':
            	optimizerRules.add(opts.getValStr());
                break;
                            
            case 'v':
                properties.setProperty(VERBOSE, ""+true);
                verbose = true;
                break;

            case 'w':
                properties.setProperty("aggregate.warning", ""+false);
                break;

            case 'x':
                try {
                    execType = PigServer.parseExecType(opts.getValStr());
                    } catch (IOException e) {
                        throw new RuntimeException("ERROR: Unrecognized exectype.", e);
                    }
                break;
            default: {
                Character cc = new Character(opt);
                throw new AssertionError("Unhandled option " + cc.toString());
                     }
            }
        }
        // configure logging
        configureLog4J(properties);
        // create the context with the parameter
        PigContext pigContext = new PigContext(execType, properties);
        
        if(logFileName == null) {
            logFileName = validateLogFile(null, null);
        }
        
        pigContext.getProperties().setProperty("pig.logfile", logFileName);
        
        if(optimizerRules.size() > 0) {
        	pigContext.getProperties().setProperty("pig.optimizer.rules", ObjectSerializer.serialize(optimizerRules));
        }

        LogicalPlanBuilder.classloader = pigContext.createCl(null);

        // construct the parameter substitution preprocessor
        Grunt grunt = null;
        BufferedReader in;
        String substFile = null;
        switch (mode) {
        case FILE: {
            // Run, using the provided file as a pig file
            in = new BufferedReader(new FileReader(file));

            // run parameter substitution preprocessor first
            substFile = file + ".substituted";
            pin = runParamPreprocessor(in, params, paramFiles, substFile, debug || dryrun);
            if (dryrun) {
                log.info("Dry run completed. Substituted pig script is at " + substFile);
                return;
            }

            logFileName = validateLogFile(logFileName, file);
            pigContext.getProperties().setProperty("pig.logfile", logFileName);

            // Set job name based on name of the script
            pigContext.getProperties().setProperty(PigContext.JOB_NAME, 
                                                   "PigLatin:" +new File(file).getName()
            );
            
            if (!debug) {
                new File(substFile).deleteOnExit();
            }
            
            grunt = new Grunt(pin, pigContext);
            gruntCalled = true;
            int results[] = grunt.exec();
            rc = getReturnCodeForStats(results);
            return;
        }

        case STRING: {
            // Gather up all the remaining arguments into a string and pass them into
            // grunt.
            StringBuffer sb = new StringBuffer();
            String remainders[] = opts.getRemainingArgs();
            for (int i = 0; i < remainders.length; i++) {
                if (i != 0) sb.append(' ');
                sb.append(remainders[i]);
            }
            in = new BufferedReader(new StringReader(sb.toString()));
            grunt = new Grunt(in, pigContext);
            gruntCalled = true;
            int results[] = grunt.exec();
            rc = getReturnCodeForStats(results);
            return;
            }

        default:
            break;
        }

        // If we're here, we don't know yet what they want.  They may have just
        // given us a jar to execute, they might have given us a pig script to
        // execute, or they might have given us a dash (or nothing) which means to
        // run grunt interactive.
        String remainders[] = opts.getRemainingArgs();
        if (remainders == null) {
            // Interactive
            mode = ExecMode.SHELL;
            ConsoleReader reader = new ConsoleReader(System.in, new OutputStreamWriter(System.out));
            reader.setDefaultPrompt("grunt> ");
            final String HISTORYFILE = ".pig_history";
            String historyFile = System.getProperty("user.home") + File.separator  + HISTORYFILE;
            reader.setHistory(new History(new File(historyFile)));
            ConsoleReaderInputStream inputStream = new ConsoleReaderInputStream(reader);
            grunt = new Grunt(new BufferedReader(new InputStreamReader(inputStream)), pigContext);
            grunt.setConsoleReader(reader);
            gruntCalled = true;
            grunt.run();
            rc = 0;
            return;
        } else {
            // They have a pig script they want us to run.
            if (remainders.length > 1) {
                   throw new RuntimeException("You can only run one pig script "
                    + "at a time from the command line.");
            }
            mode = ExecMode.FILE;
            in = new BufferedReader(new FileReader(remainders[0]));

            // run parameter substitution preprocessor first
            substFile = remainders[0] + ".substituted";
            pin = runParamPreprocessor(in, params, paramFiles, substFile, debug || dryrun);
            if (dryrun){
                log.info("Dry run completed. Substituted pig script is at " + substFile);
                return;
            }
            
            logFileName = validateLogFile(logFileName, remainders[0]);
            pigContext.getProperties().setProperty("pig.logfile", logFileName);

            if (!debug) {
                new File(substFile).deleteOnExit();
            }

            // Set job name based on name of the script
            pigContext.getProperties().setProperty(PigContext.JOB_NAME, 
                                                   "PigLatin:" +new File(remainders[0]).getName()
            );

            grunt = new Grunt(pin, pigContext);
            gruntCalled = true;
            int[] results = grunt.exec();
            rc = getReturnCodeForStats(results);
            return;
        }

        // Per Utkarsh and Chris invocation of jar file via pig depricated.
    } catch (ParseException e) {
        usage();
        rc = 2;
    } catch (NumberFormatException e) {
        usage();
        rc = 2;
    } catch (PigException pe) {
        if(pe.retriable()) {
            rc = 1; 
        } else {
            rc = 2;
        }

        if(!gruntCalled) {
        	LogUtils.writeLog(pe, logFileName, log, verbose);
        }
    } catch (Throwable e) {
        rc = 2;
        if(!gruntCalled) {
        	LogUtils.writeLog(e, logFileName, log, verbose);
        }
    } finally {
        // clear temp files
        FileLocalizer.deleteTempFiles();
        PerformanceTimerFactory.getPerfTimerFactory().dumpTimers();
        System.exit(rc);
    }
}

private static int getReturnCodeForStats(int[] stats) {
    if (stats[1] == 0) {
        // no failed jobs
        return 0;
    }
    else {
        if (stats[0] == 0) {
            // no succeeded jobs
            return 2;
        }
        else {
            // some jobs have failed
            return 3;
        }
    }
}

//TODO jz: log4j.properties should be used instead
private static void configureLog4J(Properties properties) {
    // TODO Add a file appender for the logs
    // TODO Need to create a property in the properties file for it.
    // sgroschupf, 25Feb2008: this method will be obsolete with PIG-115.
     
    String log4jconf = properties.getProperty(LOG4J_CONF);
    String trueString = "true";
    boolean brief = trueString.equalsIgnoreCase(properties.getProperty(BRIEF));
    Level logLevel = Level.INFO;

    String logLevelString = properties.getProperty(DEBUG);
    if (logLevelString != null){
        logLevel = Level.toLevel(logLevelString, Level.INFO);
    }
    
    if (log4jconf != null) {
         PropertyConfigurator.configure(log4jconf);
     } else if (!brief ) {
         // non-brief logging - timestamps
         Properties props = new Properties();
         props.setProperty("log4j.rootLogger", "INFO, PIGCONSOLE");
         props.setProperty("log4j.appender.PIGCONSOLE",
                           "org.apache.log4j.ConsoleAppender");
         props.setProperty("log4j.appender.PIGCONSOLE.layout",
                           "org.apache.log4j.PatternLayout");
         props.setProperty("log4j.appender.PIGCONSOLE.layout.ConversionPattern",
                           "%d [%t] %-5p %c - %m%n");
         props.setProperty("log4j.appender.PIGCONSOLE.target",
         "System.err");
         PropertyConfigurator.configure(props);
     } else {
         // brief logging - no timestamps
         Properties props = new Properties();
         props.setProperty("log4j.rootLogger", "INFO, PIGCONSOLE");
         props.setProperty("log4j.appender.PIGCONSOLE",
                           "org.apache.log4j.ConsoleAppender");
         props.setProperty("log4j.appender.PIGCONSOLE.layout",
                           "org.apache.log4j.PatternLayout");
         props.setProperty("log4j.appender.PIGCONSOLE.layout.ConversionPattern",
                           "%m%n");
         props.setProperty("log4j.appender.PIGCONSOLE.target",
         "System.err");
         PropertyConfigurator.configure(props);
     }
}
 
// returns the stream of final pig script to be passed to Grunt
private static BufferedReader runParamPreprocessor(BufferedReader origPigScript, ArrayList<String> params,
                                            ArrayList<String> paramFiles, String scriptFile, boolean createFile) 
                                throws org.apache.pig.tools.parameters.ParseException, IOException{
    ParameterSubstitutionPreprocessor psp = new ParameterSubstitutionPreprocessor(50);
    String[] type1 = new String[1];
    String[] type2 = new String[1];

    if (createFile){
        BufferedWriter fw = new BufferedWriter(new FileWriter(scriptFile));
        psp.genSubstitutedFile (origPigScript, fw, params.size() > 0 ? params.toArray(type1) : null, 
                                paramFiles.size() > 0 ? paramFiles.toArray(type2) : null);
        return new BufferedReader(new FileReader (scriptFile));

    } else {
        StringWriter writer = new StringWriter();
        psp.genSubstitutedFile (origPigScript, writer,  params.size() > 0 ? params.toArray(type1) : null, 
                                paramFiles.size() > 0 ? paramFiles.toArray(type2) : null);
        return new BufferedReader(new StringReader(writer.toString()));
    }
}
    
private static String getVersionString() {
	String findContainingJar = JarManager.findContainingJar(Main.class);
	  try { 
		  StringBuffer buffer = new  StringBuffer();
          JarFile jar = new JarFile(findContainingJar); 
          final Manifest manifest = jar.getManifest(); 
          final Map <String,Attributes> attrs = manifest.getEntries(); 
          Attributes attr = attrs.get("org/apache/pig");
          String version = (String) attr.getValue("Implementation-Version");
          String svnRevision = (String) attr.getValue("Svn-Revision");
          String buildTime = (String) attr.getValue("Build-TimeStamp");
          // we use a version string similar to svn 
          //svn, version 1.4.4 (r25188)
          // compiled Sep 23 2007, 22:32:34
          return "Apache Pig version " + version + " (r" + svnRevision + ") \ncompiled "+buildTime;
      } catch (Exception e) { 
          throw new RuntimeException("unable to read pigs manifest file", e); 
      } 
}

public static void usage()
{
	System.out.println("\n"+getVersionString()+"\n");
        System.out.println("USAGE: Pig [options] [-] : Run interactively in grunt shell.");
        System.out.println("       Pig [options] -e[xecute] cmd [cmd ...] : Run cmd(s).");
        System.out.println("       Pig [options] [-f[ile]] file : Run cmds found in file.");
        System.out.println("  options include:");
        System.out.println("    -4, -log4jconf log4j configuration file, overrides log conf");
        System.out.println("    -b, -brief brief logging (no timestamps)");
        System.out.println("    -c, -cluster clustername, kryptonite is default");
        System.out.println("    -d, -debug debug level, INFO is default");
        System.out.println("    -e, -execute commands to execute (within quotes)");
        System.out.println("    -f, -file path to the script to execute");
        System.out.println("    -h, -help display this message");
        System.out.println("    -i, -version display version information");
        System.out.println("    -j, -jar jarfile load jarfile"); 
        System.out.println("    -l, -logfile path to client side log file; current working directory is default");
        System.out.println("    -m, -param_file path to the parameter file");
        System.out.println("    -o, -hod read hod server from system property ssh.gateway");
        System.out.println("    -p, -param key value pair of the form param=val");
        System.out.println("    -r, -dryrun CmdLineParser.ValueExpected.NOT_ACCEPTED");
        System.out.println("    -t, -optimizer_off optimizer rule name, turn optimizer off for this rule; use all to turn all rules off, optimizer is turned on by default");
        System.out.println("    -v, -verbose print all error messages to screen");
        System.out.println("    -w, -warning turn warning on; also turns warning aggregation off");
        System.out.println("    -x, -exectype local|mapreduce, mapreduce is default");

        System.out.println("    -F, -stop_on_failure aborts execution on the first failed job; off by default");
        System.out.println("    -M, -no_multiquery turn multiquery optimization off; Multiquery is on by default");
}

private static String validateLogFile(String logFileName, String scriptName) {
    String strippedDownScriptName = null;
    
    if(scriptName != null) {
        File scriptFile = new File(scriptName);
        if(!scriptFile.isDirectory()) {
            String scriptFileAbsPath;
            try {
                scriptFileAbsPath = scriptFile.getCanonicalPath();
            } catch (IOException ioe) {
                throw new AssertionError("Could not compute canonical path to the script file " + ioe.getMessage());      
            }            
            strippedDownScriptName = getFileFromCanonicalPath(scriptFileAbsPath);
        }
    }
    
    String defaultLogFileName = (strippedDownScriptName == null ? "pig_" : strippedDownScriptName) + new Date().getTime() + ".log";
    File logFile;    
    
    if(logFileName != null) {
        logFile = new File(logFileName);
    
        //Check if the file name is a directory 
        //append the default file name to the file
        if(logFile.isDirectory()) {            
            if(logFile.canWrite()) {
                try {
                    logFileName = logFile.getCanonicalPath() + File.separator + defaultLogFileName;
                } catch (IOException ioe) {
                    throw new AssertionError("Could not compute canonical path to the log file " + ioe.getMessage());       
                }
                return logFileName;
            } else {
                throw new AssertionError("Need write permission in the directory: " + logFileName + " to create log file.");
            }
        } else {
            //we have a relative path or an absolute path to the log file
            //check if we can write to the directory where this file is/will be stored
            
            if (logFile.exists()) {
                if(logFile.canWrite()) {
                    try {
                        logFileName = new File(logFileName).getCanonicalPath();
                    } catch (IOException ioe) {
                        throw new AssertionError("Could not compute canonical path to the log file " + ioe.getMessage());
                    }
                    return logFileName;
                } else {
                    //do not have write permissions for the log file
                    //bail out with an error message
                    throw new AssertionError("Cannot write to file: " + logFileName + ". Need write permission.");
                }
            } else {
                logFile = logFile.getParentFile();
                
                if(logFile != null) {
                    //if the directory is writable we are good to go
                    if(logFile.canWrite()) {
                        try {
                            logFileName = new File(logFileName).getCanonicalPath();
                        } catch (IOException ioe) {
                            throw new AssertionError("Could not compute canonical path to the log file " + ioe.getMessage());
                        }
                        return logFileName;
                    } else {
                        throw new AssertionError("Need write permission in the directory: " + logFile + " to create log file.");
                    }
                }//end if logFile != null else is the default in fall through                
            }//end else part of logFile.exists()
        }//end else part of logFile.isDirectory()
    }//end if logFileName != null
    
    //file name is null or its in the current working directory 
    //revert to the current working directory
    String currDir = System.getProperty("user.dir");
    logFile = new File(currDir);
    logFileName = currDir + File.separator + (logFileName == null? defaultLogFileName : logFileName);
    if(logFile.canWrite()) {        
        return logFileName;
    }    
    throw new RuntimeException("Cannot write to log file: " + logFileName);
}

private static String getFileFromCanonicalPath(String canonicalPath) {
    return canonicalPath.substring(canonicalPath.lastIndexOf(File.separator));
}

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.data;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.backend.executionengine.ExecException;

/**
 * A class to handle reading and writing of intermediate results of data
 * types.  This class could also be used for storing permanent results.
 */
public class DataReaderWriter {
    private static TupleFactory mTupleFactory = TupleFactory.getInstance();
    private static BagFactory mBagFactory = BagFactory.getInstance();
    static final int UNSIGNED_SHORT_MAX = 65535;
    static final String UTF8 = "UTF-8";

    public static Tuple bytesToTuple(DataInput in) throws IOException {
        // Don't use Tuple.readFields, because it requires you to
        // create a tuple with no size and then append fields.
        // That's less efficient than allocating the tuple size up
        // front and then filling in the spaces.
        // Read the size.
        int sz = in.readInt();
        // if sz == 0, we construct an "empty" tuple -
        // presumably the writer wrote an empty tuple!
        if (sz < 0) {
            throw new IOException("Invalid size " + sz + " for a tuple");
        }
        Tuple t = mTupleFactory.newTuple(sz);
        for (int i = 0; i < sz; i++) {
            t.set(i, readDatum(in));
        }
        return t;

    }
    
    public static DataBag bytesToBag(DataInput in) throws IOException {
        DataBag bag = mBagFactory.newDefaultBag();
        bag.readFields(in);
        return bag;
    }
    
    public static Map<Object, Object> bytesToMap(DataInput in) throws IOException {
        int size = in.readInt();    
        Map<Object, Object> m = new HashMap<Object, Object>(size);
        for (int i = 0; i < size; i++) {
            Object key = readDatum(in);
            m.put(key, readDatum(in));
        }
        return m;    
    }
    
    public static String bytesToCharArray(DataInput in) throws IOException{
        int size = in.readUnsignedShort();
        byte[] ba = new byte[size];
        in.readFully(ba);
        return new String(ba, DataReaderWriter.UTF8);
    }

    public static String bytesToBigCharArray(DataInput in) throws IOException{
        int size = in.readInt();
        byte[] ba = new byte[size];
        in.readFully(ba);
        return new String(ba, DataReaderWriter.UTF8);
    }
    
    public static Object readDatum(DataInput in) throws IOException, ExecException {
        // Read the data type
        byte b = in.readByte();
        return readDatum(in, b);
    }
        
    public static Object readDatum(DataInput in, byte type) throws IOException, ExecException {
        switch (type) {
            case DataType.TUPLE: 
                return bytesToTuple(in);
            
            case DataType.BAG: 
                return bytesToBag(in);

            case DataType.MAP: 
                return bytesToMap(in);    

            case DataType.INTEGER:
                return new Integer(in.readInt());

            case DataType.LONG:
                return new Long(in.readLong());

            case DataType.FLOAT:
                return new Float(in.readFloat());

            case DataType.DOUBLE:
                return new Double(in.readDouble());

            case DataType.BOOLEAN:
                return new Boolean(in.readBoolean());

            case DataType.BYTE:
                return new Byte(in.readByte());

            case DataType.BYTEARRAY: {
                int size = in.readInt();
                byte[] ba = new byte[size];
                in.readFully(ba);
                return new DataByteArray(ba);
                                     }

            case DataType.BIGCHARARRAY: 
                return bytesToBigCharArray(in);
            

            case DataType.CHARARRAY: 
                return bytesToCharArray(in);
            
            case DataType.NULL:
                return null;

            default:
                throw new RuntimeException("Unexpected data type " + type +
                    " found in stream.");
        }
    }

    public static void writeDatum(
            DataOutput out,
            Object val) throws IOException {
        // Read the data type
        byte type = DataType.findType(val);
        switch (type) {
            case DataType.TUPLE:
                // Because tuples are written directly by hadoop, the
                // tuple's write method needs to write the indicator byte.
                // So don't write the indicator byte here as it is for
                // everyone else.
                ((Tuple)val).write(out);
                break;
                
            case DataType.BAG:
                out.writeByte(DataType.BAG);
                ((DataBag)val).write(out);
                break;

            case DataType.MAP: {
                out.writeByte(DataType.MAP);
                Map<Object, Object> m = (Map<Object, Object>)val;
                out.writeInt(m.size());
                Iterator<Map.Entry<Object, Object> > i =
                    m.entrySet().iterator();
                while (i.hasNext()) {
                    Map.Entry<Object, Object> entry = i.next();
                    writeDatum(out, entry.getKey());
                    writeDatum(out, entry.getValue());
                }
                break;
                               }

            case DataType.INTEGER:
                out.writeByte(DataType.INTEGER);
                out.writeInt((Integer)val);
                break;

            case DataType.LONG:
                out.writeByte(DataType.LONG);
                out.writeLong((Long)val);
                break;

            case DataType.FLOAT:
                out.writeByte(DataType.FLOAT);
                out.writeFloat((Float)val);
                break;

            case DataType.DOUBLE:
                out.writeByte(DataType.DOUBLE);
                out.writeDouble((Double)val);
                break;

            case DataType.BOOLEAN:
                out.writeByte(DataType.BOOLEAN);
                out.writeBoolean((Boolean)val);
                break;

            case DataType.BYTE:
                out.writeByte(DataType.BYTE);
                out.writeByte((Byte)val);
                break;

            case DataType.BYTEARRAY: {
                out.writeByte(DataType.BYTEARRAY);
                DataByteArray bytes = (DataByteArray)val;
                out.writeInt(bytes.size());
                out.write(bytes.mData);
                break;
                                     }

            case DataType.CHARARRAY: {
                String s = (String)val;
                byte[] utfBytes = s.getBytes(DataReaderWriter.UTF8);
                int length = utfBytes.length;
                
                if(length < DataReaderWriter.UNSIGNED_SHORT_MAX) {
                    out.writeByte(DataType.CHARARRAY);
                    out.writeShort(length);
                    out.write(utfBytes);
                } else {
                	out.writeByte(DataType.BIGCHARARRAY);
                	out.writeInt(length);
                	out.write(utfBytes);
                }
                break;
                                     }

            case DataType.NULL:
                out.writeByte(DataType.NULL);
                break;

            default:
                throw new RuntimeException("Unexpected data type " + type +
                    " found in stream.");
        }
    }
}


/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.PrintStream;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.LinkedList;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapred.Counters;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.jobcontrol.Job;
import org.apache.hadoop.mapred.jobcontrol.JobControl;
import org.apache.pig.PigException;
import org.apache.pig.PigWarning;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.executionengine.ExecutionEngine;
import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
import org.apache.pig.impl.PigContext;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LastInputStreamingOptimizer;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MRPrinter;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.DotMRPrinter;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MRStreamHandler;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.POPackageAnnotator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
import org.apache.pig.impl.plan.CompilationMessageCollector;
import org.apache.pig.impl.plan.PlanException;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.plan.CompilationMessageCollector.Message;
import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType;
import org.apache.pig.impl.util.ConfigurationValidator;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.tools.pigstats.PigStats;

/**
 * Main class that launches pig for Map Reduce
 *
 */
public class MapReduceLauncher extends Launcher{
    private static final Log log = LogFactory.getLog(MapReduceLauncher.class);
 
    //used to track the exception thrown by the job control which is run in a separate thread
    private Exception jobControlException = null;
    private boolean aggregateWarning = false;
    private Map<FileSpec, Exception> failureMap;

    /**
     * Get the exception that caused a failure on the backend for a
     * store location (if any).
     */
    public Exception getError(FileSpec spec) {
        return failureMap.get(spec);
    }

    @Override
    public void reset() {
        failureMap = new HashMap<FileSpec, Exception>();
        super.reset();
    }

    @Override
    public PigStats launchPig(PhysicalPlan php,
                              String grpName,
                              PigContext pc) throws PlanException,
                                                    VisitorException,
                                                    IOException,
                                                    ExecException,
                                                    JobCreationException,
                                                    Exception {
        long sleepTime = 500;
        aggregateWarning = "true".equalsIgnoreCase(pc.getProperties().getProperty("aggregate.warning"));
        MROperPlan mrp = compile(php, pc);
        PigStats stats = new PigStats();
        stats.setMROperatorPlan(mrp);
        stats.setExecType(pc.getExecType());
        stats.setPhysicalPlan(php);
        
        ExecutionEngine exe = pc.getExecutionEngine();
        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
        JobClient jobClient = ((HExecutionEngine)exe).getJobClient();

        JobControlCompiler jcc = new JobControlCompiler(pc, conf);
        
        List<Job> failedJobs = new LinkedList<Job>();
        List<Job> succJobs = new LinkedList<Job>();
        JobControl jc;
        int totalMRJobs = mrp.size();
        int numMRJobsCompl = 0;
        int numMRJobsCurrent = 0;
        double lastProg = -1;
        
        //create the exception handler for the job control thread
        //and register the handler with the job control thread
        JobControlThreadExceptionHandler jctExceptionHandler = new JobControlThreadExceptionHandler();

        while((jc = jcc.compile(mrp, grpName)) != null) {
            numMRJobsCurrent = jc.getWaitingJobs().size();

            Thread jcThread = new Thread(jc);
            jcThread.setUncaughtExceptionHandler(jctExceptionHandler);
            jcThread.start();

            while(!jc.allFinished()){
                try {
                    Thread.sleep(sleepTime);
                } catch (InterruptedException e) {}
                double prog = (numMRJobsCompl+calculateProgress(jc, jobClient))/totalMRJobs;
                if(prog>=(lastProg+0.01)){
                    int perCom = (int)(prog * 100);
                    if(perCom!=100)
                        log.info( perCom + "% complete");
                }
                lastProg = prog;
            }

            //check for the jobControlException first
            //if the job controller fails before launching the jobs then there are
            //no jobs to check for failure
            if(jobControlException != null) {
        	if(jobControlException instanceof PigException) {
                    throw jobControlException;
        	} else {
                    int errCode = 2117;
                    String msg = "Unexpected error when launching map reduce job.";        	
                    throw new ExecException(msg, errCode, PigException.BUG, jobControlException);
        	}
            }

            numMRJobsCompl += numMRJobsCurrent;
            failedJobs.addAll(jc.getFailedJobs());

            if (!failedJobs.isEmpty() 
                && "true".equalsIgnoreCase(
                  pc.getProperties().getProperty("stop.on.failure","false"))) {
                int errCode = 6017;
                StringBuilder msg = new StringBuilder("Execution failed, while processing ");
                
                for (Job j: failedJobs) {
                    List<POStore> sts = jcc.getStores(j);
                    for (POStore st: sts) {
                        msg.append(st.getSFile().getFileName());
                        msg.append(", ");
                    }
                }
                
                throw new ExecException(msg.substring(0,msg.length()-2), 
                                        errCode, PigException.REMOTE_ENVIRONMENT);
            }

            List<Job> jobs = jc.getSuccessfulJobs();
            jcc.moveResults(jobs);
            succJobs.addAll(jobs);
            
            
            stats.setJobClient(jobClient);
            stats.setJobControl(jc);
            stats.accumulateStats();
            
            jc.stop(); 
        }

        log.info( "100% complete");

        boolean failed = false;
        int finalStores = 0;
        // Look to see if any jobs failed.  If so, we need to report that.
        if (failedJobs != null && failedJobs.size() > 0) {
            log.error(failedJobs.size()+" map reduce job(s) failed!");
            Exception backendException = null;

            for (Job fj : failedJobs) {
                
                try {
                    getStats(fj, jobClient, true, pc);
                } catch (Exception e) {
                    backendException = e;
                }

                List<POStore> sts = jcc.getStores(fj);
                for (POStore st: sts) {
                    if (!st.isTmpStore()) {
                        failedStores.add(st.getSFile());
                        failureMap.put(st.getSFile(), backendException);
                        finalStores++;
                    }

                    FileLocalizer.registerDeleteOnFail(st.getSFile().getFileName(), pc);
                    log.error("Failed to produce result in: \""+st.getSFile().getFileName()+"\"");
                }
            }
            failed = true;
        }

        Map<Enum, Long> warningAggMap = new HashMap<Enum, Long>();
                
        if(succJobs!=null) {
            for(Job job : succJobs){
                List<POStore> sts = jcc.getStores(job);
                for (POStore st: sts) {
                    if (!st.isTmpStore()) {
                        succeededStores.add(st.getSFile());
                        finalStores++;
                    }
                    log.info("Successfully stored result in: \""+st.getSFile().getFileName()+"\"");
                }
                getStats(job,jobClient, false, pc);
                if(aggregateWarning) {
                    computeWarningAggregate(job, jobClient, warningAggMap);
                }
            }
        }
        
        if(aggregateWarning) {
            CompilationMessageCollector.logAggregate(warningAggMap, MessageType.Warning, log) ;
        }
        
        // Report records and bytes written.  Only do this in the single store case.  Multi-store
        // scripts mess up the stats reporting from hadoop.
        List<String> rji = stats.getRootJobIDs();
        if (rji != null && rji.size() == 1 && finalStores == 1) {
            log.info("Records written : " + stats.getRecordsWritten());
            log.info("Bytes written : " + stats.getBytesWritten());
        }

        if (!failed) {
            log.info("Success!");
        } else {
            if (succJobs != null && succJobs.size() > 0) {
                log.info("Some jobs have failed!");
            } else {
                log.info("Failed!");
            }
        }
        jcc.reset();

        return stats;
    }

    @Override
    public void explain(
            PhysicalPlan php,
            PigContext pc,
            PrintStream ps,
            String format,
            boolean verbose) throws PlanException, VisitorException,
                                   IOException {
        log.trace("Entering MapReduceLauncher.explain");
        MROperPlan mrp = compile(php, pc);

        if (format.equals("text")) {
            MRPrinter printer = new MRPrinter(ps, mrp);
            printer.setVerbose(verbose);
            printer.visit();
        } else {
            ps.println("#--------------------------------------------------");
            ps.println("# Map Reduce Plan                                  ");
            ps.println("#--------------------------------------------------");
            
            DotMRPrinter printer =new DotMRPrinter(mrp, ps);
            printer.setVerbose(verbose);
            printer.dump();
            ps.println("");
        }
    }

    private MROperPlan compile(
            PhysicalPlan php,
            PigContext pc) throws PlanException, IOException, VisitorException {
        MRCompiler comp = new MRCompiler(php, pc);
        comp.randomizeFileLocalizer();
        comp.compile();
        MROperPlan plan = comp.getMRPlan();
        
        //display the warning message(s) from the MRCompiler
        comp.getMessageCollector().logMessages(MessageType.Warning, aggregateWarning, log);
        
        String lastInputChunkSize = 
            pc.getProperties().getProperty(
                    "last.input.chunksize", POJoinPackage.DEFAULT_CHUNK_SIZE);
        String prop = System.getProperty("pig.exec.nocombiner");
        if (!("true".equals(prop)))  {
            CombinerOptimizer co = new CombinerOptimizer(plan, lastInputChunkSize);
            co.visit();
            //display the warning message(s) from the CombinerOptimizer
            co.getMessageCollector().logMessages(MessageType.Warning, aggregateWarning, log);
        }
        
        // optimize key - value handling in package
        POPackageAnnotator pkgAnnotator = new POPackageAnnotator(plan);
        pkgAnnotator.visit();
        
        // optimize joins
        LastInputStreamingOptimizer liso = 
            new MRCompiler.LastInputStreamingOptimizer(plan, lastInputChunkSize);
        liso.visit();
        
        // figure out the type of the key for the map plan
        // this is needed when the key is null to create
        // an appropriate NullableXXXWritable object
        KeyTypeDiscoveryVisitor kdv = new KeyTypeDiscoveryVisitor(plan);
        kdv.visit();

        // removes the filter(constant(true)) operators introduced by
        // splits.
        NoopFilterRemover fRem = new NoopFilterRemover(plan);
        fRem.visit();
        
        // reduces the number of MROpers in the MR plan generated 
        // by multi-query (multi-store) script.
        MultiQueryOptimizer mqOptimizer = new MultiQueryOptimizer(plan);
        mqOptimizer.visit();

        // removes unnecessary stores (as can happen with splits in
        // some cases.). This has to run after the MultiQuery and
        // NoopFilterRemover.
        NoopStoreRemover sRem = new NoopStoreRemover(plan);
        sRem.visit();
      
        // check whether stream operator is present
        // after MultiQueryOptimizer because it can shift streams from
        // map to reduce, etc.
        MRStreamHandler checker = new MRStreamHandler(plan);
        checker.visit();
        
        return plan;
    }
    
    /**
     * An exception handler class to handle exceptions thrown by the job controller thread
     * Its a local class. This is the only mechanism to catch unhandled thread exceptions
     * Unhandled exceptions in threads are handled by the VM if the handler is not registered
     * explicitly or if the default handler is null
     */
    class JobControlThreadExceptionHandler implements Thread.UncaughtExceptionHandler {
    	
    	public void uncaughtException(Thread thread, Throwable throwable) {
    		ByteArrayOutputStream baos = new ByteArrayOutputStream();
    		PrintStream ps = new PrintStream(baos);
    		throwable.printStackTrace(ps);
    		String exceptionString = baos.toString();    		
    		try {	
    			jobControlException = getExceptionFromString(exceptionString);
    		} catch (Exception e) {
    			String errMsg = "Could not resolve error that occured when launching map reduce job.";
    			jobControlException = new RuntimeException(errMsg, e);
    		}
    	}
    }
    
    void computeWarningAggregate(Job job, JobClient jobClient, Map<Enum, Long> aggMap) {
    	JobID mapRedJobID = job.getAssignedJobID();
    	RunningJob runningJob = null;
    	try {
    		runningJob = jobClient.getJob(mapRedJobID);
    		if(runningJob != null) {
    		Counters counters = runningJob.getCounters();
        		for(Enum e : PigWarning.values()) {
        			Long currentCount = aggMap.get(e);
        			currentCount = (currentCount == null? 0 : currentCount);
        			currentCount += counters.getCounter(e);
        			aggMap.put(e, currentCount);
        		}
    		}
    	} catch (IOException ioe) {
    		String msg = "Unable to retrieve job to compute warning aggregation.";
    		log.warn(msg);
    	}    	
    }

}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;

public class LONegative extends UnaryExpressionOperator {

    private static final long serialVersionUID = 2L;

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LONegative(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, getOperand().getType());
            mFieldSchema.setParent(getOperand().getFieldSchema().canonicalName, getOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Negative " + mKey.scope + "-" + mKey.id;
    }

    
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOGreaterThan extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOGreaterThan.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOGreaterThan(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "GreaterThan " + mKey.scope + "-" + mKey.id;
    }
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOEqual extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOEqual.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOEqual(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Equal " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;

import org.apache.pig.EvalFunc;
import org.apache.pig.ExecType;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.PigServer;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.LogUtils;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import junit.framework.TestCase;

public class TestBestFitCast extends TestCase {
    private PigServer pigServer;
    private MiniCluster cluster = MiniCluster.buildCluster();
    private File tmpFile, tmpFile2;
    int LOOP_SIZE = 20;
    
    public TestBestFitCast() throws ExecException, IOException{
        pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
//        pigServer = new PigServer(ExecType.LOCAL);
        tmpFile = File.createTempFile("test", "txt");
        PrintStream ps = new PrintStream(new FileOutputStream(tmpFile));
        long l = 0;
        for(int i = 1; i <= LOOP_SIZE; i++) {
            ps.println(l + "\t" + i);
        }
        ps.close();
        
        tmpFile2 = File.createTempFile("test2", "txt");
        ps = new PrintStream(new FileOutputStream(tmpFile2));
        l = 0;
        for(int i = 1; i <= LOOP_SIZE; i++) {
            ps.println(l + "\t" + i + "\t" + i);
        }
        ps.close();
    }
    
    @Before
    public void setUp() throws Exception {
        
    }

    @After
    public void tearDown() throws Exception {
    }
    
    public static class UDF1 extends EvalFunc<Tuple>{
        /**
         * java level API
         * @param input expects a single numeric DataAtom value
         * @param output returns a single numeric DataAtom value, cosine value of the argument
         */
        @Override
        public Tuple exec(Tuple input) throws IOException {
            return input;
        }

        /* (non-Javadoc)
         * @see org.apache.pig.EvalFunc#getArgToFuncMapping()
         */
        @Override
        public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
            List<FuncSpec> funcList = new ArrayList<FuncSpec>();
            funcList.add(new FuncSpec(this.getClass().getName(), new Schema(Arrays.asList(new Schema.FieldSchema(null, DataType.FLOAT),new Schema.FieldSchema(null, DataType.FLOAT)))));
            funcList.add(new FuncSpec(this.getClass().getName(), new Schema(Arrays.asList(new Schema.FieldSchema(null, DataType.LONG),new Schema.FieldSchema(null, DataType.DOUBLE)))));
            funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.FLOAT))));
            funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.INTEGER))));
            funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DOUBLE))));
            /*funcList.add(new FuncSpec(DoubleMax.class.getName(), Schema.generateNestedSchema(DataType.BAG, DataType.DOUBLE)));
            funcList.add(new FuncSpec(FloatMax.class.getName(), Schema.generateNestedSchema(DataType.BAG, DataType.FLOAT)));
            funcList.add(new FuncSpec(IntMax.class.getName(), Schema.generateNestedSchema(DataType.BAG, DataType.INTEGER)));
            funcList.add(new FuncSpec(LongMax.class.getName(), Schema.generateNestedSchema(DataType.BAG, DataType.LONG)));
            funcList.add(new FuncSpec(StringMax.class.getName(), Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY)));*/
            return funcList;
        }    

    }
    
    public static class UDF2 extends EvalFunc<String>{
        /**
         * java level API
         * @param input expects a single numeric DataAtom value
         * @param output returns a single numeric DataAtom value, cosine value of the argument
         */
        @Override
        public String exec(Tuple input) throws IOException {
            try{
                String str = (String)input.get(0);
                return str.toUpperCase();
            }catch (Exception e){
                return null;
            }
        }

        /* (non-Javadoc)
         * @see org.apache.pig.EvalFunc#getArgToFuncMapping()
         */
        @Override
        public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
            List<FuncSpec> funcList = new ArrayList<FuncSpec>();
            funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY))));
            return funcList;
        }    

    }
    
    /**
     * For testing with input schemas which have byte arrays
     */
    public static class UDF3 extends EvalFunc<Tuple>{
        
        /**
         * a UDF which simply returns its input as output
         */
        @Override
        public Tuple exec(Tuple input) throws IOException {
            return input;
        }

        /* (non-Javadoc)
         * @see org.apache.pig.EvalFunc#getArgToFuncMapping()
         */
        @Override
        public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
            List<FuncSpec> funcList = new ArrayList<FuncSpec>();
            
            // the following schema should match when the input is
            // just a {bytearray} - exact match
            funcList.add(new FuncSpec(this.getClass().getName(), 
                    new Schema(new Schema.FieldSchema(null, DataType.BYTEARRAY))));
            // the following schema should match when the input is
            // just a {int} - exact match
            funcList.add(new FuncSpec(this.getClass().getName(), 
                    new Schema(new Schema.FieldSchema(null, DataType.INTEGER))));
            
            // The following two schemas will cause conflict when input schema
            // is {float, bytearray} since bytearray can be casted either to long
            // or double. However when input schema is {bytearray, int}, it should work
            // since bytearray should get casted to float and int to long. Likewise if
            // input schema is {bytearray, long} or {bytearray, double} it should work
            funcList.add(new FuncSpec(this.getClass().getName(), 
                    new Schema(Arrays.asList(new Schema.FieldSchema(null, DataType.FLOAT),
                            new Schema.FieldSchema(null, DataType.DOUBLE)))));
            funcList.add(new FuncSpec(this.getClass().getName(), 
                    new Schema(Arrays.asList(new Schema.FieldSchema(null, DataType.FLOAT),
                            new Schema.FieldSchema(null, DataType.LONG)))));
            
            
            // The following two schemas will cause conflict when input schema is
            // {bytearray, int, int} since the two ints could be casted to long, double
            // or double, long. Likewise input schema of either {bytearray, long, long}
            // or {bytearray, double, double} would cause conflict. Input schema of
            // {bytearray, long, double} or {bytearray, double, long} should not cause
            // conflict since only the bytearray needs to be casted to float. Input schema
            // of {float, bytearray, long} or {float, long, bytearray} should also
            // work since only the bytearray needs to be casted. Input schema of
            // {float, bytearray, int} will cause conflict since we could cast int to 
            // long or double and bytearray to long or double. Input schema of
            // {bytearray, long, int} should work and should match the first schema below for 
            // matching wherein the bytearray is cast to float and the int to double.
            funcList.add(new FuncSpec(this.getClass().getName(), 
                    new Schema(Arrays.asList(new Schema.FieldSchema(null, DataType.FLOAT),
                            new Schema.FieldSchema(null, DataType.DOUBLE),
                            new Schema.FieldSchema(null, DataType.LONG)))));
            funcList.add(new FuncSpec(this.getClass().getName(), 
                    new Schema(Arrays.asList(new Schema.FieldSchema(null, DataType.FLOAT),
                            new Schema.FieldSchema(null, DataType.LONG),
                            new Schema.FieldSchema(null, DataType.DOUBLE)))));
            
            return funcList;
        }    

    }

    @Test
    public void testByteArrayCast1() throws IOException {
        //Passing (float, bytearray)
        //Ambiguous matches: (float, long) , (float, double)
        boolean exceptionCaused = false;
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x:float, y);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y);");
        try {
            Iterator<Tuple> iter = pigServer.openIterator("B");
        } catch(Exception e) {
            exceptionCaused = true;
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertTrue(msg.contains("Multiple matching functions"));
            assertTrue(msg.contains("{float,double}, {float,long}"));
        }
        assertTrue(exceptionCaused);
    }
    
    @Test
    public void testByteArrayCast2() throws IOException, ExecException {
        // Passing (bytearray, int)
        // Possible matches: (float, long) , (float, double)
        // Chooses (float, long) since in both cases bytearray is cast to float and the
        // cost of casting int to long < int to double
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(1), new Long(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast3() throws IOException, ExecException {
        // Passing (bytearray, long)
        // Possible matches: (float, long) , (float, double)
        // Chooses (float, long) since that is the only exact match without bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:long);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x, y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(1), new Long(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast4() throws IOException, ExecException {
        // Passing (bytearray, double)
        // Possible matches: (float, long) , (float, double)
        // Chooses (float, double) since that is the only exact match without bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:double);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Double);
            assertEquals((Double)((Tuple)t.get(1)).get(1), new Double(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }

    @Test
    public void testByteArrayCast5() throws IOException, ExecException {
        // Passing (bytearray, int, int )
        // Ambiguous matches: (float, long, double) , (float, double, long)
        // bytearray can be casted to float but the two ints cannot be unambiguously
        // casted
        boolean exceptionCaused = false;
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y, y);");
        try {
            Iterator<Tuple> iter = pigServer.openIterator("B");
        }catch(Exception e) {
            exceptionCaused = true;
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertTrue(msg.contains("Multiple matching functions"));
            assertTrue(msg.contains("({float,double,long}, {float,long,double})"));
        }
        assertTrue(exceptionCaused);
    }
    
    @Test
    public void testByteArrayCast6() throws IOException, ExecException {
        // Passing (bytearray, long, long )
        // Ambiguous matches: (float, long, double) , (float, double, long)
        // bytearray can be casted to float but the two longs cannot be
        // unambiguously casted
        boolean exceptionCaused = false;
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:long);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y, y);");
        try {
            Iterator<Tuple> iter = pigServer.openIterator("B");
        }catch(Exception e) {
            exceptionCaused = true;
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertTrue(msg.contains("Multiple matching functions"));
            assertTrue(msg.contains("({float,double,long}, {float,long,double})"));
        }
        assertTrue(exceptionCaused);
    }
    
    @Test
    public void testByteArrayCast7() throws IOException, ExecException {
        // Passing (bytearray, double, double )
        // Ambiguous matches: (float, long, double) , (float, double, long)
        // bytearray can be casted to float but the two doubles cannot be 
        // casted with a permissible cast
        boolean exceptionCaused = false;
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:double);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y, y);");
        try {
            Iterator<Tuple> iter = pigServer.openIterator("B");
        }catch(Exception e) {
            exceptionCaused = true;
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertTrue(msg.contains("Could not infer the matching function"));
        }
        assertTrue(exceptionCaused);
    }
    
    @Test
    public void testByteArrayCast8() throws IOException, ExecException {
        // Passing (bytearray, long, double)
        // Possible matches: (float, long, double) , (float, double, long)
        // Chooses (float, long, double) since that is the only exact match without bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile2.toString()) + "' as (x, y:long, z:double);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y,z);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(1), new Long(cnt + 1));
            assertTrue(((Tuple)t.get(1)).get(2) instanceof Double);
            assertEquals((Double)((Tuple)t.get(1)).get(2), new Double(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast9() throws IOException, ExecException {
        // Passing (bytearray, double, long)
        // Possible matches: (float, long, double) , (float, double, long)
        // Chooses (float, double, long) since that is the only exact match without bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile2.toString()) + "' as (x, y:double, z:long);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y,z);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Double);
            assertEquals((Double)((Tuple)t.get(1)).get(1), new Double(cnt + 1));
            assertTrue(((Tuple)t.get(1)).get(2) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(2), new Long(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast10() throws IOException, ExecException {
        // Passing (float, long, bytearray)
        // Possible matches: (float, long, double) , (float, double, long)
        // Chooses (float, long, double) since that is the only exact match without bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile2.toString()) + "' as (x:float, y:long, z);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y,z);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(1), new Long(cnt + 1));
            assertTrue(((Tuple)t.get(1)).get(2) instanceof Double);
            assertEquals((Double)((Tuple)t.get(1)).get(2), new Double(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast11() throws IOException, ExecException {
        // Passing (float, bytearray, long)
        // Possible matches: (float, long, double) , (float, double, long)
        // Chooses (float, double, long) since that is the only exact match without bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile2.toString()) + "' as (x:float, y, z:long);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y,z);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Double);
            assertEquals((Double)((Tuple)t.get(1)).get(1), new Double(cnt + 1));
            assertTrue(((Tuple)t.get(1)).get(2) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(2), new Long(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast12() throws IOException, ExecException {
        // Passing (float, bytearray, int )
        // Ambiguous matches: (float, long, double) , (float, double, long)
        // will cause conflict since we could cast int to 
        // long or double and bytearray to long or double.
        boolean exceptionCaused = false;
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile2.toString()) + "' as (x:float, y, z:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y, y);");
        try {
            Iterator<Tuple> iter = pigServer.openIterator("B");
        }catch(Exception e) {
            exceptionCaused = true;
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertTrue(msg.contains("Multiple matching functions"));
            assertTrue(msg.contains("({float,double,long}, {float,long,double}"));
        }
        assertTrue(exceptionCaused);
    }
    
    @Test
    public void testByteArrayCast13() throws IOException, ExecException {
        // Passing (bytearray, long, int)
        // Possible matches: (float, long, double) , (float, double, long)
        // Chooses (float, long, double) since for the bytearray there is a 
        // single unambiguous cast to float. For the other two args, it is
        // less "costlier" to cast the last int to double than cast the long
        // to double and int to long
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile2.toString()) + "' as (x, y:long, z:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF3.class.getName() + "(x,y,z);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals((Float)((Tuple)t.get(1)).get(0), 0.0f);
            assertTrue(((Tuple)t.get(1)).get(1) instanceof Long);
            assertEquals((Long)((Tuple)t.get(1)).get(1), new Long(cnt + 1));
            assertTrue(((Tuple)t.get(1)).get(2) instanceof Double);
            assertEquals((Double)((Tuple)t.get(1)).get(2), new Double(cnt + 1));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast14() throws IOException, ExecException {
        // Passing (bag{(bytearray)})
        // Possible matches: bag{(bytearray)}, bag{(int)}, bag{(long)}, bag{(float)}, bag{(double)}
        // Chooses bag{(bytearray)} because it is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y);");
        pigServer.registerQuery("B = group A all;");
        pigServer.registerQuery("C = FOREACH B generate SUM(A.y);");
        Iterator<Tuple> iter = pigServer.openIterator("C");
        Tuple t = iter.next();
        assertTrue(t.get(0) instanceof Double);
        assertEquals(new Double(210), (Double)t.get(0));
    }
    
    @Test
    public void testByteArrayCast15() throws IOException, ExecException {
        // Passing (bytearray)
        // Possible matches: (bytearray), (int)
        // Chooses (bytearray) because that is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y);");
        pigServer.registerQuery("B = FOREACH A generate " + UDF3.class.getName() + "(y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(0)).get(0) instanceof DataByteArray);
            byte[] expected = Integer.toString(cnt + 1).getBytes();
            byte[] actual = ((DataByteArray)((Tuple)t.get(0)).get(0)).get();
            assertEquals(expected.length, actual.length);
            for(int i = 0; i < expected.length; i++) {
                assertEquals(expected[i], actual[i]);
            }
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testByteArrayCast16() throws IOException, ExecException {
        // Passing (int)
        // Possible matches: (bytearray), (int)
        // Chooses (int) because that is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:int);");
        pigServer.registerQuery("B = FOREACH A generate " + UDF3.class.getName() + "(y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertTrue(((Tuple)t.get(0)).get(0) instanceof Integer);
            assertEquals(new Integer(cnt + 1), (Integer)((Tuple)t.get(0)).get(0));
            ++cnt;
        }
        assertEquals(LOOP_SIZE, cnt);
    }
    
    @Test
    public void testIntSum() throws IOException, ExecException {
        // Passing (bag{(int)})
        // Possible matches: bag{(bytearray)}, bag{(int)}, bag{(long)}, bag{(float)}, bag{(double)}
        // Chooses bag{(int)} since it is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:int);");
        pigServer.registerQuery("B = group A all;");
        pigServer.registerQuery("C = FOREACH B generate SUM(A.y);");
        Iterator<Tuple> iter = pigServer.openIterator("C");
        Tuple t = iter.next();
        assertTrue(t.get(0) instanceof Long);
        assertEquals(new Long(210), (Long)t.get(0));
    }
    
    @Test
    public void testLongSum() throws IOException, ExecException {
        // Passing (bag{(long)})
        // Possible matches: bag{(bytearray)}, bag{(int)}, bag{(long)}, bag{(float)}, bag{(double)}
        // Chooses bag{(long)} since it is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:long);");
        pigServer.registerQuery("B = group A all;");
        pigServer.registerQuery("C = FOREACH B generate SUM(A.y);");
        Iterator<Tuple> iter = pigServer.openIterator("C");
        Tuple t = iter.next();
        assertTrue(t.get(0) instanceof Long);
        assertEquals(new Long(210), (Long)t.get(0));
    }
    
    @Test
    public void testFloatSum() throws IOException, ExecException {
        // Passing (bag{(float)})
        // Possible matches: bag{(bytearray)}, bag{(int)}, bag{(long)}, bag{(float)}, bag{(double)}
        // Chooses bag{(float)} since it is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:float);");
        pigServer.registerQuery("B = group A all;");
        pigServer.registerQuery("C = FOREACH B generate SUM(A.y);");
        Iterator<Tuple> iter = pigServer.openIterator("C");
        Tuple t = iter.next();
        assertTrue(t.get(0) instanceof Double);
        assertEquals(new Double(210), (Double)t.get(0));
    }
    
    @Test
    public void testDoubleSum() throws IOException, ExecException {
        // Passing (bag{(double)})
        // Possible matches: bag{(bytearray)}, bag{(int)}, bag{(long)}, bag{(float)}, bag{(double)}
        // Chooses bag{(double)} since it is an exact match
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x, y:double);");
        pigServer.registerQuery("B = group A all;");
        pigServer.registerQuery("C = FOREACH B generate SUM(A.y);");
        Iterator<Tuple> iter = pigServer.openIterator("C");
        Tuple t = iter.next();
        assertTrue(t.get(0) instanceof Double);
        assertEquals(new Double(210), (Double)t.get(0));
    }
    
    @Test
    public void test1() throws Exception{
        //Passing (long, int)
        //Possible matches: (float, float) , (long, double)
        //Chooses (long, double) as it has only one cast compared to two for (float, float)
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x:long, y:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF1.class.getName() + "(x,y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertEquals(true,((Tuple)t.get(1)).get(0) instanceof Long);
            assertEquals(true,((Tuple)t.get(1)).get(1) instanceof Double);
            ++cnt;
        }
        assertEquals(20, cnt);
    }
    
    @Test
    public void test2() throws Exception{
        //Passing (int, int)
        //Possible matches: (float, float) , (long, double)
        //Throws Exception as ambiguous definitions found
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x:long, y:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF1.class.getName() + "(y,y);");
        try{
            pigServer.openIterator("B");
        }catch (Exception e) {
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertEquals(true,msg.contains("as multiple or none of them fit"));
        }
        
    }
    
    @Test
    public void test3() throws Exception{
        //Passing (int, int)
        //Possible matches: (float, float) , (long, double)
        //Chooses (float, float) as both options lead to same score and (float, float) occurs first.
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x:long, y:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF1.class.getName() + "((float)y,(float)y);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertEquals(true,((Tuple)t.get(1)).get(0) instanceof Float);
            assertEquals(true,((Tuple)t.get(1)).get(1) instanceof Float);
            ++cnt;
        }
        assertEquals(20, cnt);
    }
    
    @Test
    public void test4() throws Exception{
        //Passing (long)
        //Possible matches: (float), (integer), (double)
        //Chooses (float) as it leads to a better score that to (double)
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "' as (x:long, y:int);");
        pigServer.registerQuery("B = FOREACH A generate x, " + UDF1.class.getName() + "(x);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        int cnt = 0;
        while(iter.hasNext()){
            Tuple t = iter.next();
            assertEquals(true,((Tuple)t.get(1)).get(0) instanceof Float);
            ++cnt;
        }
        assertEquals(20, cnt);
    }
    
    @Test
    public void test5() throws Exception{
        //Passing bytearrays
        //Possible matches: (float, float) , (long, double)
        //Throws exception since more than one funcSpec and inp is bytearray
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(tmpFile.toString()) + "';");
        pigServer.registerQuery("B = FOREACH A generate $0, " + UDF1.class.getName() + "($1,$1);");
        try{
            pigServer.openIterator("B");
        }catch (Exception e) {
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertEquals(true,msg.contains("Multiple matching functions"));
        }
        
    }

    @Test
    public void test6() throws Exception{
        // test UDF with single mapping function 
        // where bytearray is passed in as input parameter
        File input = Util.createInputFile("tmp", "", new String[] {"abc"});
        pigServer.registerQuery("A = LOAD '" + Util.generateURI(input.toString()) +"';");
        pigServer.registerQuery("B = FOREACH A GENERATE " + UDF2.class.getName() + "($0);");
        Iterator<Tuple> iter = pigServer.openIterator("B");
        if(!iter.hasNext()) fail("No Output received");
        Tuple t = iter.next();
        assertEquals("ABC", t.get(0));
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.io.IOException;

import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.Pair;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;


/**
 * Parent for all Logical operators.
 */
abstract public class LogicalOperator extends Operator<LOVisitor> {
    private static final long serialVersionUID = 2L;

    /**
     * Schema that defines the output of this operator.
     */
    protected Schema mSchema = null;

    /**
     * A boolean variable to remember if the schema has been computed
     */
    protected boolean mIsSchemaComputed = false;

    /**
     * Datatype of this output of this operator. Operators start out with data
     * type set to UNKNOWN, and have it set for them by the type checker.
     */
    protected byte mType = DataType.UNKNOWN;

    /**
     * Requested level of parallelism for this operation.
     */
    protected int mRequestedParallelism;

    /**
     * Name of the record set that results from this operator.
     */
    protected String mAlias;

    /**
     * Logical plan that this operator is a part of.
     */
    protected LogicalPlan mPlan;

    private static Log log = LogFactory.getLog(LogicalOperator.class);

    /**
     * Equivalent to LogicalOperator(k, 0).
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LogicalOperator(LogicalPlan plan, OperatorKey k) {
        this(plan, k, -1);
    }

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k Operator key to assign to this node.
     * @param rp degree of requested parallelism with which to execute this
     *            node.
     */
    public LogicalOperator(LogicalPlan plan, OperatorKey k, int rp) {
        super(k);
        mPlan = plan;
        mRequestedParallelism = rp;
    }

    /**
     * Get the operator key for this operator.
     */
    public OperatorKey getOperatorKey() {
        return mKey;
    }

    /**
     * Set the output schema for this operator. If a schema already exists, an
     * attempt will be made to reconcile it with this new schema.
     * 
     * @param schema
     *            Schema to set.
     * @throws ParseException
     *             if there is already a schema and the existing schema cannot
     *             be reconciled with this new schema.
     */
    public void setSchema(Schema schema) throws FrontendException {
        // In general, operators don't generate their schema until they're
        // asked, so ask them to do it.
        try {
            getSchema();
        } catch (FrontendException ioe) {
            // It's fine, it just means we don't have a schema yet.
        }
        if (mSchema == null) {
            mSchema = schema;
        } else {
            mSchema.reconcile(schema);
        }
    }

    /**
     * Directly force the schema without reconcilation
     * Please use with great care
     * @param schema
     */
    public void forceSchema(Schema schema) {
        this.mSchema = schema;
    }

    /**
     * Unset the schema as if it had not been calculated.  This is used by
     * anyone who reorganizes the tree and needs to have schemas recalculated.
     */
    public void unsetSchema() throws VisitorException {
        mIsSchemaComputed = false;
        mSchema = null;
    }

    public Schema regenerateSchema() throws FrontendException, VisitorException {
        unsetSchema();
        return getSchema();
    }
    
    /**
     * Calculate canonical names for all fields in the schema.  This should
     * only be used for loads or other operators that create all new fields.
     */
    public void setCanonicalNames() {
        for (Schema.FieldSchema fs : mSchema.getFields()) {
            fs.canonicalName = CanonicalNamer.getNewName();
        }
    }


    /**
     * Get a copy of the schema for the output of this operator.
     */
    public abstract Schema getSchema() throws FrontendException;

    /**
     * Set the type of this operator. This should only be called by the type
     * checking routines.
     * 
     * @param t 
     *            Type to set this operator to.
     */
    final public void setType(byte t) {
        mType = t;
    }

    /**
     * Get the type of this operator.
     */
    public byte getType() {
        return mType;
    }

    public String getAlias() {
        return mAlias;
    }

    public void setAlias(String newAlias) {
        mAlias = newAlias;
    }

    public int getRequestedParallelism() {
        return mRequestedParallelism;
    }

    public void setRequestedParallelism(int newRequestedParallelism) {
        mRequestedParallelism = newRequestedParallelism;
    }

    @Override
    public String toString() {
        StringBuffer msg = new StringBuffer();

        msg.append("(Name: " + name() + " Operator Key: " + mKey + ")");

        return msg.toString();
    }

    /**
     * Given a schema, reconcile it with our existing schema.
     * 
     * @param schema
     *            Schema to reconcile with the existing.
     * @throws ParseException
     *             if the reconciliation is not possible.
     */
    protected void reconcileSchema(Schema schema) throws ParseException {
        if (mSchema == null) {
            mSchema = schema;
            return;
        }

        // TODO
    }

    /**
     * Visit this node with the provided visitor. This should only be called by
     * the visitor class itself, never directly.
     * 
     * @param v
     *            Visitor to visit with.
     * @throws VisitException
     *             if the visitor has a problem.
     */
    public abstract void visit(LOVisitor v) throws VisitorException;

    public LogicalPlan getPlan() {
        return mPlan ;
    }

    /**
     * Change the reference to the plan for this operator.  Don't use this
     * unless you're sure you know what you're doing.
     */
    public void setPlan(LogicalPlan plan) {
        mPlan = plan;
    }


    /***
     * IMPORTANT:
     * This method is only used for unit testing purpose.
     */
    public void setSchemaComputed(boolean computed) {
       mIsSchemaComputed = computed ;   
    }

    @Override
    public boolean supportsMultipleOutputs() {
        return true;
    }

    /**
     * @see org.apache.pig.impl.plan.Operator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        LogicalOperator loClone = (LogicalOperator)super.clone();
        if(mSchema != null)
            loClone.mSchema = this.mSchema.clone();
        return loClone;
    }    


    /**
     * Produce a map describing how this operator modifies its projection.
     * @return ProjectionMap null indicates it does not know how the projection
     * changes, for example a join of two inputs where one input does not have
     * a schema.
     */
    public ProjectionMap getProjectionMap() {
        return null;
    };

    /**
	 * Get a list of fields that this operator requires. This is not necessarily
	 * equivalent to the list of fields the operator projects. For example, a
	 * filter will project anything passed to it, but requires only the fields
	 * explicitly referenced in its filter expression.
	 * 
	 * @return list of RequiredFields null indicates that the operator does not need any
	 *         fields from its input.
	 */
	public List<RequiredFields> getRequiredFields() {
		return null;
	}

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.PathFilter;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.InputFormat;
import org.apache.hadoop.mapred.InputSplit;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.JobConfigurable;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.RecordReader;
import org.apache.hadoop.mapred.Reporter;
import org.apache.pig.ExecType;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.data.TargetedTuple;
import org.apache.pig.Slice;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.executionengine.PigSlicer;
import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
import org.apache.pig.backend.hadoop.datastorage.HDataStorage;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SliceWrapper;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.io.ValidatingInputFileSpec;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.util.ObjectSerializer;
import org.apache.pig.impl.util.Pair;

public class PigInputFormat implements InputFormat<Text, Tuple>,
       JobConfigurable {

    public static final Log LOG = LogFactory
            .getLog(PigInputFormat.class);

    private static final PathFilter hiddenFileFilter = new PathFilter() {
        public boolean accept(Path p) {
            String name = p.getName();
            return !name.startsWith("_") && !name.startsWith(".");
        }
    };

    public static JobConf sJob;

    /**
     * Is the given filename splitable? Usually, true, but if the file is stream
     * compressed, it will not be.
     * 
     * <code>FileInputFormat</code> implementations can override this and
     * return <code>false</code> to ensure that individual input files are
     * never split-up so that {@link Mapper}s process entire files.
     * 
     * @param fs
     *            the file system that the file is on
     * @param filename
     *            the file name to check
     * @return is this file splitable?
     */
    protected boolean isSplitable(FileSystem fs, Path filename) {
        return !filename.getName().endsWith(".gz");
    }

    /**
     * List input directories. Subclasses may override to, e.g., select only
     * files matching a regular expression.
     * 
     * @param job
     *            the job to list input paths for
     * @return array of Path objects
     * @throws IOException
     *             if zero items.
     */
    protected Path[] listPaths(JobConf job) throws IOException {
        Path[] dirs = FileInputFormat.getInputPaths(job);
        if (dirs.length == 0) {
            int errCode = 2092;
            String msg = "No input paths specified in job.";
            throw new ExecException(msg, errCode, PigException.BUG);
        }
        
        List<Path> result = new ArrayList<Path>();
        for (Path p : dirs) {
            FileSystem fs = p.getFileSystem(job);
            FileStatus[] matches = fs.globStatus(p, hiddenFileFilter);
            for (FileStatus match : matches) {
                result.add(fs.makeQualified(match.getPath()));
            }
        }

        return result.toArray(new Path[result.size()]);
    }

    public void validateInput(JobConf job) throws IOException {
        /*ArrayList<FileSpec> inputs = (ArrayList<FileSpec>) ObjectSerializer
                .deserialize(job.get("pig.inputs"));
        Path[] inputDirs = new Path[inputs.size()];
        int i = 0;
        for (FileSpec spec : inputs) {
            inputDirs[i++] = new Path(spec.getFileName());
        }

        if (inputDirs.length == 0) {
            throw new IOException("No input paths specified in input");
        }

        List<IOException> result = new ArrayList<IOException>();
        int totalFiles = 0;
        for (Path p : inputDirs) {
            FileSystem fs = p.getFileSystem(job);
            if (fs.exists(p)) {
                // make sure all paths are files to avoid exception
                // while generating splits
                for (Path subPath : fs.listPaths(p, hiddenFileFilter)) {
                    FileSystem subFS = subPath.getFileSystem(job);
                    if (!subFS.exists(subPath)) {
                        result.add(new IOException(
                                "Input path does not exist: " + subPath));
                    } else {
                        totalFiles++;
                    }
                }
            } else {
                Path[] paths = fs.globPaths(p, hiddenFileFilter);
                if (paths.length == 0) {
                    result.add(new IOException("Input Pattern " + p
                            + " matches 0 files"));
                } else {
                    // validate globbed paths
                    for (Path gPath : paths) {
                        FileSystem gPathFS = gPath.getFileSystem(job);
                        if (!gPathFS.exists(gPath)) {
                            result.add(new FileNotFoundException(
                                    "Input path doesnt exist : " + gPath));
                        }
                    }
                    totalFiles += paths.length;
                }
            }
        }
        if (!result.isEmpty()) {
            throw new InvalidInputException(result);
        }
        // send output to client.
        LOG.info("Total input paths to process : " + totalFiles);*/
    }
    
    /**
     * Creates input splits one per input and slices of it
     * per DFS block of the input file. Configures the PigSlice
     * and returns the list of PigSlices as an array
     */
    public InputSplit[] getSplits(JobConf job, int numSplits)
            throws IOException {
        ArrayList<Pair<FileSpec, Boolean>> inputs;
		ArrayList<ArrayList<OperatorKey>> inpTargets;
		PigContext pigContext;
		try {
			inputs = (ArrayList<Pair<FileSpec, Boolean>>) ObjectSerializer
			        .deserialize(job.get("pig.inputs"));
			inpTargets = (ArrayList<ArrayList<OperatorKey>>) ObjectSerializer
			        .deserialize(job.get("pig.inpTargets"));
			pigContext = (PigContext) ObjectSerializer.deserialize(job
			        .get("pig.pigContext"));
		} catch (Exception e) {
			int errCode = 2094;
			String msg = "Unable to deserialize object.";
			throw new ExecException(msg, errCode, PigException.BUG, e);
		}
        
        ArrayList<InputSplit> splits = new ArrayList<InputSplit>();
        for (int i = 0; i < inputs.size(); i++) {
            try {
				Path path = new Path(inputs.get(i).first.getFileName());
                                
                                FileSystem fs;
                                
                                try {
                                    fs = path.getFileSystem(job);
                                } catch (Exception e) {
                                    // If an application specific
                                    // scheme was used
                                    // (e.g.: "hbase://table") we will fail
                                    // getting the file system. That's
                                    // ok, we just use the dfs in that case.
                                    fs = new Path("/").getFileSystem(job);
                                }

				// if the execution is against Mapred DFS, set
				// working dir to /user/<userid>
				if(pigContext.getExecType() == ExecType.MAPREDUCE) {
				    fs.setWorkingDirectory(new Path("/user", job.getUser()));
                                }
				
				DataStorage store = new HDataStorage(ConfigurationUtil.toProperties(job));
				ValidatingInputFileSpec spec;
				if (inputs.get(i).first instanceof ValidatingInputFileSpec) {
				    spec = (ValidatingInputFileSpec) inputs.get(i).first;
				} else {
				    spec = new ValidatingInputFileSpec(inputs.get(i).first, store);
				}
				boolean isSplittable = inputs.get(i).second;
				if ((spec.getSlicer() instanceof PigSlicer)) {
				    ((PigSlicer)spec.getSlicer()).setSplittable(isSplittable);
				}
				Slice[] pigs = spec.getSlicer().slice(store, spec.getFileName());
				for (Slice split : pigs) {
				    splits.add(new SliceWrapper(split, pigContext.getExecType(), i, fs, inpTargets.get(i)));
				}
            } catch (ExecException ee) {
            	throw ee;
			} catch (Exception e) {
				int errCode = 2118;
				String msg = "Unable to create input slice for: " + inputs.get(i).first.getFileName();
				throw new ExecException(msg, errCode, PigException.BUG, e);
			}
        }
        return splits.toArray(new SliceWrapper[splits.size()]);
    }

    public RecordReader<Text, Tuple> getRecordReader(InputSplit split,
            JobConf job, Reporter reporter) throws IOException {
        PigInputFormat.sJob = job;
        activeSplit = (SliceWrapper) split;
        return ((SliceWrapper) split).makeReader(job);
    }

    public void configure(JobConf conf) {
    }

    public static SliceWrapper getActiveSplit() {
        return activeSplit;
    }

    private static SliceWrapper activeSplit;
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;


import java.util.List;

import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;

public class LOBinCond extends ExpressionOperator {

    // BinCond has a conditional expression and two nested queries.
    // If the conditional expression evaluates to true the first nested query
    // is executed else the second nested query is executed

    private static final long serialVersionUID = 2L;
 
    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOBinCond(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }// End Constructor LOBinCond

    public ExpressionOperator getCond() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(0);
    }

    public ExpressionOperator getLhsOp() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(1);
    }

    public ExpressionOperator getRhsOp() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(2);
    }
    
    
    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

	@Override
	public Schema getSchema() throws FrontendException {
		return mSchema;
	}

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
		//We need a check of LHS and RHS schemas
        //The type checker perform this task
        if (!mIsFieldSchemaComputed) {
            try {
                mFieldSchema = getLhsOp().getFieldSchema();
                mIsFieldSchemaComputed = true;
            } catch (FrontendException fee) {
                mFieldSchema = null;
                mIsFieldSchemaComputed = false;
                throw fee;
            }
        }
        return mFieldSchema;
    }

    @Override
    public String name() {
        return "BinCond " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.ExpressionOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        LOBinCond clone = (LOBinCond)super.clone();
        return clone;
    }

}

/**
 * 
 */
package org.apache.pig.backend.hadoop.executionengine.util;

import java.io.IOException;

import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.util.Progressable;
import org.apache.pig.PigException;
import org.apache.pig.StoreConfig;
import org.apache.pig.StoreFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.util.ObjectSerializer;

/**
 * A class of utility static methods to be used in the hadoop map reduce backend
 */
public class MapRedUtil {

    /**
     * This method is to be called from an 
     * {@link org.apache.hadoop.mapred.OutputFormat#getRecordWriter(FileSystem ignored, JobConf job,
                                     String name, Progressable progress)}
     * method to obtain a reference to the {@link org.apache.pig.StoreFunc} object to be used by
     * that OutputFormat to perform the write() operation
     * @param conf the JobConf object
     * @return the StoreFunc reference
     * @throws ExecException
     */
    public static StoreFunc getStoreFunc(JobConf conf) throws ExecException {
        StoreFunc store;
        try {
            String storeFunc = conf.get("pig.storeFunc", "");
            if (storeFunc.length() == 0) {
                store = new PigStorage();
            } else {
                storeFunc = (String) ObjectSerializer.deserialize(storeFunc);
                store = (StoreFunc) PigContext
                        .instantiateFuncFromSpec(storeFunc);
            }
        } catch (Exception e) {
            int errCode = 2081;
            String msg = "Unable to setup the store function.";
            throw new ExecException(msg, errCode, PigException.BUG, e);
        }
        return store;
    }
    
    /**
     * This method is to be called from an 
     * {@link org.apache.hadoop.mapred.OutputFormat#getRecordWriter(FileSystem ignored, JobConf job,
                                     String name, Progressable progress)}
     * method to obtain a reference to the {@link org.apache.pig.StoreConfig} object. The StoreConfig
     * object will contain metadata information like schema and location to be used by
     * that OutputFormat to perform the write() operation
     * @param conf the JobConf object
     * @return StoreConfig object containing metadata information useful for
     * an OutputFormat to write the data
     * @throws IOException
     */
    public static StoreConfig getStoreConfig(JobConf conf) throws IOException {
        return (StoreConfig) ObjectSerializer.deserialize(conf.get(JobControlCompiler.PIG_STORE_CONFIG));
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import java.util.List;
import java.util.ArrayList;


import org.junit.After;
import org.junit.Test;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.logicalLayer.*;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.test.utils.LogicalPlanTester;


public class TestProjectionMap extends junit.framework.TestCase {

    private final Log log = LogFactory.getLog(getClass());
    LogicalPlanTester planTester = new LogicalPlanTester();
    
    @After
    @Override
    public void tearDown() throws Exception{
        planTester.reset(); 
    }

    private static final String simpleEchoStreamingCommand;
    static {
        if (System.getProperty("os.name").toUpperCase().startsWith("WINDOWS"))
            simpleEchoStreamingCommand = "perl -ne 'print \\\"$_\\\"'";
        else
            simpleEchoStreamingCommand = "perl -ne 'print \"$_\"'";
    }

    
    @Test
    public void testQueryForeach1() {
        String query = "foreach (load 'a') generate $1,$2;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the foreach projection map has null mappedFields
        //and null removed fields since the input schema is null
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        assertTrue(foreachProjectionMap.getMappedFields() == null);
        assertTrue(foreachProjectionMap.getRemovedFields() == null);
        
        //check that added fields contain [0, 1]
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields.size() == 2);
        assertTrue(foreachAddedFields.get(0) == 0);
        assertTrue(foreachAddedFields.get(1) == 1);
    }

    @Test
    public void testQueryForeach2() {
        String query = "foreach (load 'a' using " + PigStorage.class.getName() + "(':')) generate $1, 'aoeuaoeu' ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the foreach projection map has null mappedFields
        //and null removed fields since the input schema is null
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        assertTrue(foreachProjectionMap.getMappedFields() == null);
        assertTrue(foreachProjectionMap.getRemovedFields() == null);
        
        //check that added fields contain [0, 1]
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields.size() == 2);
        assertTrue(foreachAddedFields.get(0) == 0);
        assertTrue(foreachAddedFields.get(1) == 1);
    }

    @Test
    public void testQueryCogroup1() {
        String query = "foreach (cogroup (load 'a') by $1, (load 'b') by $1) generate org.apache.pig.builtin.AVG($1) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);

        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);
        assertTrue(mapValues.get(1).first == 1);
        assertTrue(mapValues.get(1).second == 1);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        assertTrue(foreachProjectionMap.getMappedFields() == null);

        //check that removed fields has all the columns from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachProjectionMap.getRemovedFields().size() == 3);
        int expectedColumn = 0;
        for(Pair<Integer, Integer> removedField: foreachRemovedFields) {
            assertTrue(removedField.first == 0);
            assertTrue(removedField.second == expectedColumn++);
        }
        
        //check that added fields contain [0]
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields.size() == 1);
        assertTrue(foreachAddedFields.get(0) == 0);
    }

    @Test
    public void testQueryGroupAll() throws Exception {
        String query = "foreach (group (load 'a') ALL) generate $1 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields == null);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [0, 1]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 0);
        assertTrue(cogroupAddedFields.get(1) == 1);
        
        //check that the foreach projection map has non-null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        MultiMap<Integer, Pair<Integer, Integer>> foreachMappedFields = foreachProjectionMap.getMappedFields();
        assertTrue(foreachMappedFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMappedFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);


        //check that removed fields has all the columns from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);

        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }

    @Test
    public void testQueryGroup2() {
        String query = "foreach (group (load 'a') by $1) generate group, '1' ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);

        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [0, 1]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 1);
        assertTrue(cogroupAddedFields.get(0) == 1);
        
        //check that the foreach projection map has non-null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);        
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields(); 
        assertTrue(foreachMapFields != null);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 0);


        //check that removed fields has all the columns from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);

        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 1);
        
        //check that added fields contain [1]
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields != null);
        assertTrue(foreachAddedFields.size() == 1);
        assertTrue(foreachAddedFields.get(0) == 1);
    }

    @Test
    public void testQueryCogroup2() {
        String query = "foreach (cogroup (load 'a') by ($1), (load 'b') by ($1)) generate $1.$1, $2.$1 ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);

        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);
        assertTrue(mapValues.get(1).first == 1);
        assertTrue(mapValues.get(1).second == 1);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields(); 
        assertTrue(foreachMapFields != null);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 2);

        //check that removed fields has all the group column from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }

    @Test
    public void testQueryGroup3() {
        String query = "foreach (group (load 'a') by ($6, $7)) generate flatten(group) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);

        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 6);
        assertTrue(mapValues.get(1).first == 0);
        assertTrue(mapValues.get(1).second == 7);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 1);
        assertTrue(cogroupAddedFields.get(0) == 1);
        
        //check that the foreach projection map has null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields(); 
        assertTrue(foreachMapFields != null);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 0);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 0);

        //check that removed fields has all the group column from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 1);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);

    }

    @Test
    public void testQueryFilterNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = filter a by $1 == '3';");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the filter projection map has null mappedFields
        LOFilter filter = (LOFilter)lp.getLeaves().get(0);
        ProjectionMap filterProjectionMap = filter.getProjectionMap();
        assertTrue(filterProjectionMap == null);
    }
    
    @Test
    public void testQuerySplitNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("split a into b if $0 == '3', c if $1 == '3';");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the split projection map has null mappedFields
        LOSplit split = (LOSplit)lp.getSuccessors(load).get(0);
        ProjectionMap splitProjectionMap = split.getProjectionMap();
        assertTrue(splitProjectionMap == null);
        
        LOSplitOutput splitb = (LOSplitOutput)lp.getSuccessors(split).get(0);
        ProjectionMap splitbProjectionMap = splitb.getProjectionMap();
        assertTrue(splitbProjectionMap == null);
        
        LOSplitOutput splitc = (LOSplitOutput)lp.getSuccessors(split).get(1);
        ProjectionMap splitcProjectionMap = splitc.getProjectionMap();
        assertTrue(splitcProjectionMap == null);
    }
    
    @Test
    public void testQueryOrderByNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = order a by $1;");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the order by projection map has null mappedFields
        LOSort sort = (LOSort)lp.getLeaves().get(0);
        ProjectionMap sortProjectionMap = sort.getProjectionMap();
        assertTrue(sortProjectionMap == null);
    }
    
    @Test
    public void testQueryLimitNoSchema() {
        planTester.buildPlan("a = load 'a';");
        planTester.buildPlan("b = order a by $1;");
        LogicalPlan lp = planTester.buildPlan("c = limit b 10;");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the order by projection map is not null
        LOSort sort = (LOSort)lp.getSuccessors(load).get(0);
        ProjectionMap sortProjectionMap = sort.getProjectionMap();
        assertTrue(sortProjectionMap == null);
        
        //check that the limit projection map is null
        LOLimit limit = (LOLimit)lp.getLeaves().get(0);
        ProjectionMap limitProjectionMap = limit.getProjectionMap();
        assertTrue(limitProjectionMap == null);
    }
    
    @Test
    public void testQueryDistinctNoSchema() {
        planTester.buildPlan("a = load 'a';");
        LogicalPlan lp = planTester.buildPlan("b = distinct a;");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the distinct projection map has null mappedFields
        LODistinct distinct = (LODistinct)lp.getLeaves().get(0);
        ProjectionMap distinctProjectionMap = distinct.getProjectionMap();
        assertTrue(distinctProjectionMap == null);
    }
    
    @Test
    public void testQueryStreamingNoSchema() {
        String query = "stream (load 'a') through `" + simpleEchoStreamingCommand + "`;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the stream projection map is null
        LOStream stream = (LOStream)lp.getLeaves().get(0);
        ProjectionMap streamProjectionMap = stream.getProjectionMap();
        assertTrue(streamProjectionMap == null);
    }
    
    @Test
    public void testQueryStreamingNoSchema1() {
        String query = "stream (load 'a' as (url, hitCount)) through `" + simpleEchoStreamingCommand + "` ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the stream projection map is null
        LOStream stream = (LOStream)lp.getLeaves().get(0);
        ProjectionMap streamProjectionMap = stream.getProjectionMap();
        assertTrue(streamProjectionMap == null);
    }
    
    @Test
    public void testQueryForeach3() {
        String query = "foreach (load 'a') generate ($1 == '3'? $2 : $3) ;";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the foreach projection map has null mappedFields
        //and null removed fields since the input schema is null
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        assertTrue(foreachProjectionMap.getMappedFields() == null);
        assertTrue(foreachProjectionMap.getRemovedFields() == null);
        
        //check that added fields contain [0]
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields.size() == 1);
        assertTrue(foreachAddedFields.get(0) == 0);
    }
    
    @Test
    public void testQueryForeach4() {
        planTester.buildPlan("A = load 'a';");
        planTester.buildPlan("B = load 'b';");
        LogicalPlan lp = planTester.buildPlan("foreach (cogroup A by ($1), B by ($1)) generate A, flatten(B.($1, $2, $3));");
        
        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);

        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> cogroupMapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(cogroupMapValues.get(0).first == 0);
        assertTrue(cogroupMapValues.get(0).second == 1);
        assertTrue(cogroupMapValues.get(1).first == 1);
        assertTrue(cogroupMapValues.get(1).second == 1);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has non-null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields();
        assertTrue(foreachMapFields != null);
        
        List<Pair<Integer, Integer>> foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 1);
        
        for(int i = 1; i < 4; ++i) {
            foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(i);
            assertTrue(foreachMapValues.get(0).first == 0);
            assertTrue(foreachMapValues.get(0).second == 2);
        }
        
        //check that removed fields has all the group column from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachProjectionMap.getRemovedFields().size() == 1);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }
    
    @Test
    public void testForeach5() {
        planTester.buildPlan("A = load 'a';");
        planTester.buildPlan("B = load 'b';");
        planTester.buildPlan("C = cogroup A by ($1), B by ($1);");
        String query = "foreach C { " +
                "B = order B by $0; " +
                "generate FLATTEN(A), B.($1, $2, $3) ;" +
                "};" ;
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);

        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> cogroupMapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(cogroupMapValues.get(0).first == 0);
        assertTrue(cogroupMapValues.get(0).second == 1);
        assertTrue(cogroupMapValues.get(1).first == 1);
        assertTrue(cogroupMapValues.get(1).second == 1);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields();
        assertTrue(foreachMapFields != null);
        
        List<Pair<Integer, Integer>> foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 1);
        
        //check that removed fields has all the columns from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachProjectionMap.getRemovedFields().size() == 2);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        removedField = foreachRemovedFields.get(1);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 2);

        //check that added fields contain [0]
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields.size() == 1);
        assertTrue(foreachAddedFields.get(0) == 1);
    }
    
    
    @Test
    public void testQueryCrossNoSchema(){
        String query = "c = cross (load 'a'), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cross projection map
        LOCross cross = (LOCross)lp.getSuccessors(loada).get(0);
        ProjectionMap crossProjectionMap = cross.getProjectionMap();
        assertTrue(crossProjectionMap == null);
        
    }
    
    @Test
    public void testQueryUnionNoSchema(){
        String query = "c = union (load 'a'), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check union projection map
        LOUnion union = (LOUnion)lp.getSuccessors(loada).get(0);
        ProjectionMap unionProjectionMap = union.getProjectionMap();
        assertTrue(unionProjectionMap == null);
        
    }
    
    @Test
    public void testQueryFRJoinNoSchema(){
        String query = "c = join (load 'a') by $0, (load 'b') by $0 using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cross projection map
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        ProjectionMap frjoinProjectionMap = frjoin.getProjectionMap();
        assertTrue(frjoinProjectionMap == null);
        
    }

    @Test
    public void testQueryJoinNoSchema(){
        String query = "c = join (load 'a') by $0, (load 'b') by $0;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 0);
        assertTrue(mapValues.get(1).first == 1);
        assertTrue(mapValues.get(1).second == 0);
        
        //check the cogroup removed fields is null
        assertTrue(cogroupProjectionMap.getRemovedFields() == null);
        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has non-null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields(); 
        assertTrue(foreachMapFields != null);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 2);

        //check that removed fields has all the group column from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }
    
    @Test
    public void testQueryFilterWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = filter a by $1 == '3';");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the filter projection map is not null
        LOFilter filter = (LOFilter)lp.getLeaves().get(0);
        ProjectionMap filterProjectionMap = filter.getProjectionMap();
        assertTrue(filterProjectionMap != null);
        assertTrue(filterProjectionMap.changes() == false);
    }
    
    @Test
    public void testQuerySplitWithSchema() {
        planTester.buildPlan("a = load 'a' as (url, hitCount);");
        LogicalPlan lp = planTester.buildPlan("split a into b if url == '3', c if hitCount == '3';");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the split projection map has null mappedFields
        LOSplit split = (LOSplit)lp.getSuccessors(load).get(0);
        ProjectionMap splitProjectionMap = split.getProjectionMap();
        assertTrue(splitProjectionMap.changes() == false);
        
        LOSplitOutput splitb = (LOSplitOutput)lp.getSuccessors(split).get(0);
        ProjectionMap splitbProjectionMap = splitb.getProjectionMap();
        assertTrue(splitbProjectionMap.changes() == false);
        
        LOSplitOutput splitc = (LOSplitOutput)lp.getSuccessors(split).get(1);
        ProjectionMap splitcProjectionMap = splitc.getProjectionMap();
        assertTrue(splitcProjectionMap.changes() == false);
    }
    
    @Test
    public void testQueryOrderByWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = order a by $1;");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the order by projection map is not null
        LOSort sort = (LOSort)lp.getLeaves().get(0);
        ProjectionMap sortProjectionMap = sort.getProjectionMap();
        assertTrue(sortProjectionMap != null);
        assertTrue(sortProjectionMap.changes() == false);
    }
    
    @Test
    public void testQueryLimitWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        planTester.buildPlan("b = order a by $1;");
        LogicalPlan lp = planTester.buildPlan("c = limit b 10;");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the order by projection map is not null
        LOSort sort = (LOSort)lp.getSuccessors(load).get(0);
        ProjectionMap sortProjectionMap = sort.getProjectionMap();
        assertTrue(sortProjectionMap != null);
        assertTrue(sortProjectionMap.changes() == false);
        
        //check that the limit projection map is not null
        LOLimit limit = (LOLimit)lp.getLeaves().get(0);
        ProjectionMap limitProjectionMap = limit.getProjectionMap();
        assertTrue(limitProjectionMap != null);
        assertTrue(limitProjectionMap.changes() == false);
    }
    
    @Test
    public void testQueryDistinctWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        LogicalPlan lp = planTester.buildPlan("b = distinct a;");
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the distinct projection map is not null
        LODistinct distinct = (LODistinct)lp.getLeaves().get(0);
        ProjectionMap distinctProjectionMap = distinct.getProjectionMap();
        assertTrue(distinctProjectionMap != null);
        assertTrue(distinctProjectionMap.changes() == false);
    }

    @Test
    public void testQueryStreamingWithSchema() {
        String query = "stream (load 'a') through `" + simpleEchoStreamingCommand + "` as (x, y);";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the stream projection map is null
        LOStream stream = (LOStream)lp.getLeaves().get(0);
        ProjectionMap streamProjectionMap = stream.getProjectionMap();
        assertTrue(streamProjectionMap.changes() == true);
        assertTrue(streamProjectionMap.getMappedFields() == null);
        assertTrue(streamProjectionMap.getRemovedFields() == null);
        
        List<Integer> streamAddedFields = streamProjectionMap.getAddedFields();
        assertTrue(streamAddedFields.size() == 2);
        assertTrue(streamAddedFields.get(0) == 0);
        assertTrue(streamAddedFields.get(1) == 1);
    }

    @Test
    public void testQueryStreamingWithSchema1() {
        String query = "stream (load 'a' as (url, hitCount)) through `" + simpleEchoStreamingCommand + "` as (x, y);";
        LogicalPlan lp = planTester.buildPlan(query);
        
        //check that the load projection map is null
        LOLoad load = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadProjectionMap = load.getProjectionMap();
        assertTrue(loadProjectionMap == null);
        
        //check that the stream projection map is null
        LOStream stream = (LOStream)lp.getLeaves().get(0);
        ProjectionMap streamProjectionMap = stream.getProjectionMap();
        assertTrue(streamProjectionMap.changes() == true);
        assertTrue(streamProjectionMap.getMappedFields() == null);
        
        //check that removed fields has all the columns from the input load
        List<Pair<Integer, Integer>> streamRemovedFields = streamProjectionMap.getRemovedFields();
        assertTrue(streamRemovedFields.size() == 2);
        Pair<Integer, Integer> removedField = streamRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        removedField = streamRemovedFields.get(1);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 1);
        
        List<Integer> streamAddedFields = streamProjectionMap.getAddedFields();
        assertTrue(streamAddedFields.size() == 2);
        assertTrue(streamAddedFields.get(0) == 0);
        assertTrue(streamAddedFields.get(1) == 1);
    }
    
    @Test
    public void testQueryImplicitJoinWithSchema() {
        planTester.buildPlan("a = load 'a' as (url,hitCount);");
        planTester.buildPlan("b = load 'b' as (url,rank);");
        planTester.buildPlan("c = cogroup a by url, b by url;");
        LogicalPlan lp = planTester.buildPlan("d = foreach c generate group,flatten(a),flatten(b);");

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);

        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> cogroupMapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(cogroupMapValues.get(0).first == 0);
        assertTrue(cogroupMapValues.get(0).second == 0);
        assertTrue(cogroupMapValues.get(1).first == 1);
        assertTrue(cogroupMapValues.get(1).second == 0);
        
        //check that removed fields has hitCount from a and rank from b
        List<Pair<Integer, Integer>> cogroupRemovedFields = cogroupProjectionMap.getRemovedFields();
        assertTrue(cogroupRemovedFields.size() == 2);
        Pair<Integer, Integer> removedField = cogroupRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 1);
        
        removedField = cogroupRemovedFields.get(1);
        assertTrue(removedField.first == 1);
        assertTrue(removedField.second == 1);

        
        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields();
        assertTrue(foreachMapFields != null);
        
        List<Pair<Integer, Integer>> foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 0);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 1);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(2);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 1);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(3);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 2);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(4);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 2);
        
        //check that removed fields is null
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields == null);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
        
        lp = planTester.buildPlan("e = foreach d generate group, a::url, b::url, b::rank, rank;");
        
        foreach = (LOForEach)lp.getLeaves().get(0);
        foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        foreachMapFields = foreachProjectionMap.getMappedFields();
        assertTrue(foreachMapFields != null);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 0);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 1);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(2);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 3);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(3);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 4);
        
        foreachMapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(4);
        assertTrue(foreachMapValues.get(0).first == 0);
        assertTrue(foreachMapValues.get(0).second == 4);
        
        //check that removed fields is null
        foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields != null);
        assertTrue(foreachRemovedFields.size() == 1);
        
        removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 2);
        
        //check that added fields is null
        foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }
    
    @Test
    public void testQueryCrossWithSchema(){
        String query = "c = cross (load 'a' as (url, hitcount)), (load 'b' as (url, rank));";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cross projection map
        LOCross cross = (LOCross)lp.getSuccessors(loada).get(0);
        ProjectionMap crossProjectionMap = cross.getProjectionMap();
        assertTrue(crossProjectionMap != null);
        
        MultiMap<Integer, Pair<Integer, Integer>> crossMapFields = crossProjectionMap.getMappedFields();
        assertTrue(crossMapFields != null);
        
        List<Pair<Integer, Integer>> crossMapValues = (ArrayList<Pair<Integer, Integer>>)crossMapFields.get(0);
        assertTrue(crossMapValues.get(0).first == 0);
        assertTrue(crossMapValues.get(0).second == 0);
        
        crossMapValues = (ArrayList<Pair<Integer, Integer>>)crossMapFields.get(1);
        assertTrue(crossMapValues.get(0).first == 0);
        assertTrue(crossMapValues.get(0).second == 1);
        
        crossMapValues = (ArrayList<Pair<Integer, Integer>>)crossMapFields.get(2);
        assertTrue(crossMapValues.get(0).first == 1);
        assertTrue(crossMapValues.get(0).second == 0);
        
        crossMapValues = (ArrayList<Pair<Integer, Integer>>)crossMapFields.get(3);
        assertTrue(crossMapValues.get(0).first == 1);
        assertTrue(crossMapValues.get(0).second == 1);
        
        //check that removed fields is null
        List<Pair<Integer, Integer>> crossRemovedFields = crossProjectionMap.getRemovedFields();
        assertTrue(crossRemovedFields == null);
        
        //check that added fields is null
        List<Integer> crossAddedFields = crossProjectionMap.getAddedFields();
        assertTrue(crossAddedFields == null);
    }
    
    @Test
    public void testQueryUnionWithSchema(){
        String query = "c = union (load 'a' as (url, hitcount)), (load 'b' as (url, rank));";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check union projection map
        LOUnion union = (LOUnion)lp.getSuccessors(loada).get(0);
        ProjectionMap unionProjectionMap = union.getProjectionMap();
        assertTrue(unionProjectionMap != null);
        
        MultiMap<Integer, Pair<Integer, Integer>> unionMapFields = unionProjectionMap.getMappedFields();
        assertTrue(unionMapFields != null);

        List<Pair<Integer, Integer>> unionMapValues = (ArrayList<Pair<Integer, Integer>>)unionMapFields.get(0);
        assertTrue(unionMapValues.get(0).first == 0);
        assertTrue(unionMapValues.get(0).second == 0);
        
        assertTrue(unionMapValues.get(1).first == 1);
        assertTrue(unionMapValues.get(1).second == 0);
        
        unionMapValues = (ArrayList<Pair<Integer, Integer>>)unionMapFields.get(1);
        assertTrue(unionMapValues.get(0).first == 0);
        assertTrue(unionMapValues.get(0).second == 1);
        
        assertTrue(unionMapValues.get(1).first == 1);
        assertTrue(unionMapValues.get(1).second == 1);
        
        //check that removed fields is null
        List<Pair<Integer, Integer>> unionRemovedFields = unionProjectionMap.getRemovedFields();
        assertTrue(unionRemovedFields == null);
        
        //check that added fields is null
        List<Integer> unionAddedFields = unionProjectionMap.getAddedFields();
        assertTrue(unionAddedFields == null);

    }
    
    @Test
    public void testQueryFRJoinWithSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b' as (url, rank)) by $0 using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cross projection map
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        ProjectionMap frjoinProjectionMap = frjoin.getProjectionMap();
        assertTrue(frjoinProjectionMap != null);
        
        MultiMap<Integer, Pair<Integer, Integer>> frjoinMapFields = frjoinProjectionMap.getMappedFields();
        assertTrue(frjoinMapFields != null);

        List<Pair<Integer, Integer>> frjoinMapValues = (ArrayList<Pair<Integer, Integer>>)frjoinMapFields.get(0);
        assertTrue(frjoinMapValues.get(0).first == 0);
        assertTrue(frjoinMapValues.get(0).second == 0);
        
        frjoinMapValues = (ArrayList<Pair<Integer, Integer>>)frjoinMapFields.get(1);
        assertTrue(frjoinMapValues.get(0).first == 0);
        assertTrue(frjoinMapValues.get(0).second == 1);
        
        frjoinMapValues = (ArrayList<Pair<Integer, Integer>>)frjoinMapFields.get(2);
        assertTrue(frjoinMapValues.get(0).first == 1);
        assertTrue(frjoinMapValues.get(0).second == 0);
        
        frjoinMapValues = (ArrayList<Pair<Integer, Integer>>)frjoinMapFields.get(3);
        assertTrue(frjoinMapValues.get(0).first == 1);
        assertTrue(frjoinMapValues.get(0).second == 1);
        
        //check that removed fields is null
        List<Pair<Integer, Integer>> frjoinRemovedFields = frjoinProjectionMap.getRemovedFields();
        assertTrue(frjoinRemovedFields == null);
        
        //check that added fields is null
        List<Integer> frjoinAddedFields = frjoinProjectionMap.getAddedFields();
        assertTrue(frjoinAddedFields == null);

    }

    @Test
    public void testQueryJoinWithSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b' as (url, rank)) by $0;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 0);
        assertTrue(mapValues.get(1).first == 1);
        assertTrue(mapValues.get(1).second == 0);
        
        //check that removed fields is not null
        List<Pair<Integer, Integer>> cogroupRemovedFields = cogroupProjectionMap.getRemovedFields();
        assertTrue(cogroupRemovedFields != null);
        
        Pair<Integer, Integer> removedFields = cogroupRemovedFields.get(0);
        assertTrue(removedFields.first == 0);
        assertTrue(removedFields.second == 1);

        removedFields = cogroupRemovedFields.get(1);
        assertTrue(removedFields.first == 1);
        assertTrue(removedFields.second == 1);

        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has non-null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields(); 
        assertTrue(foreachMapFields != null);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);

        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(2);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 2);

        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(3);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 2);

        
        //check that removed fields has all the group column from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }

    @Test
    public void testQueryCrossWithMixedSchema(){
        String query = "c = cross (load 'a' as (url, hitcount)), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cross projection map
        LOCross cross = (LOCross)lp.getSuccessors(loada).get(0);
        ProjectionMap crossProjectionMap = cross.getProjectionMap();
        assertTrue(crossProjectionMap == null);
    }
    
    @Test
    public void testQueryUnionWithMixedSchema(){
        String query = "c = union (load 'a' as (url, hitcount)), (load 'b');";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check union projection map
        LOUnion union = (LOUnion)lp.getSuccessors(loada).get(0);
        ProjectionMap unionProjectionMap = union.getProjectionMap();
        assertTrue(unionProjectionMap == null);
    }
    
    @Test
    public void testQueryFRJoinWithMixedSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b') by $0 using \"replicated\";";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cross projection map
        LOFRJoin frjoin = (LOFRJoin)lp.getSuccessors(loada).get(0);
        ProjectionMap frjoinProjectionMap = frjoin.getProjectionMap();
        assertTrue(frjoinProjectionMap == null);
        
    }
    
    @Test
    public void testQueryJoinWithMixedSchema(){
        String query = "c = join (load 'a' as (url, hitcount)) by $0, (load 'b') by $0;";
        LogicalPlan lp = planTester.buildPlan(query);

        //check that the loads' projection map is null
        LOLoad loada = (LOLoad) lp.getRoots().get(0);
        ProjectionMap loadaProjectionMap = loada.getProjectionMap();
        assertTrue(loadaProjectionMap == null);
        
        LOLoad loadb = (LOLoad) lp.getRoots().get(1);
        ProjectionMap loadbProjectionMap = loadb.getProjectionMap();
        assertTrue(loadbProjectionMap == null);
        
        //check cogroup projection map
        LOCogroup cogroup = (LOCogroup)lp.getSuccessors(loada).get(0);
        ProjectionMap cogroupProjectionMap = cogroup.getProjectionMap();
        assertTrue(cogroupProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> cogroupMapFields = cogroupProjectionMap.getMappedFields(); 
        assertTrue(cogroupMapFields != null);
        
        List<Pair<Integer, Integer>> mapValues = (ArrayList<Pair<Integer, Integer>>)cogroupMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 0);
        assertTrue(mapValues.get(1).first == 1);
        assertTrue(mapValues.get(1).second == 0);
        
        //check that removed fields is not null
        List<Pair<Integer, Integer>> cogroupRemovedFields = cogroupProjectionMap.getRemovedFields();
        assertTrue(cogroupRemovedFields.size() == 1);
        
        Pair<Integer, Integer> removedFields = cogroupRemovedFields.get(0);
        assertTrue(removedFields.first == 0);
        assertTrue(removedFields.second == 1);

        //check that cogroup added fields contain [1, 2]
        List<Integer> cogroupAddedFields = cogroupProjectionMap.getAddedFields();
        assertTrue(cogroupAddedFields.size() == 2);
        assertTrue(cogroupAddedFields.get(0) == 1);
        assertTrue(cogroupAddedFields.get(1) == 2);
        
        //check that the foreach projection map has non-null mappedFields
        LOForEach foreach = (LOForEach)lp.getLeaves().get(0);
        ProjectionMap foreachProjectionMap = foreach.getProjectionMap();
        assertTrue(foreachProjectionMap.changes() == true);
        
        MultiMap<Integer, Pair<Integer, Integer>> foreachMapFields = foreachProjectionMap.getMappedFields(); 
        assertTrue(foreachMapFields.size() == 3);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(0);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);
        
        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(1);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 1);

        mapValues = (ArrayList<Pair<Integer, Integer>>)foreachMapFields.get(2);
        assertTrue(mapValues.get(0).first == 0);
        assertTrue(mapValues.get(0).second == 2);

        
        //check that removed fields has all the group column from the input cogroup
        List<Pair<Integer, Integer>> foreachRemovedFields = foreachProjectionMap.getRemovedFields();
        assertTrue(foreachRemovedFields.size() == 1);
        Pair<Integer, Integer> removedField = foreachRemovedFields.get(0);
        assertTrue(removedField.first == 0);
        assertTrue(removedField.second == 0);
        
        //check that added fields is null
        List<Integer> foreachAddedFields = foreachProjectionMap.getAddedFields();
        assertTrue(foreachAddedFields == null);
    }
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;

import java.util.Iterator;
import java.util.List;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.plan.NodeIdGenerator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.VisitorException;

/**
 * Find the distinct set of tuples in a bag.
 * This is a blocking operator. All the input is put in the hashset implemented
 * in DistinctDataBag which also provides the other DataBag interfaces.
 * 
 * 
 */
public class PODistinct extends PhysicalOperator implements Cloneable {

    private boolean inputsAccumulated = false;
    private DataBag distinctBag = null;
    private final Log log = LogFactory.getLog(getClass());
    transient Iterator<Tuple> it;

    public PODistinct(OperatorKey k, int rp, List<PhysicalOperator> inp) {
        super(k, rp, inp);
    }

    public PODistinct(OperatorKey k, int rp) {
        super(k, rp);
    }

    public PODistinct(OperatorKey k, List<PhysicalOperator> inp) {
        super(k, inp);
    }

    public PODistinct(OperatorKey k) {
        super(k);
    }

    @Override
    public boolean isBlocking() {
        return true;
    }

    @Override
    public Result getNext(Tuple t) throws ExecException {
        if (!inputsAccumulated) {
            Result in = processInput();
            distinctBag = BagFactory.getInstance().newDistinctBag();
            while (in.returnStatus != POStatus.STATUS_EOP) {
                if (in.returnStatus == POStatus.STATUS_ERR) {
                    log.error("Error in reading from inputs");
                    return in;
                    //continue;
                } else if (in.returnStatus == POStatus.STATUS_NULL) {
                    // Ignore the null, read the next tuple.  It's not clear
                    // to me that we should ever get this, or if we should, 
                    // how it differs from EOP.  But ignoring it here seems
                    // to work.
                    in = processInput();
                    continue;
                }
                distinctBag.add((Tuple) in.result);
                in = processInput();
            }
            inputsAccumulated = true;
        }
        if (it == null) {
            it = distinctBag.iterator();
        }
        res.result = it.next();
        if (res.result == null){
            res.returnStatus = POStatus.STATUS_EOP;
            reset();
        } else {
            res.returnStatus = POStatus.STATUS_OK;
        }
        return res;
    }

    @Override
    public String name() {
        return "PODistinct" + "[" + DataType.findTypeName(resultType) + "]" +" - " + mKey.toString();
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    @Override
    public boolean supportsMultipleOutputs() {
        return false;
    }

    @Override
    public void reset() {
        inputsAccumulated = false;
        distinctBag = null;
        it = null;
    }

    @Override
    public void visit(PhyPlanVisitor v) throws VisitorException {
        v.visitDistinct(this);
    }
    /* (non-Javadoc)
     * @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator#clone()
     */
    @Override
    public PODistinct clone() throws CloneNotSupportedException {
        // TODO Auto-generated method stub
        return new PODistinct(new OperatorKey(this.mKey.scope, NodeIdGenerator.getGenerator().getNextNodeId(this.mKey.scope)), this.requestedParallelism, this.inputs);
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import java.util.*;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;

import junit.framework.Assert;

import org.apache.pig.ExecType;
import org.apache.pig.FuncSpec;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.DefaultBagFactory;
import org.apache.pig.data.DefaultTuple;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.PigServer;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
import org.apache.pig.test.utils.GenPhyOp;
import org.apache.pig.test.utils.TestHelper;
import org.apache.pig.impl.logicalLayer.LOLoad;
import org.apache.pig.impl.logicalLayer.LogicalOperator;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.impl.logicalLayer.LogicalPlanBuilder;
import org.apache.pig.backend.datastorage.ContainerDescriptor;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.DataStorageException;
import org.apache.pig.backend.datastorage.ElementDescriptor;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class TestLoad extends junit.framework.TestCase {
    FileSpec inpFSpec;
    POLoad ld;
    PigContext pc;
    DataBag inpDB;
    String curDir;
    String inpDir;
    PigServer pig;
    
    static MiniCluster cluster = MiniCluster.buildCluster();
    @Before
    public void setUp() throws Exception {
        curDir = System.getProperty("user.dir");
        inpDir = curDir + File.separatorChar + "test/org/apache/pig/test/data/InputFiles/";
        if ((System.getProperty("os.name").toUpperCase().startsWith("WINDOWS")))
            inpDir="/"+FileLocalizer.parseCygPath(inpDir, FileLocalizer.STYLE_WINDOWS);
        inpFSpec = new FileSpec("file:" + inpDir + "passwd", new FuncSpec(PigStorage.class.getName(), new String[]{":"}));

        FileLocalizer.deleteTempFiles();
        pig = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        pc = pig.getPigContext();
        
        ld = GenPhyOp.topLoadOp();
        ld.setLFile(inpFSpec);
        ld.setPc(pc);
        
        inpDB = DefaultBagFactory.getInstance().newDefaultBag();
        BufferedReader br = new BufferedReader(new FileReader("test/org/apache/pig/test/data/InputFiles/passwd"));
        
        for(String line = br.readLine();line!=null;line=br.readLine()){
            String[] flds = line.split(":",-1);
            Tuple t = new DefaultTuple();
            for (String fld : flds) {
                t.append((fld.compareTo("")!=0 ? new DataByteArray(fld.getBytes()) : null));
            }
            inpDB.add(t);
        }
    }
    
    

    @After
    public void tearDown() throws Exception {
    }

    @Test
    public void testGetNextTuple() throws ExecException {
        Tuple t=null;
        int size = 0;
        for(Result res = ld.getNext(t);res.returnStatus!=POStatus.STATUS_EOP;res=ld.getNext(t)){
            assertEquals(true, TestHelper.bagContains(inpDB, (Tuple)res.result));
            ++size;
        }
        assertEquals(true, size==inpDB.size());
    }

    @Test
    public void testLoadLocalRel() throws Exception {
        checkLoadPath("file:test/org/apache/pig/test/data/passwd", "", true);
    }

    @Test
    public void testLoadLocalAbs() throws Exception {
    	String filename = curDir + File.separatorChar+"test/org/apache/pig/test/data/passwd";
        if ((System.getProperty("os.name").toUpperCase().startsWith("WINDOWS")))
        {
            filename="/"+FileLocalizer.parseCygPath(filename, FileLocalizer.STYLE_WINDOWS);
            filename=Util.encodeEscape(filename);
        }
        checkLoadPath("file:"+filename, "", true);
    }

    @Test
    public void testLoadRemoteRel() throws Exception {
        checkLoadPath("test","/tmp/test");
    }

    @Test
    public void testLoadRemoteAbs() throws Exception {
        checkLoadPath("/tmp/test","/tmp/test");
    }

    @Test
    public void testLoadRemoteRelScheme() throws Exception {
        checkLoadPath("test","/tmp/test");
    }

    @Test
    public void testLoadRemoteAbsScheme() throws Exception {
        checkLoadPath("hdfs:/tmp/test","/tmp/test");
    }

    @Test
    public void testLoadRemoteAbsAuth() throws Exception {
        checkLoadPath("hdfs://localhost:9000/test","/test");
    }

    @Test
    public void testLoadRemoteNormalize() throws Exception {
        checkLoadPath("/tmp/foo/../././","/tmp");
    }

    @Test
    public void testGlobChars() throws Exception {
        checkLoadPath("t?s*","/tmp/t?s*");
    }

    private void checkLoadPath(String orig, String expected) throws Exception {
        checkLoadPath(orig, expected, false);
    }

    private void checkLoadPath(String orig, String expected, boolean isTmp) throws Exception {
        pc.getProperties().setProperty("opt.multiquery",""+true);
                
        DataStorage dfs = pc.getDfs();
        dfs.setActiveContainer(dfs.asContainer("/tmp"));
        Map<LogicalOperator, LogicalPlan> aliases = new HashMap<LogicalOperator, LogicalPlan>();
        Map<OperatorKey, LogicalOperator> logicalOpTable = new HashMap<OperatorKey, LogicalOperator>();
        Map<String, LogicalOperator> aliasOp = new HashMap<String, LogicalOperator>();
        Map<String, String> fileNameMap = new HashMap<String, String>();
        
        LogicalPlanBuilder builder = new LogicalPlanBuilder(pc);
        
        String query = "a = load '"+orig+"';";
        LogicalPlan lp = builder.parse("Test-Load",
                                       query,
                                       aliases,
                                       logicalOpTable,
                                       aliasOp,
                                       fileNameMap);
        Assert.assertTrue(lp.size()>0);
        LogicalOperator op = lp.getRoots().get(0);
        
        Assert.assertTrue(op instanceof LOLoad);
        LOLoad load = (LOLoad)op;

        String p = load.getInputFile().getFileName();
        p = p.replaceAll("hdfs://[0-9a-zA-Z:\\.]*/","/");

        if (isTmp) {
            Assert.assertTrue(p.matches("/tmp.*"));
        } else {
            Assert.assertEquals(p, expected);
        }
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintStream;
import java.net.URL;
import java.util.ArrayList;
import java.util.LinkedList;
import java.util.Collection;
import java.util.Date;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.Stack;
import org.apache.pig.impl.plan.PlanException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.backend.datastorage.ContainerDescriptor;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.ElementDescriptor;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.executionengine.ExecJob;
import org.apache.pig.backend.executionengine.ExecJob.JOB_STATUS;
import org.apache.pig.builtin.BinStorage;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.LOCogroup;
import org.apache.pig.impl.logicalLayer.LOFRJoin;
import org.apache.pig.impl.logicalLayer.LOLoad;
import org.apache.pig.impl.logicalLayer.LogicalOperator;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.impl.logicalLayer.LogicalPlanBuilder;
import org.apache.pig.impl.logicalLayer.PlanSetter;
import org.apache.pig.impl.logicalLayer.optimizer.LogicalOptimizer;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.parser.QueryParser;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.impl.plan.CompilationMessageCollector;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType;
import org.apache.pig.impl.streaming.StreamingCommand;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.ObjectSerializer;
import org.apache.pig.impl.util.PropertiesUtil;
import org.apache.pig.impl.logicalLayer.LODefine;
import org.apache.pig.impl.logicalLayer.LOStore;
import org.apache.pig.pen.ExampleGenerator;
import org.apache.pig.impl.util.LogUtils;
import org.apache.pig.tools.grunt.GruntParser;


/**
 * 
 * This class is the program's connection to Pig. Typically a program will create a PigServer
 * instance. The programmer then registers queries using registerQuery() and
 * retrieves results using openIterator() or store().
 * 
 */
public class PigServer {
    
    private final Log log = LogFactory.getLog(getClass());
    
    public static ExecType parseExecType(String str) throws IOException {
        String normStr = str.toLowerCase();
        
        if (normStr.equals("local")) return ExecType.LOCAL;
        if (normStr.equals("mapreduce")) return ExecType.MAPREDUCE;
        if (normStr.equals("mapred")) return ExecType.MAPREDUCE;
        if (normStr.equals("pig")) return ExecType.PIG;
        if (normStr.equals("pigbody")) return ExecType.PIG;
   
        int errCode = 2040;
        String msg = "Unknown exec type: " + str;
        throw new PigException(msg, errCode, PigException.BUG);
    }

    /*
     * The data structure to support grunt shell operations. 
     * The grunt shell can only work on one graph at a time. 
     * If a script is contained inside another script, the grunt
     * shell first saves the current graph on the stack and works 
     * on a new graph. After the nested script is done, the grunt 
     * shell pops up the saved graph and continues working on it.
     */
    private Stack<Graph> graphs = new Stack<Graph>();
    
    /*
     * The current Graph the grunt shell is working on.
     */
    private Graph currDAG;
 
    private PigContext pigContext;
    
    private static int scopeCounter = 0;
    private String scope = constructScope();

    private ArrayList<String> cachedScript = new ArrayList<String>();
    private boolean aggregateWarning = true;
    private boolean isMultiQuery = true;
    
    private String constructScope() {
        // scope servers for now as a session id
        
        // String user = System.getProperty("user.name", "DEFAULT_USER_ID");
        // String date = (new Date()).toString();

        // scope is not really used in the system right now. It will
        // however make your explain statements look lengthy if set to
        // username-date. For now let's simplify the scope, if a real
        // scope is needed again, we might need to update all the
        // operators to not include scope in their name().
        return ""+(++scopeCounter);
    }
    
    public PigServer(String execTypeString) throws ExecException, IOException {
        this(parseExecType(execTypeString));
    }
    
    public PigServer(ExecType execType) throws ExecException {
        this(execType, PropertiesUtil.loadPropertiesFromFile());
    }

    public PigServer(ExecType execType, Properties properties) throws ExecException {
        this(new PigContext(execType, properties));
    }
  
    public PigServer(PigContext context) throws ExecException {
        this(context, true);
    }
    
    public PigServer(PigContext context, boolean connect) throws ExecException {
        this.pigContext = context;
        currDAG = new Graph(false);
        
        aggregateWarning = "true".equalsIgnoreCase(pigContext.getProperties().getProperty("aggregate.warning"));
        isMultiQuery = "true".equalsIgnoreCase(pigContext.getProperties().getProperty("opt.multiquery","true"));

        if (connect) {
            pigContext.connect();
        }
    }
    
    public PigContext getPigContext(){
        return pigContext;
    }
    
    public void debugOn() {
        pigContext.debug = true;
    }
    
    public void debugOff() {
        pigContext.debug = false;
    }
 
    /**
     * Starts batch execution mode.
     */
    public void setBatchOn() {
        log.debug("Create a new graph.");
        
        if (currDAG != null) {
            graphs.push(currDAG);
        }
        currDAG = new Graph(isMultiQuery);
    }

    /**
     * Retrieve the current execution mode.
     * 
     * @return true if the execution mode is batch; false otherwise.
     */
    public boolean isBatchOn() {
        // Batch is on when there are multiple graphs on the
        // stack. That gives the right response even if multiquery was
        // turned off.
        return graphs.size() > 0;
    }

    /**
     * Returns whether there is anything to process in the current batch.
     * @throws FrontendException
     * @return true if there are no stores to process in the current
     * batch, false otherwise.
     */
    public boolean isBatchEmpty() throws FrontendException {
        if (currDAG == null) {
            int errCode = 1083;
            String msg = "setBatchOn() must be called first.";
            throw new FrontendException(msg, errCode, PigException.INPUT);
        }

        return currDAG.isBatchEmpty();
    }

    /**
     * Submits a batch of Pig commands for execution. 
     * 
     * @throws FrontendException
     * @throws ExecException
     */
    public List<ExecJob> executeBatch() throws FrontendException, ExecException {
        if (!isMultiQuery) {
            // ignore if multiquery is off
            return new LinkedList<ExecJob>();
        }

        if (currDAG == null || !isBatchOn()) {
            int errCode = 1083;
            String msg = "setBatchOn() must be called first.";
            throw new FrontendException(msg, errCode, PigException.INPUT);
        }
        
        return currDAG.execute();
    }

    /**
     * Discards a batch of Pig commands.
     * 
     * @throws FrontendException
     * @throws ExecException
     */
    public void discardBatch() throws FrontendException {
        if (currDAG == null || !isBatchOn()) {
            int errCode = 1083;
            String msg = "setBatchOn() must be called first.";
            throw new FrontendException(msg, errCode, PigException.INPUT);
        }
        
        currDAG = graphs.pop();
    }
       
    /**
     * Add a path to be skipped while automatically shipping binaries for 
     * streaming.
     *  
     * @param path path to be skipped
     */
    public void addPathToSkip(String path) {
        pigContext.addPathToSkip(path);
    }
    
    /**
     * Defines an alias for the given function spec. This
     * is useful for functions that require arguments to the 
     * constructor.
     * 
     * @param function - the new function alias to define.
     * @param functionSpec - the name of the function and any arguments.
     * It should have the form: classname('arg1', 'arg2', ...)
     */
    @Deprecated
    public void registerFunction(String function, String functionSpec) {
        registerFunction(function, new FuncSpec(functionSpec));
    }
    
    /**
     * Defines an alias for the given function spec. This
     * is useful for functions that require arguments to the 
     * constructor.
     * 
     * @param function - the new function alias to define.
     * @param funcSpec - the FuncSpec object representing the name of 
     * the function class and any arguments to constructor.
     */
    public void registerFunction(String function, FuncSpec funcSpec) {
        pigContext.registerFunction(function, funcSpec);
    }
    
    /**
     * Defines an alias for the given streaming command.
     * 
     * @param commandAlias - the new command alias to define
     * @param command - streaming command to be executed
     */
    public void registerStreamingCommand(String commandAlias, StreamingCommand command) {
        pigContext.registerStreamCmd(commandAlias, command);
    }

    private URL locateJarFromResources(String jarName) throws IOException {
        Enumeration<URL> urls = ClassLoader.getSystemResources(jarName);
        URL resourceLocation = null;
        
        if (urls.hasMoreElements()) {
            resourceLocation = urls.nextElement();
        }
        
        if (pigContext.debug && urls.hasMoreElements()) {
            String logMessage = "Found multiple resources that match " 
                + jarName + ": " + resourceLocation;
            
            while (urls.hasMoreElements()) {
                logMessage += (logMessage + urls.nextElement() + "; ");
            }
            
            log.debug(logMessage);
        }
    
        return resourceLocation;
    }
    
    /**
     * Registers a jar file. Name of the jar file can be an absolute or 
     * relative path.
     * 
     * If multiple resources are found with the specified name, the
     * first one is registered as returned by getSystemResources.
     * A warning is issued to inform the user.
     * 
     * @param name of the jar file to register
     * @throws IOException
     */
    public void registerJar(String name) throws IOException {
        // first try to locate jar via system resources
        // if this fails, try by using "name" as File (this preserves 
        // compatibility with case when user passes absolute path or path 
        // relative to current working directory.)        
        if (name != null) {
            URL resource = locateJarFromResources(name);

            if (resource == null) {
                File f = new File(name);
                
                if (!f.canRead()) {
                    int errCode = 4002;
                    String msg = "Can't read jar file: " + name;
                    throw new FrontendException(msg, errCode, PigException.USER_ENVIRONMENT);
                }
                
                resource = f.toURI().toURL();
            }

            pigContext.addJar(resource);        
        }
    }
    
    /**
     * Register a query with the Pig runtime. The query is parsed and registered, but it is not
     * executed until it is needed.
     * 
     * @param query
     *            a Pig Latin expression to be evaluated.
     * @param startLine
     *            line number of the query within the whold script
     * @throws IOException
     */    
    public void registerQuery(String query, int startLine) throws IOException {            
    	currDAG.registerQuery(query, startLine);
    }
 
    public LogicalPlan clonePlan(String alias) throws IOException {
        Graph graph = currDAG.clone();

        if (graph == null) {
            int errCode = 2127;
            String msg = "Cloning of plan failed.";
            throw new FrontendException(msg, errCode, PigException.BUG);
        }

        return graph.getPlan(alias);
    }
    
    public void registerQuery(String query) throws IOException {
        registerQuery(query, 1);
    }
    
    public void registerScript(String fileName) throws IOException {
        try {
            GruntParser grunt = new GruntParser(new FileReader(new File(fileName)));
            grunt.setInteractive(false);
            grunt.setParams(this);
            grunt.parseStopOnError(true);
        } catch (FileNotFoundException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
            throw new IOException(e.getCause());
        } catch (org.apache.pig.tools.pigscript.parser.ParseException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
            throw new IOException(e.getCause());
        }
    }

    public void printAliases () throws FrontendException {
        System.out.println("aliases: " + currDAG.getAliasOp().keySet());
    }

    public Schema dumpSchema(String alias) throws IOException{
        try {
            LogicalPlan lp = getPlanFromAlias(alias, "describe");
            lp = compileLp(alias, false);
            Schema schema = lp.getLeaves().get(0).getSchema();
            if (schema != null) System.out.println(alias + ": " + schema.toString());    
            else System.out.println("Schema for " + alias + " unknown.");
            return schema;
        } catch (FrontendException fee) {
            int errCode = 1001;
            String msg = "Unable to describe schema for alias " + alias; 
            throw new FrontendException (msg, errCode, PigException.INPUT, false, null, fee);
        }
    }

    public void setJobName(String name){
        currDAG.setJobName(name);
    }
    
    /**
     * Forces execution of query (and all queries from which it reads), in order to materialize
     * result
     */
    public Iterator<Tuple> openIterator(String id) throws IOException {
        try {
            LogicalOperator op = currDAG.getAliasOp().get(id);
            if(null == op) {
                int errCode = 1003;
                String msg = "Unable to find an operator for alias " + id;
                throw new FrontendException(msg, errCode, PigException.INPUT);
            }

            if (currDAG.isBatchOn()) {
                currDAG.execute();
            }
            ExecJob job = store(id, FileLocalizer.getTemporaryPath(null, pigContext).toString(), BinStorage.class.getName() + "()");
            
            // invocation of "execute" is synchronous!

            if (job.getStatus() == JOB_STATUS.COMPLETED) {
                return job.getResults();
            } else if (job.getStatus() == JOB_STATUS.FAILED
                       && job.getException() != null) {
                // throw the backend exception in the failed case
                throw job.getException();
            } else {
                throw new IOException("Job terminated with anomalous status "
                    + job.getStatus().toString());
            }
        } catch (Exception e) {
            int errCode = 1066;
            String msg = "Unable to open iterator for alias " + id; 
            throw new FrontendException(msg, errCode, PigException.INPUT, e);
        }
    }
    
    /**
     * Store an alias into a file
     * @param id The alias to store
     * @param filename The file to which to store to
     * @throws IOException
     */

    public ExecJob store(String id, String filename) throws IOException {
        return store(id, filename, PigStorage.class.getName() + "()");   // SFPig is the default store function
    }
        
    /**
     *  forces execution of query (and all queries from which it reads), in order to store result in file
     */        
    public ExecJob store(
            String id,
            String filename,
            String func) throws IOException {

        if (!currDAG.getAliasOp().containsKey(id)) {
            throw new IOException("Invalid alias: " + id);
        }

        try {
            LogicalPlan lp = compileLp(id);

            // MRCompiler needs a store to be the leaf - hence
            // add a store to the plan to explain
            
            // figure out the leaf to which the store needs to be added
            List<LogicalOperator> leaves = lp.getLeaves();
            LogicalOperator leaf = null;
            if(leaves.size() == 1) {
                leaf = leaves.get(0);
            } else {
                for (Iterator<LogicalOperator> it = leaves.iterator(); it.hasNext();) {
                    LogicalOperator leafOp = it.next();
                    if(leafOp.getAlias().equals(id))
                        leaf = leafOp;
                }
            }
            
            LogicalPlan storePlan = QueryParser.generateStorePlan(scope, lp, filename, func, leaf);
            List<ExecJob> jobs = executeCompiledLogicalPlan(storePlan);
            if (jobs.size() < 1) {
                throw new IOException("Couldn't retrieve job.");
            }
            return jobs.get(0);
        } catch (Exception e) {
            int errCode = 1002;
            String msg = "Unable to store alias " + id;
            throw new FrontendException(msg, errCode, PigException.INPUT, e);
        }

    }

    /**
     * Provide information on how a pig query will be executed.  For now
     * this information is very developer focussed, and probably not very
     * useful to the average user.
     * @param alias Name of alias to explain.
     * @param stream PrintStream to write explanation to.
     * @throws IOException if the requested alias cannot be found.
     */
    public void explain(String alias,
                        PrintStream stream) throws IOException {
        explain(alias, "text", true, false, stream, stream, stream);
    }

    /**
     * Provide information on how a pig query will be executed.
     * @param alias Name of alias to explain.
     * @param format Format in which the explain should be printed
     * @param verbose Controls the amount of information printed
     * @param markAsExecute When set will treat the explain like a
     * call to execute in the respoect that all the pending stores are
     * marked as complete.
     * @param lps Stream to print the logical tree
     * @param pps Stream to print the physical tree
     * @param eps Stream to print the execution tree
     * @throws IOException if the requested alias cannot be found.
     */
    public void explain(String alias,
                        String format,
                        boolean verbose,
                        boolean markAsExecute,
                        PrintStream lps,
                        PrintStream pps,
                        PrintStream eps) throws IOException {
        try {
            pigContext.inExplain = true;
            LogicalPlan lp = getStorePlan(alias);
            if (lp.size() == 0) {
                lps.println("Logical plan is empty.");
                pps.println("Physical plan is empty.");
                eps.println("Execution plan is empty.");
                return;
            }
            PhysicalPlan pp = compilePp(lp);
            lp.explain(lps, format, verbose);
            pp.explain(pps, format, verbose);
            pigContext.getExecutionEngine().explain(pp, eps, format, verbose);
            if (markAsExecute) {
                currDAG.markAsExecuted();
            }
        } catch (Exception e) {
            int errCode = 1067;
            String msg = "Unable to explain alias " + alias;
            throw new FrontendException(msg, errCode, PigException.INPUT, e);
        } finally {
            pigContext.inExplain = false;
        }
    }

    /**
     * Returns the unused byte capacity of an HDFS filesystem. This value does
     * not take into account a replication factor, as that can vary from file
     * to file. Thus if you are using this to determine if you data set will fit
     * in the HDFS, you need to divide the result of this call by your specific replication
     * setting. 
     * @return unused byte capacity of the file system.
     * @throws IOException
     */
    public long capacity() throws IOException {
        if (pigContext.getExecType() == ExecType.LOCAL) {
            throw new IOException("capacity only supported for non-local execution");
        } 
        else {
            DataStorage dds = pigContext.getDfs();
            
            Map<String, Object> stats = dds.getStatistics();

            String rawCapacityStr = (String) stats.get(DataStorage.RAW_CAPACITY_KEY);
            String rawUsedStr = (String) stats.get(DataStorage.RAW_USED_KEY);
            
            if ((rawCapacityStr == null) || (rawUsedStr == null)) {
                throw new IOException("Failed to retrieve capacity stats");
            }
            
            long rawCapacityBytes = new Long(rawCapacityStr).longValue();
            long rawUsedBytes = new Long(rawUsedStr).longValue();
            
            return rawCapacityBytes - rawUsedBytes;
        }
    }

    /**
     * Returns the length of a file in bytes which exists in the HDFS (accounts for replication).
     * @param filename
     * @return length of the file in bytes
     * @throws IOException
     */
    public long fileSize(String filename) throws IOException {
        DataStorage dfs = pigContext.getDfs();
        ElementDescriptor elem = dfs.asElement(filename);
        Map<String, Object> stats = elem.getStatistics();
        long length = (Long) stats.get(ElementDescriptor.LENGTH_KEY);
        int replication = (Short) stats
                .get(ElementDescriptor.BLOCK_REPLICATION_KEY);

        return length * replication;
    }
    
    public boolean existsFile(String filename) throws IOException {
        ElementDescriptor elem = pigContext.getDfs().asElement(filename);
        return elem.exists();
    }
    
    public boolean deleteFile(String filename) throws IOException {
        ElementDescriptor elem = pigContext.getDfs().asElement(filename);
        elem.delete();
        return true;
    }
    
    public boolean renameFile(String source, String target) throws IOException {
        pigContext.rename(source, target);
        return true;
    }
    
    public boolean mkdirs(String dirs) throws IOException {
        ContainerDescriptor container = pigContext.getDfs().asContainer(dirs);
        container.create();
        return true;
    }
    
    public String[] listPaths(String dir) throws IOException {
        Collection<String> allPaths = new ArrayList<String>();
        ContainerDescriptor container = pigContext.getDfs().asContainer(dir);
        Iterator<ElementDescriptor> iter = container.iterator();
            
        while (iter.hasNext()) {
            ElementDescriptor elem = iter.next();
            allPaths.add(elem.toString());
        }
            
        return (String[])(allPaths.toArray());
    }
    
    public long totalHadoopTimeSpent() {
//      TODO FIX Need to uncomment this with the right logic
//        return MapReduceLauncher.totalHadoopTimeSpent;
        return 0L;
    }
  
    public Map<String, LogicalPlan> getAliases() {
        Map<String, LogicalPlan> aliasPlans = new HashMap<String, LogicalPlan>();
        for(LogicalOperator op:  currDAG.getAliases().keySet()) {
            String alias = op.getAlias();
            if(null != alias) {
                aliasPlans.put(alias, currDAG.getAliases().get(op));
            }
        }
        return aliasPlans;
    }
    
    public void shutdown() {
        // clean-up activities
            // TODO: reclaim scope to free up resources. Currently
        // this is not implemented and throws an exception
            // hence, for now, we won't call it.
        //
        // pigContext.getExecutionEngine().reclaimScope(this.scope);
    }

    public Set<String> getAliasKeySet() {
        return currDAG.getAliasOp().keySet();
    }

    public Map<LogicalOperator, DataBag> getExamples(String alias) {
        LogicalPlan plan = null;

        try {        
            if (currDAG.isBatchOn()) {
                currDAG.execute();
            }
            
            plan = clonePlan(alias);
        } catch (IOException e) {
            //Since the original script is parsed anyway, there should not be an
            //error in this parsing. The only reason there can be an error is when
            //the files being loaded in load don't exist anymore.
            e.printStackTrace();
        }
        ExampleGenerator exgen = new ExampleGenerator(plan, pigContext);
        return exgen.getExamples();
    }

    private LogicalPlan getStorePlan(String alias) throws IOException {
        LogicalPlan lp = compileLp(alias);
        
        if (!isBatchOn() || alias != null) {
            // MRCompiler needs a store to be the leaf - hence
            // add a store to the plan to explain
            
            // figure out the leaves to which stores need to be added
            List<LogicalOperator> leaves = lp.getLeaves();
            LogicalOperator leaf = null;
            if(leaves.size() == 1) {
                leaf = leaves.get(0);
            } else {
                for (Iterator<LogicalOperator> it = leaves.iterator(); it.hasNext();) {
                    LogicalOperator leafOp = it.next();
                    if(leafOp.getAlias().equals(alias))
                        leaf = leafOp;
                }
            }
            
            lp = QueryParser.generateStorePlan(scope, lp, "fakefile", 
                                               PigStorage.class.getName(), leaf);
        }

        return lp;
    }
    
    private List<ExecJob> execute(String alias) throws FrontendException, ExecException {
        LogicalPlan typeCheckedLp = compileLp(alias);

        if (typeCheckedLp.size() == 0) {
            return new LinkedList<ExecJob>();
        }

        LogicalOperator op = typeCheckedLp.getLeaves().get(0);
        if (op instanceof LODefine) {
            log.info("Skip execution of DEFINE only logical plan.");
            return new LinkedList<ExecJob>();
        }

        return executeCompiledLogicalPlan(typeCheckedLp);
    }
    
    private List<ExecJob> executeCompiledLogicalPlan(LogicalPlan compiledLp) throws ExecException {
        PhysicalPlan pp = compilePp(compiledLp);
        // execute using appropriate engine
        FileLocalizer.clearDeleteOnFail();
        List<ExecJob> execJobs = pigContext.getExecutionEngine().execute(pp, "execute");
        for (ExecJob execJob: execJobs) {
            if (execJob.getStatus()==ExecJob.JOB_STATUS.FAILED) {
                FileLocalizer.triggerDeleteOnFail();
                break;
            }
        }
        return execJobs;
    }

    private LogicalPlan compileLp(
            String alias) throws FrontendException {
        return compileLp(alias, true);
    }

    private LogicalPlan compileLp(
            String alias,
            boolean optimize) throws FrontendException {
        
        // create a clone of the logical plan and give it
        // to the operations below
        LogicalPlan lpClone;
 
        try {
            lpClone = clonePlan(alias);
        } catch (IOException e) {
            int errCode = 2001;
            String msg = "Unable to clone plan before compiling";
            throw new FrontendException(msg, errCode, PigException.BUG, e);
        }
        
        // Set the logical plan values correctly in all the operators
        PlanSetter ps = new PlanSetter(lpClone);
        ps.visit();
        
        // run through validator
        CompilationMessageCollector collector = new CompilationMessageCollector() ;
        FrontendException caught = null;
        try {
            LogicalPlanValidationExecutor validator = 
                new LogicalPlanValidationExecutor(lpClone, pigContext);
            validator.validate(lpClone, collector);
        } catch (FrontendException fe) {
            // Need to go through and see what the collector has in it.  But
            // remember what we've caught so we can wrap it into what we
            // throw.
            caught = fe;            
        }
        
        if(aggregateWarning) {
        	CompilationMessageCollector.logMessages(collector, MessageType.Warning, aggregateWarning, log);
        } else {
        	for(Enum type: MessageType.values()) {
        		CompilationMessageCollector.logAllMessages(collector, log);
        	}
        }
        
        if (caught != null) {
            throw caught;
        }

        // optimize
        if (optimize) {
            HashSet<String> optimizerRules = null;
            try {
                optimizerRules = (HashSet<String>) ObjectSerializer
                        .deserialize(pigContext.getProperties().getProperty(
                                "pig.optimizer.rules"));
            } catch (IOException ioe) {
                int errCode = 2110;
                String msg = "Unable to deserialize optimizer rules.";
                throw new FrontendException(msg, errCode, PigException.BUG, ioe);
            }

            LogicalOptimizer optimizer = new LogicalOptimizer(lpClone, pigContext.getExecType(), optimizerRules);
            optimizer.optimize();
        }

        return lpClone;
    }

    private PhysicalPlan compilePp(LogicalPlan lp) throws ExecException {
        // translate lp to physical plan
        PhysicalPlan pp = pigContext.getExecutionEngine().compile(lp, null);

        // TODO optimize

        return pp;
    }

    private LogicalPlan getPlanFromAlias(
            String alias,
            String operation) throws FrontendException {
        LogicalOperator lo = currDAG.getAliasOp().get(alias);
        if (lo == null) {
            int errCode = 1004;
            String msg = "No alias " + alias + " to " + operation;
            throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
        }
        LogicalPlan lp = currDAG.getAliases().get(lo);
        if (lp == null) {
            int errCode = 1005;
            String msg = "No plan for " + alias + " to " + operation;
            throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
        }        
        return lp;
    }

    /*
     * This class holds the internal states of a grunt shell session.
     */
    private class Graph {
    	
        private Map<LogicalOperator, LogicalPlan> aliases = new HashMap<LogicalOperator, LogicalPlan>();
        
        private Map<OperatorKey, LogicalOperator> opTable = new HashMap<OperatorKey, LogicalOperator>();
        
        private Map<String, LogicalOperator> aliasOp = new HashMap<String, LogicalOperator>();
       
        private List<String> scriptCache = new ArrayList<String>();	

        // the fileNameMap contains filename to canonical filename
        // mappings. This is done so we can reparse the cached script
        // and remember the translation (current directory might only
        // be correct during the first parse
        private Map<String, String> fileNameMap = new HashMap<String, String>();
    
        private Map<LOStore, LogicalPlan> storeOpTable = new HashMap<LOStore, LogicalPlan>();
        
        private Set<LOLoad> loadOps = new HashSet<LOLoad>();

        private String jobName;
        
        private boolean batchMode;

        private int processedStores;

        private int ignoreNumStores;
        
        private LogicalPlan lp;
        
        Graph(boolean batchMode) { 
            this.batchMode = batchMode;
            this.processedStores = 0;
            this.ignoreNumStores = 0;
            this.jobName = pigContext.getProperties().getProperty(PigContext.JOB_NAME,
                                                                  PigContext.JOB_NAME_PREFIX+":DefaultJobName");
            this.lp = new LogicalPlan();
        };
        
        Map<LogicalOperator, LogicalPlan> getAliases() { return aliases; }
        
        Map<OperatorKey, LogicalOperator> getOpTable() { return opTable; }
        
        Map<String, LogicalOperator> getAliasOp() { return aliasOp; }
        
        List<String> getScriptCache() { return scriptCache; }
        
        boolean isBatchOn() { return batchMode; };

        boolean isBatchEmpty() { return processedStores == storeOpTable.keySet().size(); }
        
        List<ExecJob> execute() throws ExecException, FrontendException {
            pigContext.getProperties().setProperty(PigContext.JOB_NAME, jobName);
            List<ExecJob> jobs = PigServer.this.execute(null);
            processedStores = storeOpTable.keySet().size();
            return jobs;
        }

        void markAsExecuted() {
            processedStores = storeOpTable.keySet().size();
        }

        void setJobName(String name) {
            jobName = PigContext.JOB_NAME_PREFIX+":"+name;
        }

        LogicalPlan getPlan(String alias) throws IOException {
            LogicalPlan plan = lp;
                
            if (alias != null) {
                LogicalOperator op = aliasOp.get(alias);
                if(op == null) {
                    int errCode = 1003;
                    String msg = "Unable to find an operator for alias " + alias;
                    throw new FrontendException(msg, errCode, PigException.INPUT);
                }
                plan = aliases.get(op);
            }
            return plan;
        }

        void registerQuery(String query, int startLine) throws IOException {
            
            LogicalPlan tmpLp = parseQuery(query, startLine);
            
            // store away the query for use in cloning later
            scriptCache.add(query);
            if (tmpLp.getLeaves().size() == 1) {
                LogicalOperator op = tmpLp.getSingleLeafPlanOutputOp();
                
                // Check if we just processed a LOStore i.e. STORE
                if (op instanceof LOStore) {

                    if (!batchMode) {
                        lp = tmpLp;
                        try {
                            execute();
                        } catch (Exception e) {
                            int errCode = 1002;
                            String msg = "Unable to store alias "
                                    + op.getOperatorKey().getId();
                            throw new FrontendException(msg, errCode,
                                    PigException.INPUT, e);
                        }
                    } else {
                        if (0 == ignoreNumStores) {
                            storeOpTable.put((LOStore)op, tmpLp);
                            lp.mergeSharedPlan(tmpLp);
                            List<LogicalOperator> roots = tmpLp.getRoots();
                            for (LogicalOperator root : roots) {
                                if (root instanceof LOLoad) {
                                    loadOps.add((LOLoad)root);
                                }
                            }

                        } else {
                            --ignoreNumStores;
                        }
                    }
                }
            }
        }        
    
        LogicalPlan parseQuery(String query, int startLine) throws IOException {        
            if (query == null || query.length() == 0) { 
                int errCode = 1084;
                String msg = "Invalid Query: Query is null or of size 0";
                throw new FrontendException(msg, errCode, PigException.INPUT);
            }

            query = query.trim();
        
            try {
                return new LogicalPlanBuilder(PigServer.this.pigContext).parse(scope, query,
                                              aliases, opTable, aliasOp, startLine, fileNameMap);
            } catch (ParseException e) {
                PigException pe = LogUtils.getPigException(e);
                int errCode = 1000;
                String msg = "Error during parsing. " + (pe == null? e.getMessage() : pe.getMessage());
                throw new FrontendException(msg, errCode, PigException.INPUT, false, null, e);
            }
        }

        protected Graph clone() {
            // There are two choices on how we clone the logical plan
            // 1 - we really clone each operator and connect up the cloned operators
            // 2 - we cache away the script till the point we need to clone
            // and then simply re-parse the script. 
            // The latter approach is used here
            // FIXME: There is one open issue with this now:
            // Consider the following script:
            // A = load 'file:/somefile';
            // B = filter A by $0 > 10;
            // store B into 'bla';
            // rm 'file:/somefile';
            // A = load 'file:/someotherfile'
            // when we try to clone - we try to reparse
            // from the beginning and currently the parser
            // checks for file existence of files in the load
            // in the case where the file is a local one -i.e. with file: prefix
            // This will be a known issue now and we will need to revisit later
            
            // parse each line of the cached script
            int lineNumber = 1;
            
            // create data structures needed for parsing        
            Graph graph = new Graph(isBatchOn());
            graph.ignoreNumStores = processedStores;
            graph.processedStores = processedStores;
            graph.fileNameMap = fileNameMap;
            
            try {
                for (Iterator<String> it = getScriptCache().iterator(); it.hasNext(); lineNumber++) {
                    if (isBatchOn()) {
                        graph.registerQuery(it.next(), lineNumber);
                    } else {
                        graph.lp = graph.parseQuery(it.next(), lineNumber);
                    }
                }
                graph.postProcess();
            } catch (IOException ioe) {
                graph = null;
            }          
            return graph;
        }
        
        private void postProcess() throws IOException {
            
            // Set the logical plan values correctly in all the operators
            PlanSetter ps = new PlanSetter(lp);
            ps.visit();

            // The following code deals with store/load combination of 
            // intermediate files. In this case we will replace the load operator
            // with a (implicit) split operator, iff the load/store
            // func is reversible (because that's when we can safely
            // skip the load and keep going with the split output). If
            // the load/store func is not reversible (or they are
            // different functions), we connect the store and the load
            // to remember the dependency.
            for (LOLoad load : loadOps) {
                for (LOStore store : storeOpTable.keySet()) {
                    String ifile = load.getInputFile().getFileName();
                    String ofile = store.getOutputFile().getFileName();
                    if (ofile.compareTo(ifile) == 0) {
                        LoadFunc lFunc = (LoadFunc) pigContext.instantiateFuncFromSpec(load.getInputFile().getFuncSpec());
                        StoreFunc sFunc = (StoreFunc) pigContext.instantiateFuncFromSpec(store.getOutputFile().getFuncSpec());
                        if (lFunc.getClass() == sFunc.getClass() && lFunc instanceof ReversibleLoadStoreFunc) {
                            
                            log.info("Removing unnecessary load operation from location: "+ifile);
                            
                            // In this case we remember the input file
                            // spec in the store. We might have to use it
                            // in the MR compiler to recreate the load, if
                            // the store happens on a job boundary.
                            store.setInputSpec(load.getInputFile());

                            LogicalOperator storePred = lp.getPredecessors(store).get(0);
                            
                            // In this case we remember the input file
                            // spec in the store. We might have to use it
                            // in the MR compiler to recreate the load, if
                            // the store happens on a job boundary.
                            store.setInputSpec(load.getInputFile());
                            
                            lp.disconnect(store, load);
                            lp.replace(load, storePred);

                            List<LogicalOperator> succs = lp.getSuccessors(storePred);
                            
                            for (LogicalOperator succ : succs) {
                                MultiMap<LogicalOperator, LogicalPlan> innerPls = null;
                                
                                // fix inner plans for cogroup and frjoin operators
                                if (succ instanceof LOCogroup) {
                                    innerPls = ((LOCogroup)succ).getGroupByPlans();
                                } else if (succ instanceof LOFRJoin) {
                                    innerPls = ((LOFRJoin)succ).getJoinColPlans();
                                }
                                
                                if (innerPls != null) {
                                    if (innerPls.containsKey(load)) {
                                        Collection<LogicalPlan> pls = innerPls.get(load);
                                        innerPls.removeKey(load);
                                        innerPls.put(storePred, pls);
                                    }
                                }
                            }
                        } else {
                            try {
                                store.getPlan().connect(store, load);
                            } catch (PlanException ex) {
                                int errCode = 2128;
                                String msg = "Failed to connect store with dependent load.";
                                throw new FrontendException(msg, errCode, ex);
                            }
                        }
                    }
                }
            }
        }
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.data;

import java.io.IOException;
import java.io.Serializable;
import java.io.UnsupportedEncodingException;
import java.lang.StringBuilder;
import java.util.ArrayList;
import java.util.Collection;

/**
 * An implementation of byte array.  This is done as an object because we
 * need to be able to implement compareTo, toString, hashCode, and some
 * other methods.
 */
public class DataByteArray implements Comparable, Serializable {
    byte[] mData = null;

    /**
     * Default constructor.  The data array will not be allocated when this
     * constructor is called.
     */
    public DataByteArray() {
    }

    /**
     * Construct a byte array using the provided bytes as the content.
     * @param b byte array to use as content.  A reference to the bytes
     * will be taken, the underlying bytes will not be copied.
     */
    public DataByteArray(byte[] b) {
        mData = b;
    }

    /**
     * Construct a byte array concatenating the two provided 
     * byte arrays as the content.
     * @param b the first byte array to use as content.
     * @param c the other byte array to use as content.
     * 
     */
    public DataByteArray(DataByteArray b, DataByteArray c) {
        byte[] ba = (b == null) ?  null : b.get();
        byte[] ca = (c == null) ?  null : c.get();
        int baLength = (ba == null) ? 0 : ba.length;
        int caLength = (ca == null) ? 0 : ca.length;
        
        int totalSize = baLength + caLength;
        if(totalSize == 0) {
            return;
        }
        mData = new byte[totalSize];
        int i = 0;
        for ( ;i < baLength; i++) {
            mData[i] = ba[i];
        }
        
        for (int j = 0; j < caLength; j++, i++) {
            mData[i] = ca[j];
        }
        
    }
    
    /**
     * Construct a byte array using a portion of the provided bytes as content.
     * @param b byte array to read from.  A copy of the underlying bytes will be
     * made.
     * @param start starting point to copy from
     * @param end ending point to copy to, exclusive.
     */
    public DataByteArray(byte[] b, int start, int end) {
        mData = new byte[end - start];
        for (int i = start; i < end; i++) {
            mData[i - start] = b[i];
        }
    }

    /**
     * Construct a byte array from a String.  The contents of the string
     * are copied.
     * @param s String to make a byte array out of.
     */
    public DataByteArray(String s) {
        try {
			mData = s.getBytes("UTF8");
		} catch (UnsupportedEncodingException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
    }

    /**
     * Find the size of the byte array.
     * @return number of bytes in the array.
     */
    public int size() {
        return mData.length;
    }

    /**
     * Get the underlying byte array.  This is the real thing, not a copy,
     * so don't mess with it!
     * @return underlying byte[]
     */
    public byte[] get() {
        return mData;
    }

    /**
     * Set the internal byte array.  This should not be called unless the
     * default constructor was used.
     * @param b byte array to store.  The contents of the byte array are
     * not copied.
     */
    public void set(byte[] b) {
        mData = b;
    }

    /**
     * Set the internal byte array.  This should not be called unless the
     * default constructor was used.
     * @param s String to copy.  The contents of the string are copied.
     */
    public void set(String s) {
        mData = s.getBytes();
    }

    @Override
    public String toString() {
        String r=null;
    	try {
			r = new String(mData, "UTF8");
		} catch (Exception e) {
			// TODO: handle exception
		}
		return r;
    }

    /**
     * Compare two byte arrays.  Comparison is done first using byte values
     * then length.  So "g" will be greater than "abcdefg", but "hello worlds"
     * is greater than "hello world".  If the other object is not a
     * DataByteArray, DataType.compare will be called.
     * @param other Other object to compare to.
     * @return -1 if less than, 1 if greater than, 0 if equal.
     */
    public int compareTo(Object other) {
        if (other instanceof DataByteArray) {
            DataByteArray dba = (DataByteArray)other;
            int mySz = mData.length;
            int tSz = dba.mData.length;
            int i;
            for (i = 0; i < mySz; i++) {
                // If the other has run out of characters, we're bigger.
                if (i >= tSz) return 1;
                if (mData[i] < dba.mData[i]) return -1;
                else if (mData[i] > dba.mData[i]) return 1;
            }
            // If the other still has characters left, it's greater
            if (i < tSz) return -1;
            return 0;
        } else {
            return DataType.compare(this, other);
        }
    }

    @Override
    public boolean equals(Object other) {
        return (compareTo(other) == 0);
    }

    @Override
    public int hashCode() {
        int hash = 1;
        for (int i = 0; i < mData.length; i++) {
            // 29 chosen because hash uses 31 and bag 37, and a I want a
            // prime.
            hash = 29 * hash + mData[i];
        }
        return hash;
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer.optimizer;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.LOCast;
import org.apache.pig.impl.logicalLayer.LOForEach;
import org.apache.pig.impl.logicalLayer.LOLoad;
import org.apache.pig.impl.logicalLayer.LOProject;
import org.apache.pig.impl.logicalLayer.LOStream;
import org.apache.pig.impl.logicalLayer.LogicalOperator;
import org.apache.pig.impl.logicalLayer.LogicalPlan;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.DepthFirstWalker;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.optimizer.OptimizerException;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.impl.streaming.StreamingCommand;
import org.apache.pig.impl.streaming.StreamingCommand.HandleSpec;

/**
 * A visitor to discover if any schema has been specified for a file being
 * loaded.  If so, a projection will be injected into the plan to cast the
 * data being loaded to the appropriate types.  The optimizer can then come
 * along and move those casts as far down as possible, or in some cases remove
 * them altogether.  This visitor does not handle finding the schemas for the 
 * file, that has already been done as part of parsing.
 *
 */
public class TypeCastInserter extends LogicalTransformer {

    private String operatorClassName;

    public TypeCastInserter(LogicalPlan plan, String operatorClassName) {
        super(plan, new DepthFirstWalker<LogicalOperator, LogicalPlan>(plan));
        this.operatorClassName = operatorClassName;
    }

    @Override
    public boolean check(List<LogicalOperator> nodes) throws OptimizerException {       
        try {
            LogicalOperator op = getOperator(nodes);
            Schema s = op.getSchema();
            if (s == null) return false;
    
            boolean sawOne = false;
            List<Schema.FieldSchema> fss = s.getFields();
            List<Byte> types = new ArrayList<Byte>(s.size());
            Schema determinedSchema = null;
            if(LOLoad.class.getName().equals(operatorClassName)) {
                determinedSchema = ((LOLoad)op).getDeterminedSchema();
            }
            for (int i = 0; i < fss.size(); i++) {
                if (fss.get(i).type != DataType.BYTEARRAY) {
                    if(determinedSchema == null || 
                            (fss.get(i).type != determinedSchema.getField(i).type)) {
                            // Either no schema was determined by loader OR the type 
                            // from the "determinedSchema" is different
                            // from the type specified - so we need to cast
                            sawOne = true;
                        }
                }
                types.add(fss.get(i).type);
            }

            // If all we've found are byte arrays, we don't need a projection.
            return sawOne;
        } catch(OptimizerException oe) {
            throw oe;
        } catch (Exception e) {
            int errCode = 2004;
            String msg = "Internal error while trying to check if type casts are needed";
            throw new OptimizerException(msg, errCode, PigException.BUG, e);
        }
    }
    
    private LogicalOperator getOperator(List<LogicalOperator> nodes) throws FrontendException {
        if((nodes == null) || (nodes.size() <= 0)) {
            int errCode = 2052;
            String msg = "Internal error. Cannot retrieve operator from null or empty list.";
            throw new OptimizerException(msg, errCode, PigException.BUG);
        }
        
        LogicalOperator lo = nodes.get(0);
        if(LOLoad.class.getName().equals(operatorClassName)) {
            if (lo == null || !(lo instanceof LOLoad)) {
                int errCode = 2005;
                String msg = "Expected " + LOLoad.class.getSimpleName() + ", got " + lo.getClass().getSimpleName();
                throw new OptimizerException(msg, errCode, PigException.BUG);
            }
    
            return lo;
        } else if(LOStream.class.getName().equals(operatorClassName)){
            if (lo == null || !(lo instanceof LOStream)) {
                int errCode = 2005;
                String msg = "Expected " + LOStream.class.getSimpleName() + ", got " + lo.getClass().getSimpleName();
                throw new OptimizerException(msg, errCode, PigException.BUG);
            }
    
            return lo;
        } else {
            // we should never be called with any other operator class name
            int errCode = 1034;
            String msg = "TypeCastInserter invoked with an invalid operator class name:" + operatorClassName;
            throw new OptimizerException(msg, errCode, PigException.INPUT);
        }
   
    }

    @Override
    public void transform(List<LogicalOperator> nodes) throws OptimizerException {
        try {
            LogicalOperator lo = getOperator(nodes);
            Schema s = lo.getSchema();
            String scope = lo.getOperatorKey().scope;
            // For every field, build a logical plan.  If the field has a type
            // other than byte array, then the plan will be cast(project).  Else
            // it will just be project.
            ArrayList<LogicalPlan> genPlans = new ArrayList<LogicalPlan>(s.size());
            ArrayList<Boolean> flattens = new ArrayList<Boolean>(s.size());
            Map<String, Byte> typeChanges = new HashMap<String, Byte>();
            // if we are inserting casts in a load and if the loader
            // implements determineSchema(), insert casts only where necessary
            // Note that in this case, the data coming out of the loader is not
            // a BYTEARRAY but is whatever determineSchema() says it is.
            Schema determinedSchema = null;
            if(LOLoad.class.getName().equals(operatorClassName)) {
                determinedSchema = ((LOLoad)lo).getDeterminedSchema();
            }
            for (int i = 0; i < s.size(); i++) {
                LogicalPlan p = new LogicalPlan();
                genPlans.add(p);
                flattens.add(false);
                List<Integer> toProject = new ArrayList<Integer>(1);
                toProject.add(i);
                LOProject proj = new LOProject(p, OperatorKey.genOpKey(scope),
                    lo, toProject);
                p.add(proj);
                Schema.FieldSchema fs = s.getField(i);
                if (fs.type != DataType.BYTEARRAY) {
                    if(determinedSchema == null || (fs.type != determinedSchema.getField(i).type)) {
                            // Either no schema was determined by loader OR the type 
                            // from the "determinedSchema" is different
                            // from the type specified - so we need to cast
                            LOCast cast = new LOCast(p, 
                                        OperatorKey.genOpKey(scope), fs.type);
                            p.add(cast);
                            p.connect(proj, cast);
                            
                            cast.setFieldSchema(fs.clone());
                            FuncSpec loadFuncSpec = null;
                            if(lo instanceof LOLoad) {
                                loadFuncSpec = ((LOLoad)lo).getInputFile().getFuncSpec();
                            } else if (lo instanceof LOStream) {
                                StreamingCommand command = ((LOStream)lo).getStreamingCommand();
                                HandleSpec streamOutputSpec = command.getOutputSpec(); 
                                loadFuncSpec = new FuncSpec(streamOutputSpec.getSpec());
                            } else {
                                int errCode = 2006;
                                String msg = "TypeCastInserter invoked with an invalid operator class name: " + lo.getClass().getSimpleName();
                                throw new OptimizerException(msg, errCode, PigException.BUG);
                            }
                            cast.setLoadFuncSpec(loadFuncSpec);
                            typeChanges.put(fs.canonicalName, fs.type);
                            if(determinedSchema == null) {
                                // Reset the loads field schema to byte array so that it
                                // will reflect reality.
                                fs.type = DataType.BYTEARRAY;
                            } else {
                                // Reset the type to what determinedSchema says it is
                                fs.type = determinedSchema.getField(i).type;
                            }
                        }
                }
            }

            // Build a foreach to insert after the load, giving it a cast for each
            // position that has a type other than byte array.
            LOForEach foreach = new LOForEach(mPlan,
                OperatorKey.genOpKey(scope), genPlans, flattens);
            foreach.setAlias(lo.getAlias());
            // Insert the foreach into the plan and patch up the plan.
            insertAfter(lo, foreach, null);

            rebuildSchemas();

        } catch (OptimizerException oe) {
            throw oe;
        } catch (Exception e) {
            int errCode = 2007;
            String msg = "Unable to insert type casts into plan"; 
            throw new OptimizerException(msg, errCode, PigException.BUG, e);
        }
    }
}

 

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.tools.grunt;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.Reader;
import java.io.FileReader;
import java.io.FileInputStream;
import java.io.OutputStreamWriter;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.FileOutputStream;
import java.io.InputStreamReader;
import java.io.FileNotFoundException;
import java.io.StringReader;
import java.io.StringWriter;
import java.util.Iterator;
import java.util.Map;
import java.util.List;
import java.util.ArrayList;
import java.util.Properties;
import java.util.Date;
import java.io.ByteArrayOutputStream;
import java.io.ByteArrayInputStream;
import java.io.PrintStream;

import jline.ConsoleReader;
import jline.ConsoleReaderInputStream;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.JobID;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigServer;
import org.apache.pig.backend.datastorage.ContainerDescriptor;
import org.apache.pig.backend.datastorage.DataStorage;
import org.apache.pig.backend.datastorage.DataStorageException;
import org.apache.pig.backend.datastorage.ElementDescriptor;
import org.apache.pig.backend.executionengine.ExecutionEngine;
import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
import org.apache.pig.backend.executionengine.ExecJob;
import org.apache.pig.backend.executionengine.ExecJob.JOB_STATUS;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.util.WrappedIOException;
import org.apache.pig.tools.pigscript.parser.ParseException;
import org.apache.pig.tools.pigscript.parser.PigScriptParser;
import org.apache.pig.tools.pigscript.parser.PigScriptParserTokenManager;
import org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor;
import org.apache.pig.impl.util.LogUtils;

public class GruntParser extends PigScriptParser {

    private final Log log = LogFactory.getLog(getClass());

    public GruntParser(Reader stream) {
        super(stream);
        init();
    }

    public GruntParser(InputStream stream, String encoding) {
        super(stream, encoding);
        init();
    }

    public GruntParser(InputStream stream) {
        super(stream);
        init();
    }

    public GruntParser(PigScriptParserTokenManager tm) {
        super(tm);
        init();
    }

    private void init() {
        mDone = false;
        mLoadOnly = false;
        mExplain = null;
    }

    private void setBatchOn() {
        mPigServer.setBatchOn();
    }

    private void executeBatch() throws IOException {
        if (mPigServer.isBatchOn()) {
            if (mExplain != null) {
                explainCurrentBatch();
            }

            if (!mLoadOnly) {
                List<ExecJob> jobs = mPigServer.executeBatch();
                for(ExecJob job: jobs) {
                    if (job.getStatus() == ExecJob.JOB_STATUS.FAILED) {
                        mNumFailedJobs++;
                        if (job.getException() != null) {
                            LogUtils.writeLog(
                              job.getException(), 
                              mPigServer.getPigContext().getProperties().getProperty("pig.logfile"), 
                              log, 
                              "true".equalsIgnoreCase(mPigServer.getPigContext().getProperties().getProperty("verbose")));
                        }
                    }
                    else {
                        mNumSucceededJobs++;
                    }
                }
            }
        }
    }

    private void discardBatch() throws IOException {
        if (mPigServer.isBatchOn()) {
            mPigServer.discardBatch();
        }
    }

    public int[] parseStopOnError() throws IOException, ParseException
    {
	return parseStopOnError(false);
    }
    
    /** 
     * Parses Pig commands in either interactive mode or batch mode. 
     * In interactive mode, executes the plan right away whenever a 
     * STORE command is encountered.
     *
     * @throws IOException, ParseException
     */
    public int[] parseStopOnError(boolean sameBatch) throws IOException, ParseException
    {
        if (mPigServer == null) {
            throw new IllegalStateException();
        }

        if (!mInteractive && !sameBatch) {
            setBatchOn();
        }

        try {
            prompt();
            mDone = false;
            while(!mDone) {
                parse();
            }
            
	    if (!sameBatch) {
		executeBatch();
	    }
        } 
        finally {
	    if (!sameBatch) {
		discardBatch();
	    }
        }
        int [] res = { mNumSucceededJobs, mNumFailedJobs };
        return res;
    }

    public void setLoadOnly(boolean loadOnly) 
    {
        mLoadOnly = loadOnly;
    }

    public void setParams(PigServer pigServer)
    {
        mPigServer = pigServer;
        
        mDfs = mPigServer.getPigContext().getDfs();
        mLfs = mPigServer.getPigContext().getLfs();
        mConf = mPigServer.getPigContext().getProperties();
        
        // TODO: this violates the abstraction layer decoupling between
        // front end and back end and needs to be changed.
        // Right now I am not clear on how the Job Id comes from to tell
        // the back end to kill a given job (mJobClient is used only in 
        // processKill)
        //
        ExecutionEngine execEngine = mPigServer.getPigContext().getExecutionEngine();
        if (execEngine instanceof HExecutionEngine) {
            mJobClient = ((HExecutionEngine)execEngine).getJobClient();
        }
        else {
            mJobClient = null;
        }
    }

    public void prompt()
    {
        if (mInteractive) {
            mConsoleReader.setDefaultPrompt("grunt> ");
        }
    }
    
    protected void quit()
    {
        mDone = true;
    }

    public boolean isDone() {
        return mDone;
    }
    
    protected void processDescribe(String alias) throws IOException {
        if(alias==null) {
            alias = mPigServer.getPigContext().getLastAlias();
        }
        mPigServer.dumpSchema(alias);
    }

    protected void processExplain(String alias, String script, boolean isVerbose, 
                                  String format, String target, 
                                  List<String> params, List<String> files) 
        throws IOException, ParseException {
        
        if (null != mExplain) {
            return;
        }

        try {
            mExplain = new ExplainState(alias, target, script, isVerbose, format);
            
            if (script != null) {
                if (!"true".equalsIgnoreCase(mPigServer.
                                             getPigContext()
                                             .getProperties().
                                             getProperty("opt.multiquery","true"))) {
                    throw new ParseException("Cannot explain script if multiquery is disabled.");
                }
                setBatchOn();
                try {
                    loadScript(script, true, true, params, files);
                } catch(IOException e) {
                    discardBatch();
                    throw e;
            } catch (ParseException e) {
                    discardBatch();
                    throw e;
                }
            }

            mExplain.mLast = true;
            explainCurrentBatch();

        } finally {
            if (script != null) {
                discardBatch();
            }
            mExplain = null;
        }
    }

    protected void explainCurrentBatch() throws IOException {
        PrintStream lp = System.out;
        PrintStream pp = System.out;
        PrintStream ep = System.out;
        
        if (!(mExplain.mLast && mExplain.mCount == 0)) {
            if (mPigServer.isBatchEmpty()) {
                return;
            }
        }

        mExplain.mCount++;
        boolean markAsExecuted = (mExplain.mScript != null);

        if (mExplain.mTarget != null) {
            File file = new File(mExplain.mTarget);
            
            if (file.isDirectory()) {
                String sCount = (mExplain.mLast && mExplain.mCount == 1)?"":"_"+mExplain.mCount;
                lp = new PrintStream(new File(file, "logical_plan-"+mExplain.mTime+sCount+"."+mExplain.mFormat));
                pp = new PrintStream(new File(file, "physical_plan-"+mExplain.mTime+sCount+"."+mExplain.mFormat));
                ep = new PrintStream(new File(file, "exec_plan-"+mExplain.mTime+sCount+"."+mExplain.mFormat));
                mPigServer.explain(mExplain.mAlias, mExplain.mFormat, 
                                   mExplain.mVerbose, markAsExecuted, lp, pp, ep);
                lp.close();
                pp.close();
                ep.close();
            }
            else {
                boolean append = !(mExplain.mCount==1);
                lp = pp = ep = new PrintStream(new FileOutputStream(mExplain.mTarget, append));
                mPigServer.explain(mExplain.mAlias, mExplain.mFormat, 
                                   mExplain.mVerbose, markAsExecuted, lp, pp, ep);
                lp.close();
            }
        }
        else {
            mPigServer.explain(mExplain.mAlias, mExplain.mFormat, 
                               mExplain.mVerbose, markAsExecuted, lp, pp, ep);
        }
    }

    protected void printAliases() throws IOException {
        mPigServer.printAliases();
    }
    
    protected void processRegister(String jar) throws IOException {
        mPigServer.registerJar(jar);
    }

    private String runPreprocessor(String script, List<String> params, 
                                   List<String> files) 
        throws IOException, ParseException {

        ParameterSubstitutionPreprocessor psp = new ParameterSubstitutionPreprocessor(50);
        StringWriter writer = new StringWriter();

        try{
            psp.genSubstitutedFile(new BufferedReader(new FileReader(script)), 
                                   writer,  
                                   params.size() > 0 ? params.toArray(new String[0]) : null, 
                                   files.size() > 0 ? files.toArray(new String[0]) : null);
        } catch (org.apache.pig.tools.parameters.ParseException pex) {
            throw new ParseException(pex.getMessage());
        }

        return writer.toString();
    }

    protected void processScript(String script, boolean batch, 
                                 List<String> params, List<String> files) 
        throws IOException, ParseException {
        
        if (script == null) {
            executeBatch();
            return;
        }
        
        if (batch) {
            setBatchOn();
            mPigServer.setJobName(script);
            try {
                loadScript(script, true, mLoadOnly, params, files);
                executeBatch();
            } finally {
                discardBatch();
            }
        } else {
            loadScript(script, false, mLoadOnly, params, files);
        }
    }

    private void loadScript(String script, boolean batch, boolean loadOnly,
                            List<String> params, List<String> files) 
        throws IOException, ParseException {
        
        Reader inputReader;
        ConsoleReader reader;
        boolean interactive;
         
        try {
            String cmds = runPreprocessor(script, params, files);

            if (mInteractive && !batch) { // Write prompt and echo commands
                // Console reader treats tabs in a special way
                cmds = cmds.replaceAll("\t","    ");

                reader = new ConsoleReader(new ByteArrayInputStream(cmds.getBytes()),
                                           new OutputStreamWriter(System.out));
                reader.setHistory(mConsoleReader.getHistory());
                InputStream in = new ConsoleReaderInputStream(reader);
                inputReader = new BufferedReader(new InputStreamReader(in));
                interactive = true;
            } else { // Quietly parse the statements
                inputReader = new StringReader(cmds);
                reader = null;
                interactive = false;
            }
        } catch (FileNotFoundException fnfe) {
            throw new ParseException("File not found: " + script);
        } catch (SecurityException se) {
            throw new ParseException("Cannot access file: " + script);
        }

        GruntParser parser = new GruntParser(inputReader);
        parser.setParams(mPigServer);
        parser.setConsoleReader(reader);
        parser.setInteractive(interactive);
        parser.setLoadOnly(loadOnly);
        parser.mExplain = mExplain;
        
        parser.prompt();
        while(!parser.isDone()) {
            parser.parse();
        }

        if (interactive) {
            System.out.println("");
        }
    }

    protected void processSet(String key, String value) throws IOException, ParseException {
        if (key.equals("debug"))
        {
            if (value.equals("on") || value.equals("'on'"))
                mPigServer.debugOn();
            else if (value.equals("off") || value.equals("'off'"))
                mPigServer.debugOff();
            else
                throw new ParseException("Invalid value " + value + " provided for " + key);
        }
        else if (key.equals("job.name"))
        {
            mPigServer.setJobName(value);
        }
        else if (key.equals("stream.skippath")) {
            // Validate
            File file = new File(value);
            if (!file.exists() || file.isDirectory()) {
                throw new IOException("Invalid value for stream.skippath:" + 
                                      value); 
            }
            mPigServer.addPathToSkip(value);
        }
        else
        {
            // other key-value pairs can go there
            // for now just throw exception since we don't support
            // anything else
            throw new ParseException("Unrecognized set key: " + key);
        }
    }
    
    protected void processCat(String path) throws IOException
    {
        executeBatch();

        try {
            byte buffer[] = new byte[65536];
            ElementDescriptor dfsPath = mDfs.asElement(path);
            int rc;
            
            if (!dfsPath.exists())
                throw new IOException("Directory " + path + " does not exist.");
    
            if (mDfs.isContainer(path)) {
                ContainerDescriptor dfsDir = (ContainerDescriptor) dfsPath;
                Iterator<ElementDescriptor> paths = dfsDir.iterator();
                
                while (paths.hasNext()) {
                    ElementDescriptor curElem = paths.next();
                    
                    if (mDfs.isContainer(curElem.toString())) {
                        continue;
                    }
                    
                    InputStream is = curElem.open();
                    while ((rc = is.read(buffer)) > 0) {
                        System.out.write(buffer, 0, rc);
                    }
                    is.close();                
                }
            }
            else {
                InputStream is = dfsPath.open();
                while ((rc = is.read(buffer)) > 0) {
                    System.out.write(buffer, 0, rc);
                }
                is.close();            
            }
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to Cat: " + path, e);
        }
    }

    protected void processCD(String path) throws IOException
    {    
        ContainerDescriptor container;

        try {
            if (path == null) {
                container = mDfs.asContainer("/user/" + System.getProperty("user.name"));
                mDfs.setActiveContainer(container);
            }
            else
            {
                container = mDfs.asContainer(path);
    
                if (!container.exists()) {
                    throw new IOException("Directory " + path + " does not exist.");
                }
                
                if (!mDfs.isContainer(path)) {
                    throw new IOException(path + " is not a directory.");
                }
                
                mDfs.setActiveContainer(container);
            }
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to change working directory to " + 
                                  ((path == null) ? ("/user/" + System.getProperty("user.name")) 
                                                     : (path)), e);
        }
    }

    protected void processDump(String alias) throws IOException
    {
        Iterator<Tuple> result = mPigServer.openIterator(alias);
        while (result.hasNext())
        {
            Tuple t = result.next();
            System.out.println(t);
        }
    }
    
    protected void processIllustrate(String alias) throws IOException
    {
	mPigServer.getExamples(alias);
    }

    protected void processKill(String jobid) throws IOException
    {
        if (mJobClient != null) {
            JobID id = JobID.forName(jobid);
            RunningJob job = mJobClient.getJob(id);
            if (job == null)
                System.out.println("Job with id " + jobid + " is not active");
            else
            {    
                job.killJob();
                log.error("kill submitted.");
            }
        }
    }
        
    protected void processLS(String path) throws IOException
    {
        try {
            ElementDescriptor pathDescriptor;
            
            if (path == null) {
                pathDescriptor = mDfs.getActiveContainer();
            }
            else {
                pathDescriptor = mDfs.asElement(path);
            }

            if (!pathDescriptor.exists()) {
                throw new IOException("File or directory " + path + " does not exist.");                
            }
            
            if (mDfs.isContainer(pathDescriptor.toString())) {
                ContainerDescriptor container = (ContainerDescriptor) pathDescriptor;
                Iterator<ElementDescriptor> elems = container.iterator();
                
                while (elems.hasNext()) {
                    ElementDescriptor curElem = elems.next();
                    
                    if (mDfs.isContainer(curElem.toString())) {
                           System.out.println(curElem.toString() + "\t<dir>");
                    } else {
                        printLengthAndReplication(curElem);
                    }
                }
            } else {
                printLengthAndReplication(pathDescriptor);
            }
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to LS on " + path, e);
        }
    }

    private void printLengthAndReplication(ElementDescriptor elem)
            throws IOException {
        Map<String, Object> stats = elem.getStatistics();

        long replication = (Short) stats
                .get(ElementDescriptor.BLOCK_REPLICATION_KEY);
        long len = (Long) stats.get(ElementDescriptor.LENGTH_KEY);

        System.out.println(elem.toString() + "<r " + replication + ">\t" + len);
    }
    
    protected void processPWD() throws IOException 
    {
        System.out.println(mDfs.getActiveContainer().toString());
    }

    protected void printHelp() 
    {
        System.out.println("Commands:");
        System.out.println("<pig latin statement>;");
        System.out.println("store <alias> into <filename> [using <functionSpec>]");
        System.out.println("dump <alias>");
        System.out.println("describe <alias>");
        System.out.println("kill <job_id>");
        System.out.println("ls <path>\r\ndu <path>\r\nmv <src> <dst>\r\ncp <src> <dst>\r\nrm <src>");
        System.out.println("copyFromLocal <localsrc> <dst>\r\ncd <dir>\r\npwd");
        System.out.println("cat <src>\r\ncopyToLocal <src> <localdst>\r\nmkdir <path>");
        System.out.println("cd <path>");
        System.out.println("define <functionAlias> <functionSpec>");
        System.out.println("register <udfJar>");
        System.out.println("set key value");
        System.out.println("quit");
    }

    protected void processMove(String src, String dst) throws IOException
    {
        executeBatch();

        try {
            ElementDescriptor srcPath = mDfs.asElement(src);
            ElementDescriptor dstPath = mDfs.asElement(dst);
            
            if (!srcPath.exists()) {
                throw new IOException("File or directory " + src + " does not exist.");                
            }
            
            srcPath.rename(dstPath);
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to move " + src + " to " + dst, e);
        }
    }
    
    protected void processCopy(String src, String dst) throws IOException
    {
        executeBatch();

        try {
            ElementDescriptor srcPath = mDfs.asElement(src);
            ElementDescriptor dstPath = mDfs.asElement(dst);
            
            srcPath.copy(dstPath, mConf, false);
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to copy " + src + " to " + dst, e);
        }
    }
    
    protected void processCopyToLocal(String src, String dst) throws IOException
    {
        executeBatch();

        try {
            ElementDescriptor srcPath = mDfs.asElement(src);
            ElementDescriptor dstPath = mLfs.asElement(dst);
            
            srcPath.copy(dstPath, false);
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to copy " + src + "to (locally) " + dst, e);
        }
    }

    protected void processCopyFromLocal(String src, String dst) throws IOException
    {
        executeBatch();

        try {
            ElementDescriptor srcPath = mLfs.asElement(src);
            ElementDescriptor dstPath = mDfs.asElement(dst);
            
            srcPath.copy(dstPath, false);
        }
        catch (DataStorageException e) {
            throw WrappedIOException.wrap("Failed to copy (loally) " + src + "to " + dst, e);
        }
    }
    
    protected void processMkdir(String dir) throws IOException
    {
        ContainerDescriptor dirDescriptor = mDfs.asContainer(dir);
        dirDescriptor.create();
    }
    
    protected void processPig(String cmd) throws IOException
    {
        int start = 1;
        if (!mInteractive) {
            start = getLineNumber();
        }
        
        if (cmd.charAt(cmd.length() - 1) != ';') {
            mPigServer.registerQuery(cmd + ";", start);
        }
        else { 
            mPigServer.registerQuery(cmd, start);
        }
    }

    protected void processRemove(String path, String options ) throws IOException
    {
        ElementDescriptor dfsPath = mDfs.asElement(path);

        executeBatch();
        
        if (!dfsPath.exists()) {
            if (options == null || !options.equalsIgnoreCase("force")) {
                throw new IOException("File or directory " + path + " does not exist."); 
            }
        }
        else {
            
            dfsPath.delete();
        }
    }

    private class ExplainState {
        public long mTime;
        public int mCount;
        public String mAlias;
        public String mTarget;
        public String mScript;
        public boolean mVerbose;
        public String mFormat;
        public boolean mLast;

        public ExplainState(String alias, String target, String script,
                            boolean verbose, String format) {
            mTime = new Date().getTime();
            mCount = 0;
            mAlias = alias;
            mTarget = target;
            mScript = script;
            mVerbose = verbose;
            mFormat = format;
            mLast = false;
        }
    }        

    private PigServer mPigServer;
    private DataStorage mDfs;
    private DataStorage mLfs;
    private Properties mConf;
    private JobClient mJobClient;
    private boolean mDone;
    private boolean mLoadOnly;
    private ExplainState mExplain;
    private int mNumFailedJobs;
    private int mNumSucceededJobs;
}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOIsNull extends UnaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOIsNull.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOIsNull(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }
    
    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getOperand().getFieldSchema().canonicalName, getOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "IsNull " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans;

import java.io.PrintStream;
import java.util.List;
import java.util.LinkedList;
import java.util.Collection;
import org.apache.pig.impl.util.MultiMap;
import java.util.HashSet;
import java.util.Set;

import org.apache.pig.impl.plan.DotPlanDumper;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;

/**
 * This class can print a physical plan in the DOT format. It uses
 * clusters to illustrate nesting. If "verbose" is off, it will skip
 * any nesting.
 */
public class DotPOPrinter extends DotPlanDumper<PhysicalOperator, PhysicalPlan, 
                                  PhysicalOperator, PhysicalPlan> {

    public DotPOPrinter(PhysicalPlan plan, PrintStream ps) {
        this(plan, ps, false, new HashSet<Operator>(), new HashSet<Operator>(),
             new HashSet<Operator>());
    }

    public DotPOPrinter(PhysicalPlan plan, PrintStream ps, boolean isSubGraph,
                        Set<Operator> subgraphs, 
                        Set<Operator> multiInputSubgraphs,
                        Set<Operator> multiOutputSubgraphs) {
        super(plan, ps, isSubGraph, subgraphs, multiInputSubgraphs,
              multiOutputSubgraphs);
    }

    @Override
    protected DotPlanDumper makeDumper(PhysicalPlan plan, PrintStream ps) {
        DotPOPrinter dumper = new DotPOPrinter(plan, ps, true, mSubgraphs, 
                                               mMultiInputSubgraphs,
                                               mMultiOutputSubgraphs);
        dumper.setVerbose(this.isVerbose());
        return dumper;
    }

    @Override
    protected String getName(PhysicalOperator op) {
        return (op.name().split(" - "))[0];
    }


    @Override
    protected String[] getAttributes(PhysicalOperator op) {
        if (op instanceof POStore || op instanceof POLoad) {
            String[] attributes = new String[3];
            String name = getName(op);
            int idx = name.lastIndexOf(":");
            if (idx != -1) {
                String part1 = name.substring(0,idx);
                String part2 = name.substring(idx+1,name.length());
                name = part1+",\\n"+part2;
            }
            attributes[0] = "label=\""+name+"\"";
            attributes[1] = "style=\"filled\"";
            attributes[2] = "fillcolor=\"gray\"";
            return attributes;
        }
        else {
            return super.getAttributes(op);
        }
    }

    @Override
    protected Collection<PhysicalPlan> getMultiOutputNestedPlans(PhysicalOperator op) {
        Collection<PhysicalPlan> plans = new LinkedList<PhysicalPlan>();
        
        if (op instanceof POSplit) {
            plans.addAll(((POSplit)op).getPlans());
        }
        else if(op instanceof PODemux) {
            Set<PhysicalPlan> pl = new HashSet<PhysicalPlan>();
            pl.addAll(((PODemux)op).getPlans());
            plans.addAll(pl);
        }
        
        return plans;
    }

    @Override
    protected Collection<PhysicalPlan> getNestedPlans(PhysicalOperator op) {
        Collection<PhysicalPlan> plans = new LinkedList<PhysicalPlan>();

        if(op instanceof POFilter){
            plans.add(((POFilter)op).getPlan());
        }
        else if(op instanceof POForEach){
            plans.addAll(((POForEach)op).getInputPlans());
        }
        else if(op instanceof POSort){
            plans.addAll(((POSort)op).getSortPlans()); 
        }
        else if(op instanceof POLocalRearrange){
            plans.addAll(((POLocalRearrange)op).getPlans());
        }
        else if(op instanceof POFRJoin) {
            POFRJoin frj = (POFRJoin)op;
            List<List<PhysicalPlan>> joinPlans = frj.getJoinPlans();
            if(joinPlans!=null) {
                for (List<PhysicalPlan> list : joinPlans) {
                    plans.addAll(list);
                }
            }
        }

        return plans;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.builtin;

import java.io.IOException;
import java.util.Map;
import java.io.ByteArrayInputStream;

import org.apache.commons.logging.LogFactory;
import org.apache.commons.logging.Log;

import org.apache.pig.PigException;
import org.apache.pig.PigWarning;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PigLogger;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataType;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.parser.ParseException;
import org.apache.pig.data.parser.TextDataParser;
import org.apache.pig.impl.util.LogUtils;

/**
 * This abstract class provides standard conversions between utf8 encoded data
 * and pig data types.  It is intended to be extended by load and store
 * functions (such as PigStorage). 
 */
abstract public class Utf8StorageConverter {

    protected BagFactory mBagFactory = BagFactory.getInstance();
    protected TupleFactory mTupleFactory = TupleFactory.getInstance();
    protected final Log mLog = LogFactory.getLog(getClass());

    private Integer mMaxInt = new Integer(Integer.MAX_VALUE);
    private Long mMaxLong = new Long(Long.MAX_VALUE);
    private TextDataParser dataParser = null;
    
    private PigLogger pigLogger = PhysicalOperator.getPigLogger();
        
    public Utf8StorageConverter() {
    }

    private Object parseFromBytes(byte[] b) throws ParseException {
        ByteArrayInputStream in = new ByteArrayInputStream(b);
        if(dataParser == null) {
            dataParser = new TextDataParser(in);
        } else {
            dataParser.ReInit(in);
        }
        return dataParser.Parse();
    }

    public DataBag bytesToBag(byte[] b) throws IOException {
        if(b == null)
            return null;
        DataBag db;
        try {
            db = (DataBag)parseFromBytes(b);
        } catch (ParseException pe) {
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to type bag, caught ParseException <" +
                    pe.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;       
        }catch (Exception e){
            // can happen if parseFromBytes identifies it as being of different type
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to type bag, caught Exception <" +
                    e.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;       
        }
        return (DataBag)db;
    }

    public String bytesToCharArray(byte[] b) throws IOException {
        if(b == null)
            return null;
        return new String(b, "UTF-8");
    }

    public Double bytesToDouble(byte[] b) {
        if(b == null)
            return null;
        try {
            return Double.valueOf(new String(b));
        } catch (NumberFormatException nfe) {
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to double, caught NumberFormatException <" +
                    nfe.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;
        }
    }

    public Float bytesToFloat(byte[] b) throws IOException {
        if(b == null)
            return null;
        String s;
        if(b.length > 0 && 
           (b[b.length - 1] == 'F' || b[b.length - 1] == 'f') ){
            s = new String(b, 0, b.length - 1);
        } 
        else {
            s = new String(b);
        }
        
        try {
            return Float.valueOf(s);
        } catch (NumberFormatException nfe) {
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to float, caught NumberFormatException <" +
                    nfe.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;
        }
    }

    public Integer bytesToInteger(byte[] b) throws IOException {
        if(b == null)
            return null;
        String s = new String(b);
        try {
            return Integer.valueOf(s);
        } catch (NumberFormatException nfe) {
            // It's possible that this field can be interpreted as a double.
            // Unfortunately Java doesn't handle this in Integer.valueOf.  So
            // we need to try to convert it to a double and if that works then
            // go to an int.
            try {
                Double d = Double.valueOf(s);
                // Need to check for an overflow error
                if (d.doubleValue() > mMaxInt.doubleValue() + 1.0) {
                    LogUtils.warn(this, "Value " + d + " too large for integer", 
                                PigWarning.TOO_LARGE_FOR_INT, mLog);
                    return null;
                }
                return new Integer(d.intValue());
            } catch (NumberFormatException nfe2) {
                LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                        "converted to int, caught NumberFormatException <" +
                        nfe.getMessage() + "> field discarded", 
                        PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
                return null;
            }
        }
    }

    public Long bytesToLong(byte[] b) throws IOException {
        if(b == null)
            return null;

        String s;
        if(b.length > 0  &&  
           (b[b.length - 1] == 'L' || b[b.length - 1] == 'l') ){
            s = new String(b, 0, b.length - 1);
        } 
        else {
            s = new String(b);
        }

        try {
            return Long.valueOf(s);
        } catch (NumberFormatException nfe) {
            // It's possible that this field can be interpreted as a double.
            // Unfortunately Java doesn't handle this in Long.valueOf.  So
            // we need to try to convert it to a double and if that works then
            // go to an long.
            try {
                Double d = Double.valueOf(s);
                // Need to check for an overflow error
                if (d.doubleValue() > mMaxLong.doubleValue() + 1.0) {
                	LogUtils.warn(this, "Value " + d + " too large for integer", 
                	            PigWarning.TOO_LARGE_FOR_INT, mLog);
                    return null;
                }
                return new Long(d.longValue());
            } catch (NumberFormatException nfe2) {
                LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                            "converted to long, caught NumberFormatException <" +
                            nfe.getMessage() + "> field discarded", 
                            PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
                return null;
            }
        }
    }

    public Map<Object, Object> bytesToMap(byte[] b) throws IOException {
        if(b == null)
            return null;
        Map<Object, Object> map;
        try {
            map = (Map<Object, Object>)parseFromBytes(b);
        }
        catch (ParseException pe) {
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to type map, caught ParseException <" +
                    pe.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;       
        }catch (Exception e){
            // can happen if parseFromBytes identifies it as being of different type
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to type map, caught Exception <" +
                    e.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;       
        }
        return map;
    }

    public Tuple bytesToTuple(byte[] b) throws IOException {
        if(b == null)
            return null;
        Tuple t;
        try {
            t = (Tuple)parseFromBytes(b);
        } 
        catch (ParseException pe) {
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to type tuple, caught ParseException <" +
                    pe.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;       
        }catch (Exception e){
            // can happen if parseFromBytes identifies it as being of different type
            LogUtils.warn(this, "Unable to interpret value " + b + " in field being " +
                    "converted to type tuple, caught Exception <" +
                    e.getMessage() + "> field discarded", 
                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
            return null;       
        }
        return t;
    }


    public byte[] toBytes(DataBag bag) throws IOException {
        return bag.toString().getBytes();
    }

    public byte[] toBytes(String s) throws IOException {
        return s.getBytes();
    }

    public byte[] toBytes(Double d) throws IOException {
        return d.toString().getBytes();
    }

    public byte[] toBytes(Float f) throws IOException {
        return f.toString().getBytes();
    }

    public byte[] toBytes(Integer i) throws IOException {
        return i.toString().getBytes();
    }

    public byte[] toBytes(Long l) throws IOException {
        return l.toString().getBytes();
    }

    public byte[] toBytes(Map<Object, Object> m) throws IOException {
        return DataType.mapToString(m).getBytes();
    }

    public byte[] toBytes(Tuple t) throws IOException {
        return t.toString().getBytes();
    }
    


}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.HashMap;
import java.util.Set;
import java.util.Iterator;

import org.apache.pig.PigException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.SchemaMergeException;
import org.apache.pig.impl.logicalLayer.optimizer.SchemaRemover;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.ProjectionMap;
import org.apache.pig.impl.plan.RequiredFields;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.MultiMap;
import org.apache.pig.impl.util.Pair;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;


public class LOForEach extends LogicalOperator {

    private static final long serialVersionUID = 2L;

    /**
     * The foreach operator supports nested query plans. At this point its one
     * level of nesting. Foreach can have a list of operators that need to be
     * applied over the input.
     */

    private ArrayList<LogicalPlan> mForEachPlans;
    private ArrayList<Boolean> mFlatten;
    private ArrayList<Schema> mUserDefinedSchema = null;
    private static Log log = LogFactory.getLog(LOForEach.class);

    /**
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     * @param foreachPlans
     *            the list of plans that are applied for each input
     * @param flattenList
     *            boolean list that tells which elements of the foreach
     *            projection should be flattened.
     */

    public LOForEach(LogicalPlan plan, OperatorKey k,
            ArrayList<LogicalPlan> foreachPlans, ArrayList<Boolean> flattenList) {

        super(plan, k);
        mForEachPlans = foreachPlans;
        mFlatten = flattenList;
    }

    public LOForEach(LogicalPlan plan, OperatorKey k,
            ArrayList<LogicalPlan> foreachPlans, ArrayList<Boolean> flattenList,
            ArrayList<Schema> userDefinedSchemaList) {

        super(plan, k);
        mForEachPlans = foreachPlans;
        mFlatten = flattenList;
        mUserDefinedSchema = userDefinedSchemaList;
    }

    public ArrayList<LogicalPlan> getForEachPlans() {
        return mForEachPlans;
    }

    public void setForEachPlans(ArrayList<LogicalPlan> foreachPlans) {
        mForEachPlans = foreachPlans;
    }

    public List<Boolean> getFlatten() {
        return mFlatten;
    }

    public void setFlatten(ArrayList<Boolean> flattenList) {
        mFlatten = flattenList;
    }

    public List<Schema> getUserDefinedSchema() {
        return mUserDefinedSchema;
    }

    public void setUserDefinedSchema(ArrayList<Schema> userDefinedSchema) {
        mUserDefinedSchema = userDefinedSchema;
    }

    @Override
    public String name() {
        return "ForEach " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    public byte getType() {
        return DataType.BAG ;
    }

    private void updateAliasCount(Map<String, Integer> aliases, String alias) {
        if((null == aliases) || (null == alias)) return;
		Integer count = aliases.get(alias);
		if(null == count) {
			aliases.put(alias, 1);
		} else {
			aliases.put(alias, ++count);
		}
    }

    @Override
    public Schema getSchema() throws FrontendException {
        log.debug("Entering getSchema");
        if (!mIsSchemaComputed) {
            List<Schema.FieldSchema> fss = new ArrayList<Schema.FieldSchema>(
                    mForEachPlans.size());

            for (LogicalPlan plan : mForEachPlans) {
                log.debug("Number of leaves in " + plan + " = " + plan.getLeaves().size());
                for(int i = 0; i < plan.getLeaves().size(); ++i) {
                    log.debug("Leaf" + i + "= " + plan.getLeaves().get(i));
                }
                //LogicalOperator op = plan.getRoots().get(0);
                LogicalOperator op = plan.getLeaves().get(0);
                log.debug("op: " + op.getClass().getName() + " " + op);
            }
            log.debug("Printed the leaves of the generate plans");

            Map<Schema.FieldSchema, String> flattenAlias = new HashMap<Schema.FieldSchema, String>();
            Map<String, Boolean> inverseFlattenAlias = new HashMap<String, Boolean>();
            Map<String, Integer> aliases = new HashMap<String, Integer>();

            for (int planCtr = 0; planCtr < mForEachPlans.size(); ++planCtr) {
                LogicalPlan plan = mForEachPlans.get(planCtr);
                LogicalOperator op = plan.getLeaves().get(0);
                log.debug("op: " + op.getClass().getName() + " " + op);
                log.debug("Flatten: " + mFlatten.get(planCtr));
                Schema.FieldSchema planFs;

                if(op instanceof LOProject) {
                    //the check for the type is required for statements like
                    //foreach cogroup {
                    // a1 = order a by *;
                    // generate a1;
                    //}
                    //In the above script, the generate a1, will translate to 
                    //project(a1) -> project(*) and will not be translated to a sequence of projects
                    //As a result the project(*) will remain but the return type is a bag
                    //project(*) with a data type set to tuple indicates a project(*) from an input
                    //that has no schema
                    if( (((LOProject)op).isStar() ) && (((LOProject)op).getType() == DataType.TUPLE) ) {
                        mSchema = null;
                        mIsSchemaComputed = true;
                        return mSchema;
                    }
                }
                
                try {
	                planFs = ((ExpressionOperator)op).getFieldSchema();
                    log.debug("planFs: " + planFs);
                    Schema userDefinedSchema = null;
                    if(null != mUserDefinedSchema) {
                        userDefinedSchema = mUserDefinedSchema.get(planCtr);
                    }
					if(null != planFs) {
						String outerCanonicalAlias = op.getAlias();
						if(null == outerCanonicalAlias) {
							outerCanonicalAlias = planFs.alias;
						}
						log.debug("Outer canonical alias: " + outerCanonicalAlias);
						if(mFlatten.get(planCtr)) {
							//need to extract the children and create the aliases
							//assumption here is that flatten is only for one column
							//i.e., flatten(A), flatten(A.x) and NOT
							//flatten(B.(x,y,z))
							Schema s = planFs.schema;
							if(null != s && s.isTwoLevelAccessRequired()) {
							    // this is the case where the schema is that of
					            // a bag which has just one tuple fieldschema which
					            // in turn has a list of fieldschemas. The schema
							    // after flattening would consist of the fieldSchemas
							    // present in the tuple
					            
					            // check that indeed we only have one field schema
					            // which is that of a tuple
					            if(s.getFields().size() != 1) {
					                int errCode = 1008;
					                String msg = "Expected a bag schema with a single " +
                                    "element of type "+ DataType.findTypeName(DataType.TUPLE) +
                                    " but got a bag schema with multiple elements.";
					                throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
					            }
					            Schema.FieldSchema tupleFS = s.getField(0);
					            if(tupleFS.type != DataType.TUPLE) {
					                int errCode = 1009;
					                String msg = "Expected a bag schema with a single " +
                                    "element of type "+ DataType.findTypeName(DataType.TUPLE) +
                                    " but got an element of type " +
                                    DataType.findTypeName(tupleFS.type);
					                throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
					            }
					            s = tupleFS.schema;
							    
							}
							if(null != s) {
								for(int i = 0; i < s.size(); ++i) {
                                    Schema.FieldSchema fs;
                                    fs = new Schema.FieldSchema(s.getField(i));
                                    fs.setParent(s.getField(i).canonicalName, op);
									log.debug("fs: " + fs);
                                    if(null != userDefinedSchema) {
                                        Schema.FieldSchema userDefinedFieldSchema;
                                        try {
                                            if(i < userDefinedSchema.size()) {
                                                userDefinedFieldSchema = userDefinedSchema.getField(i);
                                                fs = fs.mergePrefixFieldSchema(userDefinedFieldSchema);
                                            }
                                        } catch (SchemaMergeException sme) {
                                            int errCode = 1016;
                                            String msg = "Problems in merging user defined schema";
                                            throw new FrontendException(msg, errCode, PigException.INPUT, false, null, sme);
                                        }
                                        outerCanonicalAlias = null;
                                    }
									String innerCanonicalAlias = fs.alias;
                                    Schema.FieldSchema newFs;
									if((null != outerCanonicalAlias) && (null != innerCanonicalAlias)) {
										String disambiguatorAlias = outerCanonicalAlias + "::" + innerCanonicalAlias;
										newFs = new Schema.FieldSchema(disambiguatorAlias, fs.schema, fs.type);
                                        newFs.setParent(s.getField(i).canonicalName, op);
                                        fss.add(newFs);
                                        updateAliasCount(aliases, disambiguatorAlias);
										//it's fine if there are duplicates
										//we just need to record if its due to
										//flattening
									} else {
										newFs = new Schema.FieldSchema(fs);
                                        newFs.setParent(s.getField(i).canonicalName, op);
										fss.add(newFs);
									}
                                    updateAliasCount(aliases, innerCanonicalAlias);
									flattenAlias.put(newFs, innerCanonicalAlias);
									inverseFlattenAlias.put(innerCanonicalAlias, true);
								}
							} else {
                                Schema.FieldSchema newFs;
                                if(null != userDefinedSchema) {
                                    if(!DataType.isSchemaType(planFs.type)) {
                                        if(userDefinedSchema.size() > 1) {
                                            int errCode = 1017;
                                            String msg = "Schema mismatch. A basic type on flattening cannot have more than one column. User defined schema: " + userDefinedSchema;
                                            throw new FrontendException(msg, errCode, PigException.INPUT, false, null);
                                        }
								        newFs = new Schema.FieldSchema(null, planFs.type);
                                        try {
                                            newFs = newFs.mergePrefixFieldSchema(userDefinedSchema.getField(0));
                                        } catch (SchemaMergeException sme) {
                                            int errCode = 1016;
                                            String msg = "Problems in merging user defined schema";
                                            throw new FrontendException(msg, errCode, PigException.INPUT, false, null, sme);
                                        }
                                        updateAliasCount(aliases, newFs.alias);
                                        fss.add(newFs);
                                        newFs.setParent(null, op);
                                    } else {
                                        for(Schema.FieldSchema ufs: userDefinedSchema.getFields()) {
                                            Schema.FieldSchema.setFieldSchemaDefaultType(ufs, DataType.BYTEARRAY);
                                            newFs = new Schema.FieldSchema(ufs);
                                            fss.add(newFs);
                                            newFs.setParent(null, op);
                                            updateAliasCount(aliases, ufs.alias);
                                        }
                                    }
								} else {
                                    if(!DataType.isSchemaType(planFs.type)) {
								        newFs = new Schema.FieldSchema(planFs.alias, planFs.type);
                                    } else {
								        newFs = new Schema.FieldSchema(null, DataType.BYTEARRAY);
                                    }
                                    fss.add(newFs);
                                    newFs.setParent(null, op);
                                }
							}
						} else {
							//just populate the schema with the field schema of the expression operator
                            //check if the user has defined a schema for the operator; compare the schema
                            //with that of the expression operator field schema and then add it to the list
                            Schema.FieldSchema newFs = new Schema.FieldSchema(planFs);
                            if(null != userDefinedSchema) {
                                try {
                                    newFs = newFs.mergePrefixFieldSchema(userDefinedSchema.getField(0));
                                    updateAliasCount(aliases, newFs.alias);
                                } catch (SchemaMergeException sme) {
                                    int errCode = 1016;
                                    String msg = "Problems in merging user defined schema";
                                    throw new FrontendException(msg, errCode, PigException.INPUT, false, null, sme);
                                }
                            }
                            newFs.setParent(planFs.canonicalName, op);
                            fss.add(newFs);
						}
					} else {
						//did not get a valid list of field schemas
                        String outerCanonicalAlias = null;
                        if(null != userDefinedSchema) {
                            Schema.FieldSchema userDefinedFieldSchema = new Schema.FieldSchema(userDefinedSchema.getField(0));
                            fss.add(userDefinedFieldSchema);
                            userDefinedFieldSchema.setParent(null, op);
                            updateAliasCount(aliases, userDefinedFieldSchema.alias);
                        } else {
                            mSchema = null;
                            mIsSchemaComputed = true;
                            return mSchema;
                        }
					}
                } catch (FrontendException fee) {
                    mSchema = null;
                    mIsSchemaComputed = false;
                    throw fee;
                }
            }
			//check for duplicate column names and throw an error if there are duplicates
			//ensure that flatten gets rid of duplicate column names when the checks are
			//being done
			log.debug(" flattenAlias: " + flattenAlias);
			log.debug(" inverseFlattenAlias: " + inverseFlattenAlias);
			log.debug(" aliases: " + aliases);
			log.debug(" fss.size: " + fss.size());
			boolean duplicates = false;
			Map<String, Integer> duplicateAliases = new HashMap<String, Integer>();
			for(String alias: aliases.keySet()) {
				Integer count = aliases.get(alias);
				if(count > 1) {//not checking for null here as counts are intitalized to 1
					Boolean inFlatten = false;
					log.debug("inFlatten: " + inFlatten + " inverseFlattenAlias: " + inverseFlattenAlias);
					inFlatten = inverseFlattenAlias.get(alias);
					log.debug("inFlatten: " + inFlatten + " inverseFlattenAlias: " + inverseFlattenAlias);
					if((null == inFlatten) || (!inFlatten)) {
						duplicates = true;
						duplicateAliases.put(alias, count);
					}
				}
			}
			if(duplicates) {
				String errMessage = "Found duplicates in schema. ";
				if(duplicateAliases.size() > 0) {
					Set<String> duplicateCols = duplicateAliases.keySet();
					Iterator<String> iter = duplicateCols.iterator();
					String col = iter.next();
					errMessage += col + ": " + duplicateAliases.get(col) + " columns";
					while(iter.hasNext()) {
						col = iter.next();
						errMessage += ", " + col + ": " + duplicateAliases.get(col) + " columns";
					}
				}
				errMessage += ". Please alias the columns with unique names.";
				log.debug(errMessage);
				int errCode = 1007;
				throw new FrontendException(errMessage, errCode, PigException.INPUT, false, null);
			}
            mSchema = new Schema(fss);
			//add the aliases that are unique after flattening
			for(Schema.FieldSchema fs: mSchema.getFields()) {
				String alias = flattenAlias.get(fs);
				Integer count = aliases.get(alias);
				if (null == count) count = 1;
				log.debug("alias: " + alias);
				if((null != alias) && (count == 1)) {
					mSchema.addAlias(alias, fs);
				}
			}
            mIsSchemaComputed = true;
        }
        log.debug("Exiting getSchema");
        return mSchema;
    }

    public void unsetSchema() throws VisitorException{
        for(LogicalPlan plan: mForEachPlans) {
            SchemaRemover sr = new SchemaRemover(plan);
            sr.visit();
        }
        super.unsetSchema();
    }

    /**
     * @see org.apache.pig.impl.plan.Operator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        // Do generic LogicalOperator cloning
        LOForEach forEachClone = (LOForEach)super.clone();
        
        // create deep copies of attributes specific to foreach
        if(mFlatten != null) {
            forEachClone.mFlatten = new ArrayList<Boolean>();
            for (Iterator<Boolean> it = mFlatten.iterator(); it.hasNext();) {
                forEachClone.mFlatten.add(new Boolean(it.next()));
            }
        }
        
        if(mForEachPlans != null) {
            forEachClone.mForEachPlans = new ArrayList<LogicalPlan>();
            for (Iterator<LogicalPlan> it = mForEachPlans.iterator(); it.hasNext();) {
                LogicalPlanCloneHelper lpCloneHelper = new LogicalPlanCloneHelper(it.next());
                forEachClone.mForEachPlans.add(lpCloneHelper.getClonedPlan());
            }
        }
        
        if(mUserDefinedSchema != null) {
            forEachClone.mUserDefinedSchema = new ArrayList<Schema>();
            for (Iterator<Schema> it = mUserDefinedSchema.iterator(); it.hasNext();) {
                Schema s = it.next();
                forEachClone.mUserDefinedSchema.add(s != null ? s.clone() : null);
            }
        }
        return forEachClone;
    }

    @Override
    public ProjectionMap getProjectionMap() {
        Schema outputSchema;
        
        try {
            outputSchema = getSchema();
        } catch (FrontendException fee) {
            return null;
        }
        
        if(outputSchema == null) {
            return null;
        }
        
        List<LogicalOperator> predecessors = (ArrayList<LogicalOperator>)mPlan.getPredecessors(this);
        if(predecessors == null) {
            return null;
        }
        
        LogicalOperator predecessor = predecessors.get(0);
        
        Schema inputSchema;
        
        try {
            inputSchema = predecessor.getSchema();
        } catch (FrontendException fee) {
            return null;
        }
        
        List<LogicalPlan> foreachPlans = getForEachPlans();
        List<Boolean> flattenList = getFlatten();
        
        MultiMap<Integer, Pair<Integer, Integer>> mapFields = new MultiMap<Integer, Pair<Integer, Integer>>();
        List<Integer> addedFields = new ArrayList<Integer>();
        int outputColumn = 0;
        
        for(int i = 0; i < foreachPlans.size(); ++i) {
            LogicalPlan foreachPlan = foreachPlans.get(i);
            List<LogicalOperator> leaves = foreachPlan.getLeaves();
            if(leaves == null || leaves.size() > 1) {
                return null;
            }
            
            int inputColumn = -1;
            boolean mapped = false;
            
            if(leaves.get(0) instanceof LOProject) {
                //find out if this project is a chain of projects
                if(LogicalPlan.chainOfProjects(foreachPlan)) {
                    LOProject rootProject = (LOProject)foreachPlan.getRoots().get(0);
                    inputColumn = rootProject.getCol();
                    if(inputSchema != null) {
                        mapped = true;
                    }
                }
            }
            
            Schema.FieldSchema leafFS;
            try {
                leafFS = ((ExpressionOperator)leaves.get(0)).getFieldSchema();
            } catch (FrontendException fee) {
                return null;
            }
            
            if(leafFS == null) {
                return null;
            }
            
            if(flattenList.get(i)) {
                Schema innerSchema = leafFS.schema;
                
                if(innerSchema != null) {                    
                    if(innerSchema.isTwoLevelAccessRequired()) {
                        // this is the case where the schema is that of
                        // a bag which has just one tuple fieldschema which
                        // in turn has a list of fieldschemas. The schema
                        // after flattening would consist of the fieldSchemas
                        // present in the tuple
                        
                        // check that indeed we only have one field schema
                        // which is that of a tuple
                        if(innerSchema.getFields().size() != 1) {
                            return null;
                        }
                        Schema.FieldSchema tupleFS;
                        try {
                            tupleFS = innerSchema.getField(0);
                        } catch (FrontendException fee) {
                            return null;
                        }
                        
                        if(tupleFS.type != DataType.TUPLE) {
                            return null;
                        }
                        innerSchema = tupleFS.schema;
                    }
                    
                    //innerSchema could be modified and hence the second check
                    if(innerSchema != null) {
                        for(int j = 0; j < innerSchema.size(); ++j) {
                            if(mapped) {
                                //map each flattened column to the original column
                                mapFields.put(outputColumn++, new Pair<Integer, Integer>(0, inputColumn));
                            } else {
                                addedFields.add(outputColumn++);
                            }
                        }
                    } else {
                        //innerSchema is null; check for schema type
                        if(DataType.isSchemaType(leafFS.type)) {
                            //flattening a null schema results in a bytearray
                            if(mapped) {
                                //map each flattened column to the original column
                                mapFields.put(outputColumn++, new Pair<Integer, Integer>(0, inputColumn));
                            } else {
                                addedFields.add(outputColumn++);
                            }
                        } else {
                            mapFields.put(outputColumn++, new Pair<Integer, Integer>(0, inputColumn));
                        }
                    }
                } else {
                    //innerSchema is null; check for schema type
                    if(DataType.isSchemaType(leafFS.type)) {
                        //flattening a null schema results in a bytearray
                        if(mapped) {
                            //map each flattened column to the original column
                            mapFields.put(outputColumn++, new Pair<Integer, Integer>(0, inputColumn));
                        } else {
                            addedFields.add(outputColumn++);
                        }
                    } else {
                        mapFields.put(outputColumn++, new Pair<Integer, Integer>(0, inputColumn));
                    }
                }
            } else {
                //not a flattened column
                if(mapped) {
                    mapFields.put(outputColumn++, new Pair<Integer, Integer>(0, inputColumn));
                } else {
                    addedFields.add(outputColumn++);
                }
            }
        }
        
        List<Pair<Integer, Integer>> removedFields = new ArrayList<Pair<Integer, Integer>>();
       
        if(inputSchema == null) {
            //if input schema is null then there are no mappedFields and removedFields
            mapFields = null;
            removedFields = null;
        } else {
            
            //if the size of the map is zero then set it to null
            if(mapFields.size() == 0) {
                mapFields = null;
            }
            
            if(addedFields.size() == 0) {
                addedFields = null;
            }
            
            //input schema is not null. Need to compute the removedFields
            //compute the set difference between the input schema and mapped fields
            
            Set<Integer> removedSet = new HashSet<Integer>();
            for(int i = 0; i < inputSchema.size(); ++i) {
                removedSet.add(i);
            }
            
            if(mapFields != null) {
                Set<Integer> mappedSet = new HashSet<Integer>();
                for(Integer key: mapFields.keySet()) {
                    List<Pair<Integer, Integer>> values = (ArrayList<Pair<Integer, Integer>>)mapFields.get(key);
                    for(Pair<Integer, Integer> value: values) {
                        mappedSet.add(value.second);
                    }
                }
                removedSet.removeAll(mappedSet);
            }
            
            if(removedSet.size() == 0) {
                removedFields = null;
            } else {
                for(Integer i: removedSet) {
                    removedFields.add(new Pair<Integer, Integer>(0, i));
                }
            }
        }

        return new ProjectionMap(mapFields, removedFields, addedFields);
    }

    @Override
    public List<RequiredFields> getRequiredFields() {
        List<RequiredFields> requiredFields = new ArrayList<RequiredFields>();
        Set<Pair<Integer, Integer>> fields = new HashSet<Pair<Integer, Integer>>();
        Set<LOProject> projectSet = new HashSet<LOProject>();
        boolean starRequired = false;

        for (LogicalPlan plan : getForEachPlans()) {
            TopLevelProjectFinder projectFinder = new TopLevelProjectFinder(
                    plan);
            try {
                projectFinder.visit();
            } catch (VisitorException ve) {
                requiredFields.clear();
                requiredFields.add(null);
                return requiredFields;
            }
            projectSet.addAll(projectFinder.getProjectSet());
            if(projectFinder.getProjectStarSet() != null) {
                starRequired = true;
            }
        }

        if(starRequired) {
            requiredFields.add(new RequiredFields(true));
            return requiredFields;
        } else {
            for (LOProject project : projectSet) {
                for (int inputColumn : project.getProjection()) {
                    fields.add(new Pair<Integer, Integer>(0, inputColumn));
                }
            }
    
            if(fields.size() == 0) {
                requiredFields.add(new RequiredFields(false, true));
            } else {                
                requiredFields.add(new RequiredFields(new ArrayList<Pair<Integer, Integer>>(fields)));
            }
            return (requiredFields.size() == 0? null: requiredFields);
        }
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import org.apache.pig.PigException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;
import org.apache.pig.impl.plan.DepthFirstWalker;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorPlan;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;

public class PlanPrinter<O extends Operator, P extends OperatorPlan<O>> extends
        PlanVisitor<O, P> {

    String TAB1 = "    ";

    String TABMore = "|   ";

    String LSep = "|\n|---";
    
    String USep = "|   |\n|   ";

    int levelCntr = -1;

    OutputStream printer;
    
    PrintStream stream = System.out;

    boolean isVerbose = true;

    public PlanPrinter(P plan) {
        super(plan, new DepthFirstWalker<O, P>(plan));
    }
    
    public PlanPrinter(P plan, PrintStream stream) {
        super(plan, new DepthFirstWalker<O, P>(plan));
        this.stream = stream;
    }

    public void setVerbose(boolean verbose) {
        isVerbose = verbose;
    }

    @Override
    public void visit() throws VisitorException {
        try {
            stream.write(depthFirstPP().getBytes());
        } catch (IOException ioe) {
            int errCode = 2079;
            String msg = "Unexpected error while printing physical plan.";
            throw new VisitorException(msg, errCode, PigException.BUG, ioe);
        }
    }

    public void print(OutputStream printer) throws VisitorException, IOException {
        this.printer = printer;
        printer.write(depthFirstPP().getBytes());
    }

    protected void breadthFirst() throws VisitorException {
        List<O> leaves = mPlan.getLeaves();
        Set<O> seen = new HashSet<O>();
        breadthFirst(leaves, seen);
    }

    private void breadthFirst(Collection<O> predecessors, Set<O> seen)
            throws VisitorException {
        ++levelCntr;
        dispTabs();

        List<O> newPredecessors = new ArrayList<O>();
        for (O pred : predecessors) {
            if (seen.add(pred)) {
                List<O> predLst = mPlan.getPredecessors(pred);
                if (predLst != null)
                    newPredecessors.addAll(predLst);

                pred.visit(this);
            }
        }
        if (newPredecessors.size() > 0) {
            stream.println();
            breadthFirst(newPredecessors, seen);
        }
    }

    protected String depthFirstPP() throws VisitorException {
        StringBuilder sb = new StringBuilder();
        List<O> leaves = mPlan.getLeaves();
        Collections.sort(leaves);
        for (O leaf : leaves) {
            sb.append(depthFirst(leaf));
            sb.append("\n");
        }
        sb.delete(sb.length() - "\n".length(), sb.length());
        sb.delete(sb.length() - "\n".length(), sb.length());
        return sb.toString();
    }
    
    private String planString(PhysicalPlan pp){
        StringBuilder sb = new StringBuilder();
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        if(pp!=null)
            pp.explain(baos, isVerbose);
        else
            return "";
        sb.append(USep);
        sb.append(shiftStringByTabs(baos.toString(), 2));
        return sb.toString();
    }
    
    private String planString(List<PhysicalPlan> lep){
        StringBuilder sb = new StringBuilder();
        if(lep!=null)
            for (PhysicalPlan ep : lep) {
                sb.append(planString(ep));
            }
        return sb.toString();
    }

    private String depthFirst(O node) throws VisitorException {
        StringBuilder sb = new StringBuilder(node.name() + "\n");
        if (isVerbose) {
          if(node instanceof POFilter){
            sb.append(planString(((POFilter)node).getPlan()));
          }
          else if(node instanceof POLocalRearrange){
            sb.append(planString(((POLocalRearrange)node).getPlans()));
          }
          else if(node instanceof POSort){
            sb.append(planString(((POSort)node).getSortPlans())); 
          }
          else if(node instanceof POForEach){
            sb.append(planString(((POForEach)node).getInputPlans()));
          }
          else if (node instanceof POMultiQueryPackage) {
              List<POPackage> pkgs = ((POMultiQueryPackage)node).getPackages();
              for (POPackage pkg : pkgs) {
                  sb.append(LSep + pkg.name() + "\n");
              }
          }
          else if(node instanceof POFRJoin){
            POFRJoin frj = (POFRJoin)node;
            List<List<PhysicalPlan>> joinPlans = frj.getJoinPlans();
            if(joinPlans!=null)
              for (List<PhysicalPlan> list : joinPlans) {
                sb.append(planString(list));
              }
          }
        }
        
        if (node instanceof POSplit) {
            sb.append(planString(((POSplit)node).getPlans()));
        }
        else if (node instanceof PODemux) {
            List<PhysicalPlan> plans = new ArrayList<PhysicalPlan>();
            Set<PhysicalPlan> pl = new HashSet<PhysicalPlan>();
            pl.addAll(((PODemux)node).getPlans());
            plans.addAll(pl);
            sb.append(planString(plans));
        }
        
        List<O> originalPredecessors = mPlan.getPredecessors(node);
        if (originalPredecessors == null)
            return sb.toString();
        
        List<O> predecessors =  new ArrayList<O>(originalPredecessors);
        
        Collections.sort(predecessors);
        int i = 0;
        for (O pred : predecessors) {
            i++;
            String DFStr = depthFirst(pred);
            if (DFStr != null) {
                sb.append(LSep);
                if (i < predecessors.size())
                    sb.append(shiftStringByTabs(DFStr, 2));
                else
                    sb.append(shiftStringByTabs(DFStr, 1));
            }
        }
        return sb.toString();
    }

    private String shiftStringByTabs(String DFStr, int TabType) {
        StringBuilder sb = new StringBuilder();
        String[] spl = DFStr.split("\n");

        String tab = (TabType == 1) ? TAB1 : TABMore;

        sb.append(spl[0] + "\n");
        for (int i = 1; i < spl.length; i++) {
            sb.append(tab);
            sb.append(spl[i]);
            sb.append("\n");
        }
        return sb.toString();
    }

    private void dispTabs() {
        for (int i = 0; i < levelCntr; i++)
            stream.print(TAB1);
    }

    public void visitLoad(POLoad op) {
        stream.print(op.name() + "   ");
    }

    public void visitStore(POStore op) {
        stream.print(op.name() + "   ");
    }

    public void visitFilter(POFilter op) {
        stream.print(op.name() + "   ");
    }

    public void visitLocalRearrange(POLocalRearrange op) {
        stream.print(op.name() + "   ");
    }

    public void visitGlobalRearrange(POGlobalRearrange op) {
        stream.print(op.name() + "   ");
    }

    public void visitPackage(POPackage op) {
        stream.print(op.name() + "   ");
    }

    public void visitStartMap(POUnion op) {
        stream.print(op.name() + "   ");
    }
    
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.data;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;


/**
 * An unordered collection of Tuples (possibly) with multiples.  The tuples
 * are stored in an ArrayList, since there is no concern for order or
 * distinctness. The implicit assumption is that the user of this class
 * is storing only those many tuples as will fit in memory - no spilling
 * will be done on this bag to disk.
 */
public class NonSpillableDataBag implements DataBag {
    // the reason this class does NOT extend DefaultAbstractBag
    // is that we don't want to bloat this class with members it
    // does not need (DefaultAbstractBag has many members related
    // to spilling which are not needed here)
    /**
     * 
     */
    private static final long serialVersionUID = 1L;

    /**
     * 
     */
    private List<Tuple> mContents;    

    public NonSpillableDataBag() {
        mContents = new ArrayList<Tuple>();
    }

    /**
     * This constructor creates a bag out of an existing list
     * of tuples by taking ownership of the list and NOT
     * copying the contents of the list.
     * @param listOfTuples List<Tuple> containing the tuples
     */
    public NonSpillableDataBag(List<Tuple> listOfTuples) {
        mContents = listOfTuples;
    }

    public boolean isSorted() {
        return false;
    }
    
    public boolean isDistinct() {
        return false;
    }
    
    public Iterator<Tuple> iterator() {
        return new NonSpillableDataBagIterator();
    }

    /**
     * An iterator that handles getting the next tuple from the bag.
     */
    private class NonSpillableDataBagIterator implements Iterator<Tuple> {

        private int mCntr = 0;

        public boolean hasNext() { 
            return (mCntr < mContents.size());
        }

        public Tuple next() {
            // This will report progress every 1024 times through next.
            // This should be much faster than using mod.
            if ((mCntr & 0x3ff) == 0) reportProgress();

            return mContents.get(mCntr++);
        }

        /**
         * Not implemented.
         */
        public void remove() { throw new RuntimeException("Cannot remove() from NonSpillableDataBag.iterator()");}
    }    

    /**
     * Report progress to HDFS.
     */
    protected void reportProgress() {
        if (PhysicalOperator.reporter != null) {
            PhysicalOperator.reporter.progress();
        }
    }

    @Override
    public void add(Tuple t) {
        mContents.add(t);
    }

    @Override
    public void addAll(DataBag b) {
        for (Tuple t : b) {
            mContents.add(t);
        }
    }

    @Override
    public void clear() {
        mContents.clear();        
    }

    @Override
    public void markStale(boolean stale) {
        throw new RuntimeException("NonSpillableDataBag cannot be marked stale");
    }

    @Override
    public long size() {
        return mContents.size();
    }

    @Override
    public long getMemorySize() {
        return 0;
    }

    @Override
    public long spill() {
        // TODO Auto-generated method stub
        return 0;
    }

    /**
     * Write a bag's contents to disk.
     * @param out DataOutput to write data to.
     * @throws IOException (passes it on from underlying calls).
     */
    public void write(DataOutput out) throws IOException {
        // We don't care whether this bag was sorted or distinct because
        // using the iterator to write it will guarantee those things come
        // correctly.  And on the other end there'll be no reason to waste
        // time re-sorting or re-applying distinct.
        out.writeLong(size());
        Iterator<Tuple> it = iterator();
        while (it.hasNext()) {
            Tuple item = it.next();
            item.write(out);
        }    
    }
 
    /**
     * Read a bag from disk.
     * @param in DataInput to read data from.
     * @throws IOException (passes it on from underlying calls).
     */
    public void readFields(DataInput in) throws IOException {
        long size = in.readLong();
        
        for (long i = 0; i < size; i++) {
            try {
                Object o = DataReaderWriter.readDatum(in);
                add((Tuple)o);
            } catch (ExecException ee) {
                throw ee;
            }
        }
    }
    
    /* (non-Javadoc)
     * @see java.lang.Object#equals(java.lang.Object)
     */
    @Override
    public boolean equals(Object obj) {
        return compareTo(obj) == 0;
    }

    @Override
    public int compareTo(Object other) {
        if (this == other)
            return 0;
        if (other instanceof DataBag) {
            DataBag bOther = (DataBag) other;
            if (this.size() != bOther.size()) {
                if (this.size() > bOther.size()) return 1;
                else return -1;
            }

            // Ugh, this is bogus.  But I have to know if two bags have the
            // same tuples, regardless of order.  Hopefully most of the
            // time the size check above will prevent this.
            // If either bag isn't already sorted, create a sorted bag out
            // of it so I can guarantee order.
            DataBag thisClone;
            DataBag otherClone;
            thisClone = new SortedDataBag(null);
            Iterator<Tuple> i = iterator();
            while (i.hasNext()) thisClone.add(i.next());
            if (other instanceof SortedDataBag ||
                    other instanceof DistinctDataBag) {
                otherClone = bOther;
            } else {
                otherClone = new SortedDataBag(null);
                i = bOther.iterator();
                while (i.hasNext()) otherClone.add(i.next());
            }
            Iterator<Tuple> thisIt = thisClone.iterator();
            Iterator<Tuple> otherIt = otherClone.iterator();
            while (thisIt.hasNext() && otherIt.hasNext()) {
                Tuple thisT = thisIt.next();
                Tuple otherT = otherIt.next();
                
                int c = thisT.compareTo(otherT);
                if (c != 0) return c;
            }
            
            return 0;   // if we got this far, they must be equal
        } else {
            return DataType.compare(this, other);
        }
    }
    
    /**
     * Write the bag into a string. */
    @Override
    public String toString() {
        StringBuffer sb = new StringBuffer();
        sb.append('{');
        Iterator<Tuple> it = iterator();
        while ( it.hasNext() ) {
            Tuple t = it.next();
            String s = t.toString();
            sb.append(s);
            if (it.hasNext()) sb.append(",");
        }
        sb.append('}');
        return sb.toString();
    }
    
}


/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.test;

import junit.framework.TestCase;
import org.apache.pig.impl.logicalLayer.*;
import org.apache.pig.test.utils.TypeCheckingTestUtil;
import org.apache.pig.test.utils.LogicalPlanTester;
import org.junit.Test;
import org.junit.Before;

public class TestTypeChecking extends TestCase {

    final String FILE_BASE_LOCATION = "test/org/apache/pig/test/data/DotFiles/" ;

    LogicalPlanTester planTester = new LogicalPlanTester() ;

    @Before
    public void setUp() {
        planTester.reset();
    }


    @Test
    public void testSimple1() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.buildPlan("a = load 'a' as (field1: int, field2: float, field3: chararray );") ;
        LogicalPlan plan = planTester.buildPlan("b = distinct a ;") ;
        planTester.typeCheckAgainstDotFile(plan, FILE_BASE_LOCATION + "plan1.dot");
    }


    @Test
    public void testByScript1() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.typeCheckUsingDotFile(FILE_BASE_LOCATION + "testScript1.dot");
    }

    @Test
    public void testByScript2() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.typeCheckUsingDotFile(FILE_BASE_LOCATION + "testScript2.dot");
    }

    // Problem with "group" keyword in QueryParser
    /*
    @Test
    public void testByScript4() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.typeCheckUsingDotFile(FILE_BASE_LOCATION + "testScript4.dot");
    }
    */

    /*
    @Test
    public void testByScript3() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.typeCheckUsingDotFile(FILE_BASE_LOCATION + "testScript3.dot");
    }
    */

    @Test
    public void testByScript5() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.typeCheckUsingDotFile(FILE_BASE_LOCATION + "testScript5.dot");
    }
    
    @Test
    public void testByScript6() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.typeCheckUsingDotFile(FILE_BASE_LOCATION + "testScript6.dot");
    }

    // TODO: Convert all of these to dot files

    /*
    @Test
    public void testValidation1() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.buildPlan("a = load 'a' as (" 
                            + "group: tuple(field1:int, field2:double) ,"
                            + "a: bag{tuple1:tuple(field1: int,field2: long)},"
                            + "b: bag{tuple1:tuple(field1: bytearray,field2: double)} "
                            + " ) ;") ;
        LogicalPlan plan = planTester.buildPlan("b = group a by field1;");
        planTester.typeCheckPlan(plan);


    }
    */

    /*
    @Test
    public void testValidation1() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.buildPlan("a = load 'a' as (field1: chararray, field2: tuple(inner1 : bytearray, inner2 : int));");
        LogicalPlan plan = planTester.buildPlan("b = group a by field1;");
        planTester.typeCheckPlan(plan);
    }
    */

    /*
    @Test
    public void testValidation2() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: long, field2: tuple(inner1 : bytearray, inner2 : float));");
        LogicalPlan plan = buildPlan("b = group a by field2;");
        validatePlan(plan) ;
    }


    @Test
    public void testValidation3() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: int, field2: float, field3: chararray );");
        buildPlan("b = order a by $0 ;");
        LogicalPlan plan = buildPlan("c = distinct b ;");
        validatePlan(plan) ;
    }

    @Test
    public void testValidation4() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: chararray, field2: int);");
        LogicalPlan plan = buildPlan("b = group a by (field1, field2) ;");
        validatePlan(plan) ;
    }

    @Test
    public void testValidation5() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: bytearray, field2: int);");
        LogicalPlan plan = buildPlan("b = group a by field1+field2 ;");
        validatePlan(plan) ;
    }

    @Test
    public void testValidation6() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: bytearray, field2: int, field3: double);");
        LogicalPlan plan = buildPlan("b = group a by field1 % field2  ;");
        validatePlan(plan) ;
    }

    @Test
    public void testValidation7() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: int, field2: tuple(inner1 : bytearray, inner2 : int));");
        buildPlan("b = load 'a' as (field1: long, field2: tuple(inner1 : bytearray, inner2 : int));");
        LogicalPlan plan = buildPlan("c = group a by field1, b by field1;");
        validatePlan(plan) ;
    }

    @Test
    public void testValidation8() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: int, field2: long);");
        buildPlan("b = load 'a' as (field1: bytearray, field2: double);");
        LogicalPlan plan = buildPlan("c = group a by (field1,field2) , b by (field1,field2) ;");
        validatePlan(plan) ;
    }

    @Test
    public void testValidation10() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (name: chararray, details: tuple(age, gpa), mymap: map[]);");
		LogicalPlan plan = buildPlan("e = foreach a generate name, details;");
        validatePlan(plan) ;
    }



    @Test
    public void testValidation12() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: int, field2: long);");
        buildPlan("b = load 'a' as (field1: bytearray, field2: double);");
        buildPlan("c = group a by field1, b by field1  ;");
        LogicalPlan plan = buildPlan("d = foreach c generate a.(field1, field2), b.(field1, field2)  ;");
        validatePlan(plan) ;
    }
    */

    // I suspect there is something wrong in Schema.reconcile()
    /*
    @Test
    public void testValidation13() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: integer, field2: long);");
        buildPlan("b = load 'a' as (field1: bytearray, field2: double);");
        buildPlan("c = group a by field1, b by field1  ;");
        LogicalPlan plan = buildPlan("d = foreach c generate b.field2, flattten(a)  ;");
        validatePlan(plan) ;
    }
    */

    // The parser still has a bug dealing with this
    /*
    @Test
    public void testValidation13_2() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: integer, field2: long);");
        buildPlan("b = load 'a' as (field1: bytearray, field2: double);");
        buildPlan("c = group a by (field1+field2)*field1, b by field1  ;");
        LogicalPlan plan = buildPlan("d = foreach c generate b.field2, flattten(a)  ;");
        validatePlan(plan) ;
    }
    */

    /*
   @Test
    public void testQuery20() throws Throwable {
       TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: long, field2: long, field3: int);");
        String query = "b = foreach a generate ($1+$2), ($1-$2), ($1*$2), ($1/$2), ($1%$2), -($1), ($2*$2) ;";
        validatePlan(buildPlan(query));
    }

    @Test
    public void testQuery21() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        buildPlan("a = load 'a' as (field1: long, field2: long, field3: int);");
        String query = "b = foreach a generate (field1+field2)*(field1-field2) ;";
        validatePlan(buildPlan(query));
    }

    */

    public void testSUM1() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.buildPlan("a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name:chararray, age:int, gpa:double);") ;
        LogicalPlan plan1 = planTester.buildPlan("b = foreach a generate (long)age as age:long, (int)gpa as gpa:int;") ;
        LogicalPlan plan2 = planTester.buildPlan("c = foreach b generate SUM(age), SUM(gpa);") ;
        planTester.typeCheckPlan(plan2);
    }

    public void testSUM2() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.buildPlan("a = group (load '\" + tmpFile + \"') by ($0,$1);") ;
        LogicalPlan plan1 = planTester.buildPlan("b = foreach a generate flatten(group), SUM($1.$2);") ;
        planTester.typeCheckPlan(plan1);
    }


    public void testGenerate1() throws Throwable {
        TypeCheckingTestUtil.printCurrentMethodName() ;
        planTester.buildPlan("a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name:chararray, age:int, gpa:double);") ;
        LogicalPlan plan1 = planTester.buildPlan("b = foreach a generate 1 + 0.2f + 253645L, gpa+1; ") ;
        planTester.typeCheckPlan(plan1);
    }



}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOAdd extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOAdd.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOAdd(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.mergeType(getLhsOperand().getType(), getRhsOperand().getType()));
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }


    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Add " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;

import java.io.IOException;
import java.io.PrintStream;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.LinkedList;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.TaskReport;
import org.apache.hadoop.mapred.jobcontrol.Job;
import org.apache.hadoop.mapred.jobcontrol.JobControl;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.executionengine.ExecutionEngine;
import org.apache.pig.backend.hadoop.datastorage.HConfiguration;
import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
import org.apache.pig.impl.PigContext;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.impl.plan.PlanException;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.util.LogUtils;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.tools.pigstats.PigStats;

public abstract class Launcher {
    private static final Log log = LogFactory.getLog(Launcher.class);
    
    long totalHadoopTimeSpent;    
    String newLine = "\n";
    boolean pigException = false;
    boolean outOfMemory = false;
    final String OOM_ERR = "OutOfMemoryError";

    protected List<FileSpec> succeededStores = null;
    protected List<FileSpec> failedStores = null;
    
    protected Launcher(){
        totalHadoopTimeSpent = 0;
        //handle the windows portion of \r
        if (System.getProperty("os.name").toUpperCase().startsWith("WINDOWS")) {
            newLine = "\r\n";
        }
        reset();
    }

    /**
     * Returns a list of locations of results that have been
     * successfully completed.
     * @return A list of filspecs that corresponds to the locations of
     * the successful stores.
     */
    public List<FileSpec> getSucceededFiles() {
        return succeededStores;
    }

    /**
     * Returns a list of locations of results that have failed.
     * @return A list of filspecs that corresponds to the locations of
     * the failed stores.
     */
    public List<FileSpec> getFailedFiles() {
        return failedStores;
    }

    /**
     * Resets the state after a launch
     */
    public void reset() {
        succeededStores = new LinkedList<FileSpec>();
        failedStores = new LinkedList<FileSpec>();
    }

    /**
     * Method to launch pig for hadoop either for a cluster's
     * job tracker or for a local job runner. THe only difference
     * between the two is the job client. Depending on the pig context
     * the job client will be initialize to one of the two.
     * Launchers for other frameworks can overide these methods.
     * Given an input PhysicalPlan, it compiles it
     * to get a MapReduce Plan. The MapReduce plan which
     * has multiple MapReduce operators each one of which
     * has to be run as a map reduce job with dependency
     * information stored in the plan. It compiles the
     * MROperPlan into a JobControl object. Each Map Reduce
     * operator is converted into a Job and added to the JobControl
     * object. Each Job also has a set of dependent Jobs that
     * are created using the MROperPlan.
     * The JobControl object is obtained from the JobControlCompiler
     * Then a new thread is spawned that submits these jobs
     * while respecting the dependency information.
     * The parent thread monitors the submitted jobs' progress and
     * after it is complete, stops the JobControl thread.
     * @param php
     * @param grpName
     * @param pc
     * @throws PlanException
     * @throws VisitorException
     * @throws IOException
     * @throws ExecException
     * @throws JobCreationException
     */
    public abstract PigStats launchPig(PhysicalPlan php, String grpName, PigContext pc)
            throws PlanException, VisitorException, IOException, ExecException,
            JobCreationException, Exception;

    /**
     * Explain how a pig job will be executed on the underlying
     * infrastructure.
     * @param pp PhysicalPlan to explain
     * @param pc PigContext to use for configuration
     * @param ps PrintStream to write output on.
     * @param format Format to write in
     * @param verbose Amount of information to print
     * @throws VisitorException
     * @throws IOException
     */
    public abstract void explain(
            PhysicalPlan pp,
            PigContext pc,
            PrintStream ps,
            String format,
            boolean verbose) throws PlanException,
                                   VisitorException,
                                   IOException;
    
    protected boolean isComplete(double prog){
        return (int)(Math.ceil(prog)) == (int)1;
    }
    
    protected void getStats(Job job, JobClient jobClient, boolean errNotDbg, PigContext pigContext) throws Exception {
        JobID MRJobID = job.getAssignedJobID();
        String jobMessage = job.getMessage();
        if(MRJobID == null) {
            try {
                throw getExceptionFromString(jobMessage);
            } catch (Exception e) {
                //just get the first line in the message and log the rest
                String firstLine = getFirstLineFromMessage(jobMessage);
                
                LogUtils.writeLog(new Exception(jobMessage), pigContext.getProperties().getProperty("pig.logfile"), 
                                  log, false, null, false, false);
                int errCode = 2997;
                String msg = "Unable to recreate exception from backend error: " + firstLine;
                throw new ExecException(msg, errCode, PigException.BUG, e);
            } 
        }
        try {
            TaskReport[] mapRep = jobClient.getMapTaskReports(MRJobID);
            getErrorMessages(mapRep, "map", errNotDbg, pigContext);
            totalHadoopTimeSpent += computeTimeSpent(mapRep);
            TaskReport[] redRep = jobClient.getReduceTaskReports(MRJobID);
            getErrorMessages(redRep, "reduce", errNotDbg, pigContext);
            totalHadoopTimeSpent += computeTimeSpent(mapRep);
        } catch (IOException e) {
            if(job.getState() == Job.SUCCESS) {
                // if the job succeeded, let the user know that
                // we were unable to get statistics
                log.warn("Unable to get job related diagnostics");
            } else {
                throw e;
            }
        }
    }
    
    protected long computeTimeSpent(TaskReport[] mapReports) {
        long timeSpent = 0;
        for (TaskReport r : mapReports) {
            timeSpent += (r.getFinishTime() - r.getStartTime());
        }
        return timeSpent;
    }
    
    
    protected void getErrorMessages(TaskReport reports[], String type, boolean errNotDbg, PigContext pigContext) throws Exception
    {        
    	for (int i = 0; i < reports.length; i++) {
            String msgs[] = reports[i].getDiagnostics();
            ArrayList<Exception> exceptions = new ArrayList<Exception>();
            boolean jobFailed = false;
            float successfulProgress = 1.0f;
            if (msgs.length > 0) {
            	//if the progress reported is not 1.0f then the map or reduce job failed
            	//this comparison is in place till Hadoop 0.20 provides methods to query
            	//job status            	
            	if(reports[i].getProgress() != successfulProgress) {
                    jobFailed = true;
            	}
                Set<String> errorMessageSet = new HashSet<String>();
                for (int j = 0; j < msgs.length; j++) {                	
                    if(!errorMessageSet.contains(msgs[j])) {
                        errorMessageSet.add(msgs[j]);
                        if (errNotDbg) {
                            //errNotDbg is used only for failed jobs
                            //keep track of all the unique exceptions
                            try {
                                Exception e = getExceptionFromString(msgs[j]);
                                exceptions.add(e);
                            } catch (Exception e1) {
                                String firstLine = getFirstLineFromMessage(msgs[j]);                                
                                LogUtils.writeLog(new Exception(msgs[j]), pigContext.getProperties().getProperty("pig.logfile"), 
                                                  log, false, null, false, false);
                                int errCode = 2997;
                                String msg = "Unable to recreate exception from backed error: " + firstLine;
                                throw new ExecException(msg, errCode, PigException.BUG, e1);
                            }
                        } else {
                            log.debug("Error message from task (" + type + ") " +
                                      reports[i].getTaskID() + msgs[j]);
                        }
                    }
                }
            }
            
            //if its a failed job then check if there is more than one exception
            //more than one exception implies possibly different kinds of failures
            //log all the different failures and throw the exception corresponding
            //to the first failure
            if(jobFailed) {
                if(exceptions.size() > 1) {
                    for(int j = 0; j < exceptions.size(); ++j) {
                        String headerMessage = "Error message from task (" + type + ") " + reports[i].getTaskID();
                        LogUtils.writeLog(exceptions.get(j), pigContext.getProperties().getProperty("pig.logfile"), log, false, headerMessage, false, false);
                    }
                    throw exceptions.get(0);
                } else if(exceptions.size() == 1) {
                	throw exceptions.get(0);
                } else {
                	int errCode = 2115;
                	String msg = "Internal error. Expected to throw exception from the backend. Did not find any exception to throw.";
                	throw new ExecException(msg, errCode, PigException.BUG);
                }
            }
        }
    }
    
    /**
     * Compute the progress of the current job submitted 
     * through the JobControl object jc to the JobClient jobClient
     * @param jc - The JobControl object that has been submitted
     * @param jobClient - The JobClient to which it has been submitted
     * @return The progress as a precentage in double format
     * @throws IOException
     */
    protected double calculateProgress(JobControl jc, JobClient jobClient) throws IOException{
        double prog = 0.0;
        prog += jc.getSuccessfulJobs().size();
        
        List runnJobs = jc.getRunningJobs();
        for (Object object : runnJobs) {
            Job j = (Job)object;
            prog += progressOfRunningJob(j, jobClient);
        }
        return prog;
    }
    
    /**
     * Returns the progress of a Job j which is part of a submitted
     * JobControl object. The progress is for this Job. So it has to
     * be scaled down by the num of jobs that are present in the 
     * JobControl.
     * @param j - The Job for which progress is required
     * @param jobClient - the JobClient to which it has been submitted
     * @return Returns the percentage progress of this Job
     * @throws IOException
     */
    protected double progressOfRunningJob(Job j, JobClient jobClient) throws IOException{
        JobID mrJobID = j.getAssignedJobID();
        RunningJob rj = jobClient.getJob(mrJobID);
        if(rj==null && j.getState()==Job.SUCCESS)
            return 1;
        else if(rj==null)
            return 0;
        else{
            double mapProg = rj.mapProgress();
            double redProg = rj.reduceProgress();
            return (mapProg + redProg)/2;
        }
    }
    public long getTotalHadoopTimeSpent() {
        return totalHadoopTimeSpent;
    }


    /**
     * 
     * @param stackTraceLine The string representation of {@link Throwable#printStackTrace() printStackTrace}
     * Handles internal PigException and its subclasses that override the {@link Throwable#toString() toString} method
     * @return An exception object whose string representation of printStackTrace is the input stackTrace 
     * @throws Exception
     */
    Exception getExceptionFromString(String stackTrace) throws Exception{
        String[] lines = stackTrace.split(newLine);
        Throwable t = getExceptionFromStrings(lines, 0);
        
        if(!pigException) {
            int errCode = 6015;
            String msg = "During execution, encountered a Hadoop error.";
            ExecException ee = new ExecException(msg, errCode, PigException.REMOTE_ENVIRONMENT, t);
            ee.setStackTrace(t.getStackTrace());
            return ee;
        } else {
            pigException = false;
            if(outOfMemory) {
                outOfMemory = false;
                int errCode = 6016;
                String msg = "Out of memory.";
                ExecException ee = new ExecException(msg, errCode, PigException.REMOTE_ENVIRONMENT, t);
                ee.setStackTrace(t.getStackTrace());
                return ee;
            }
            return (Exception)t;
        }
    }

    /**
     * 
     * @param stackTraceLine An array of strings that represent {@link Throwable#printStackTrace() printStackTrace}
     * output, split by newline
     * @return An exception object whose string representation of printStackTrace is the input stackTrace 
     * @throws Exception
     */
    private Throwable getExceptionFromStrings(String[] stackTraceLines, int startingLineNum) throws Exception{
        /*
         * parse the array of string and throw the appropriate exception
         * first: from the line startingLineNum extract the exception name extract the message if any
         * fourth: create the appropriate exception and return it
         * An example of the stack trace:
		org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to int.
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:152)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr.getNext(LessThanExpr.java:85)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter.getNext(POFilter.java:148)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:184)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:174)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
         */

        int prevStartingLineNum = startingLineNum;
        
        if(stackTraceLines.length > 0 && startingLineNum < (stackTraceLines.length - 1)) {
            
            //the regex for matching the exception class name; note the use of the $ for matching nested classes
            String exceptionNameDelimiter = "(\\w+(\\$\\w+)?\\.)+\\w+";
            Pattern exceptionNamePattern = Pattern.compile(exceptionNameDelimiter);
            
        	//from the first line extract the exception name and the exception message
            Matcher exceptionNameMatcher = exceptionNamePattern.matcher(stackTraceLines[startingLineNum]);
            String exceptionName = null;
            String exceptionMessage = null;
            if (exceptionNameMatcher.find()) {
            	exceptionName = exceptionNameMatcher.group();
            	/*
            	 * note that the substring is from end + 2
            	 * the regex matcher ends at one position beyond the match
            	 * in this case it will end at colon (:)
            	 * the exception message will have a preceding space (after the colon (:)) 
            	 */ 
            	if (exceptionName.contains(OOM_ERR)) {
            	    outOfMemory = true;
            	}
            	
            	if(stackTraceLines[startingLineNum].length() > exceptionNameMatcher.end()) {
	            	exceptionMessage = stackTraceLines[startingLineNum].substring(exceptionNameMatcher.end() + 2);
            	}
            	
            	++startingLineNum;
            }
        	
            //the exceptionName should not be null
            if(exceptionName != null) {            	

                ArrayList<StackTraceElement> stackTraceElements = new ArrayList<StackTraceElement>();
                
                //Create stack trace elements for the remaining lines
                String stackElementRegex = "\\s+at\\s+(\\w+(\\$\\w+)?\\.)+(\\<)?\\w+(\\>)?";
                Pattern stackElementPattern = Pattern.compile(stackElementRegex);
                String pigExceptionRegex = "org\\.apache\\.pig\\.";
                Pattern pigExceptionPattern = Pattern.compile(pigExceptionRegex);              
                String moreElementRegex = "\\s+\\.\\.\\.\\s+\\d+\\s+more";
                Pattern moreElementPattern = Pattern.compile(moreElementRegex);
                
                
                String pigPackageRegex = "org.apache.pig";
                
                int lineNum = startingLineNum;
                for(; lineNum < (stackTraceLines.length - 1); ++lineNum) {
                    Matcher stackElementMatcher = stackElementPattern.matcher(stackTraceLines[lineNum]);

                    if(stackElementMatcher.find()) {
                        StackTraceElement ste = getStackTraceElement(stackTraceLines[lineNum]);
                        stackTraceElements.add(ste);
                        String className = ste.getClassName();
                        Matcher pigExceptionMatcher = pigExceptionPattern.matcher(className);
                        if(pigExceptionMatcher.find()) {
                            pigException = true;
                        }                       
                    } else {
                        Matcher moreElementMatcher = moreElementPattern.matcher(stackTraceLines[lineNum]);
                        if(moreElementMatcher.find()) {
                            ++lineNum;
                        }
                        break;
                    }
                }
                
                startingLineNum = lineNum;               

            	//create the appropriate exception; setup the stack trace and message
            	Object object = PigContext.instantiateFuncFromSpec(exceptionName);
            	
            	if(object instanceof PigException) {
            		//extract the error code and message the regex for matching the custom format of ERROR <ERROR CODE>:
            		String errMessageRegex = "ERROR\\s+\\d+:";
            		Pattern errMessagePattern = Pattern.compile(errMessageRegex);
            		Matcher errMessageMatcher = errMessagePattern.matcher(exceptionMessage);
            		
            		if(errMessageMatcher.find()) {
            			String errMessageStub = errMessageMatcher.group();
            			/*
            			 * extract the actual exception message sans the ERROR <ERROR CODE>:
            			 * again note that the matcher ends at the space following the colon (:)
            			 * the exception message appears after the space and hence the end + 1
            			 */            			
            			exceptionMessage = exceptionMessage.substring(errMessageMatcher.end() + 1);
                		
            			//the regex to match the error code wich is a string of numerals
            			String errCodeRegex = "\\d+";
                		Pattern errCodePattern = Pattern.compile(errCodeRegex);
                		Matcher errCodeMatcher = errCodePattern.matcher(errMessageStub);
                		
                		String code = null;
                		if(errCodeMatcher.find()) {
                			code = errCodeMatcher.group();	
                		}
            			
                		//could receive a number format exception here but it will be propagated up the stack                		
                		int errCode = Integer.parseInt(code);
                		
                		//create the exception with the message and then set the error code and error source
                		FuncSpec funcSpec = new FuncSpec(exceptionName, exceptionMessage);		                		
                		object = PigContext.instantiateFuncFromSpec(funcSpec);
                		((PigException)object).setErrorCode(errCode);
                		((PigException)object).setErrorSource(PigException.determineErrorSource(errCode));
            		} else { //else for if(errMessageMatcher.find())
            			/*
            			 * did not find the error code which means that the PigException or its
            			 * subclass is not returning the error code
            			 * highly unlikely: should never be here
            			 */
            			FuncSpec funcSpec = new FuncSpec(exceptionName, exceptionMessage);		                		
                		object = PigContext.instantiateFuncFromSpec(funcSpec);
                		((PigException)object).setErrorCode(2997);//generic error code
                		((PigException)object).setErrorSource(PigException.BUG);		                			
            		}		                		
            	} else { //else for if(object instanceof PigException)
            		//its not PigException; create the exception with the message
            		object = PigContext.instantiateFuncFromSpec(exceptionName + "(" + exceptionMessage + ")");
            	}
            	
            	StackTraceElement[] steArr = new StackTraceElement[stackTraceElements.size()];
            	((Throwable)object).setStackTrace((StackTraceElement[])(stackTraceElements.toArray(steArr)));
            	
            	if(startingLineNum < (stackTraceLines.length - 1)) {
            	    Throwable e = getExceptionFromStrings(stackTraceLines, startingLineNum);
            	    ((Throwable)object).initCause(e);
            	}
            	
            	return (Throwable)object;
            } else { //else for if(exceptionName != null)
        		int errCode = 2055;
        		String msg = "Did not find exception name to create exception from string: " + stackTraceLines.toString();
        		throw new ExecException(msg, errCode, PigException.BUG);
            }
        } else { //else for if(lines.length > 0)
    		int errCode = 2056;
    		String msg = "Cannot create exception from empty string.";
    		throw new ExecException(msg, errCode, PigException.BUG);
        }
    }
    
    /**
     * 
     * @param line the string representation of a stack trace returned by {@link Throwable#printStackTrace() printStackTrace}
     * @return the StackTraceElement object representing the stack trace
     * @throws Exception
     */
    public StackTraceElement getStackTraceElement(String line) throws Exception{
    	/*    	
    	 * the format of the line is something like:
    	 *     	        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
    	 * note the white space before the 'at'. Its not of much importance but noted for posterity.    
    	 */
    	String[] items;
    	
    	/*
    	 * regex for matching the fully qualified method Name
    	 * note the use of the $ for matching nested classes
    	 * and the use of < and > for constructors
    	 */
    	String qualifiedMethodNameRegex = "(\\w+(\\$\\w+)?\\.)+(<)?\\w+(>)?";
        Pattern qualifiedMethodNamePattern = Pattern.compile(qualifiedMethodNameRegex);
        Matcher contentMatcher = qualifiedMethodNamePattern.matcher(line);
        
        //org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
    	String content = null;
        if(contentMatcher.find()) {
        	content = line.substring(contentMatcher.start());
        } else {
    		int errCode = 2057;
    		String msg = "Did not find fully qualified method name to reconstruct stack trace: " + line;
    		throw new ExecException(msg, errCode, PigException.BUG);        	
        }
        
        Matcher qualifiedMethodNameMatcher = qualifiedMethodNamePattern.matcher(content);
        
        //org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map
        String qualifiedMethodName = null;
        //(PigMapOnly.java:65)
        String fileDetails = null;
        
        if(qualifiedMethodNameMatcher.find()) {
        	qualifiedMethodName = qualifiedMethodNameMatcher.group();
        	fileDetails = content.substring(qualifiedMethodNameMatcher.end() + 1);
        } else {
    		int errCode = 2057;
    		String msg = "Did not find fully qualified method name to reconstruct stack trace: " + line;
    		throw new ExecException(msg, errCode, PigException.BUG);        	
        }
    	
        //From the fully qualified method name, extract the declaring class and method name
        items = qualifiedMethodName.split("\\.");
        
        //initialize the declaringClass (to org in most cases)
        String declaringClass = items[0]; 
        //the last member is always the method name
        String methodName = items[items.length - 1];
        
        //concatenate the names by adding the dot (.) between the members till the penultimate member
        for(int i = 1; i < items.length - 1; ++i) {
        	declaringClass += ".";
        	declaringClass += items[i];
        }
        
        //from the file details extract the file name and the line number
        //PigMapOnly.java:65
        fileDetails = fileDetails.substring(0, fileDetails.length() - 1);
        items = fileDetails.split(":");
        //PigMapOnly.java
        String fileName = null;
        int lineNumber = 0;
        if(items.length > 0) {
            fileName = items[0];
            lineNumber = Integer.parseInt(items[1]);
        }
        return new StackTraceElement(declaringClass, methodName, fileName, lineNumber);
    }
    
    private String getFirstLineFromMessage(String message) {
        String[] messages = message.split(newLine);
        if(messages.length > 0) {
            return messages[0];
        } else {
            return new String(message);
        }        
    }

}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.*;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.OutputFormat;
import org.apache.hadoop.mapred.jobcontrol.Job;
import org.apache.hadoop.mapred.jobcontrol.JobControl;

import org.apache.pig.ComparisonFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.PigException;
import org.apache.pig.StoreConfig;
import org.apache.pig.StoreFunc;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.backend.hadoop.HDataType;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataType;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.PigContext;
import org.apache.pig.impl.io.FileLocalizer;
import org.apache.pig.impl.io.FileSpec;
import org.apache.pig.impl.io.NullableBytesWritable;
import org.apache.pig.impl.io.NullableDoubleWritable;
import org.apache.pig.impl.io.NullableFloatWritable;
import org.apache.pig.impl.io.NullableIntWritable;
import org.apache.pig.impl.io.NullableLongWritable;
import org.apache.pig.impl.io.NullableText;
import org.apache.pig.impl.io.NullableTuple;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.util.JarManager;
import org.apache.pig.impl.util.ObjectSerializer;
import org.apache.pig.impl.util.Pair;

/**
 * This is compiler class that takes an MROperPlan and converts
 * it into a JobControl object with the relevant dependency info
 * maintained. The JobControl Object is made up of Jobs each of
 * which has a JobConf. The MapReduceOper corresponds to a Job
 * and the getJobCong method returns the JobConf that is configured
 * as per the MapReduceOper
 *
 * <h2>Comparator Design</h2>
 * <p>
 * A few words on how comparators are chosen.  In almost all cases we use raw
 * comparators (the one exception being when the user provides a comparison
 * function for order by).  For order by queries the PigTYPERawComparator
 * functions are used, where TYPE is Int, Long, etc.  These comparators are
 * null aware and asc/desc aware.  The first byte of each of the
 * NullableTYPEWritable classes contains info on whether the value is null.
 * Asc/desc is written as an array into the JobConf with the key pig.sortOrder
 * so that it can be read by each of the comparators as part of their 
 * setConf call.
 * <p>
 * For non-order by queries, PigTYPEWritableComparator classes are used.
 * These are all just type specific instances of WritableComparator.
 *
 */
public class JobControlCompiler{
    MROperPlan plan;
    Configuration conf;
    PigContext pigContext;
    
    private final Log log = LogFactory.getLog(getClass());
    
    public static final String PIG_STORE_CONFIG = "pig.store.config";

    public static final String LOG_DIR = "_logs";

    // A mapping of job to pair of store locations and tmp locations for that job
    private Map<Job, Pair<List<POStore>, Path>> jobStoreMap;

    public JobControlCompiler(PigContext pigContext, Configuration conf) throws IOException {
        this.pigContext = pigContext;
        this.conf = conf;
        jobStoreMap = new HashMap<Job, Pair<List<POStore>, Path>>();
    }

    /**
     * Returns all store locations of a previously compiled job
     */
    public List<POStore> getStores(Job job) {
        Pair<List<POStore>, Path> pair = jobStoreMap.get(job);
        if (pair != null && pair.first != null) {
            return pair.first;
        } else {
            return new ArrayList<POStore>();
        }
    }

    /**
     * Resets the state
     */
    public void reset() {
        jobStoreMap = new HashMap<Job, Pair<List<POStore>, Path>>();
    }

    /**
     * Moves all the results of a collection of MR jobs to the final
     * output directory. Some of the results may have been put into a
     * temp location to work around restrictions with multiple output
     * from a single map reduce job.
     *
     * This method should always be called after the job execution
     * completes.
     */
    public void moveResults(List<Job> completedJobs) throws IOException {
        for (Job job: completedJobs) {
            Pair<List<POStore>, Path> pair = jobStoreMap.get(job);
            if (pair != null && pair.second != null) {
                Path tmp = pair.second;
                Path abs = new Path(tmp, "abs");
                Path rel = new Path(tmp, "rel");
                FileSystem fs = tmp.getFileSystem(conf);

                if (fs.exists(abs)) {
                    moveResults(abs, abs.toUri().getPath(), fs);
                }
                
                if (fs.exists(rel)) {        
                    moveResults(rel, rel.toUri().getPath()+"/", fs);
                }
            }
        }
    }

    /**
     * Walks the temporary directory structure to move (rename) files
     * to their final location.
     */
    private void moveResults(Path p, String rem, FileSystem fs) throws IOException {
        for (FileStatus fstat: fs.listStatus(p)) {
            Path src = fstat.getPath();
            if (fstat.isDir()) {
                fs.mkdirs(removePart(src, rem));
                moveResults(fstat.getPath(), rem, fs);
            } else {
                Path dst = removePart(src, rem);
                fs.rename(src,dst);
            }
        }
    }

    private Path removePart(Path src, String part) {
        URI uri = src.toUri();
        String pathStr = uri.getPath().replace(part, "");
        return new Path(pathStr);
    }

    private Path makeTmpPath() throws IOException {
        Path tmpPath = null;
        for (int tries = 0;;) {
            try {
                tmpPath = 
                    new Path(FileLocalizer
                             .getTemporaryPath(null, pigContext).toString());
                FileSystem fs = tmpPath.getFileSystem(conf);
                tmpPath = tmpPath.makeQualified(fs);
                fs.mkdirs(tmpPath);
                break;
            } catch (IOException ioe) {
                if (++tries==100) {
                    throw ioe;
                }
            }
        }
        return tmpPath;
    }

    /**
     * The map between MapReduceOpers and their corresponding Jobs
     */
    Map<OperatorKey, Job> seen = new Hashtable<OperatorKey, Job>();
    
    /**
     * Compiles all jobs that have no dependencies removes them from
     * the plan and returns. Should be called with the same plan until
     * exhausted. 
     * @param plan - The MROperPlan to be compiled
     * @param grpName - The name given to the JobControl
     * @return JobControl object - null if no more jobs in plan
     * @throws JobCreationException
     */
    public JobControl compile(MROperPlan plan, String grpName) throws JobCreationException{
        this.plan = plan;

        if (plan.size() == 0) {
            return null;
        }

        JobControl jobCtrl = new JobControl(grpName);

        try {
            List<MapReduceOper> roots = new LinkedList<MapReduceOper>();
            roots.addAll(plan.getRoots());
            for (MapReduceOper mro: roots) {
                jobCtrl.addJob(getJob(mro, conf, pigContext));
                plan.remove(mro);
            }
        } catch (JobCreationException jce) {
        	throw jce;
        } catch(Exception e) {
            int errCode = 2017;
            String msg = "Internal error creating job configuration.";
            throw new JobCreationException(msg, errCode, PigException.BUG, e);
        }

        return jobCtrl;
    }
        
    /**
     * The method that creates the Job corresponding to a MapReduceOper.
     * The assumption is that
     * every MapReduceOper will have a load and a store. The JobConf removes
     * the load operator and serializes the input filespec so that PigInputFormat can
     * take over the creation of splits. It also removes the store operator
     * and serializes the output filespec so that PigOutputFormat can take over
     * record writing. The remaining portion of the map plan and reduce plans are
     * serialized and stored for the PigMapReduce or PigMapOnly objects to take over
     * the actual running of the plans.
     * The Mapper &amp; Reducer classes and the required key value formats are set.
     * Checks if this is a map only job and uses PigMapOnly class as the mapper
     * and uses PigMapReduce otherwise.
     * If it is a Map Reduce job, it is bound to have a package operator. Remove it from
     * the reduce plan and serializes it so that the PigMapReduce class can use it to package
     * the indexed tuples received by the reducer.
     * @param mro - The MapReduceOper for which the JobConf is required
     * @param conf - the Configuration object from which JobConf is built
     * @param pigContext - The PigContext passed on from execution engine
     * @return Job corresponding to mro
     * @throws JobCreationException
     */
    private Job getJob(MapReduceOper mro, Configuration conf, PigContext pigContext) throws JobCreationException{
        JobConf jobConf = new JobConf(conf);
        ArrayList<Pair<FileSpec, Boolean>> inp = new ArrayList<Pair<FileSpec, Boolean>>();
        ArrayList<List<OperatorKey>> inpTargets = new ArrayList<List<OperatorKey>>();
        ArrayList<POStore> storeLocations = new ArrayList<POStore>();
        Path tmpLocation = null;
        
        //Set the User Name for this job. This will be
        //used as the working directory
        String user = System.getProperty("user.name");
        jobConf.setUser(user != null ? user : "Pigster");

        try{        
            //Process the POLoads
            List<POLoad> lds = PlanHelper.getLoads(mro.mapPlan);
            
            if(lds!=null && lds.size()>0){
                for (POLoad ld : lds) {
                    
                    Pair<FileSpec, Boolean> p = new Pair<FileSpec, Boolean>(ld.getLFile(), ld.isSplittable());
                    //Store the inp filespecs
                    inp.add(p);
                    
                    //Store the target operators for tuples read
                    //from this input
                    List<PhysicalOperator> ldSucs = mro.mapPlan.getSuccessors(ld);
                    List<OperatorKey> ldSucKeys = new ArrayList<OperatorKey>();
                    if(ldSucs!=null){
                        for (PhysicalOperator operator2 : ldSucs) {
                            ldSucKeys.add(operator2.getOperatorKey());
                        }
                    }
                    inpTargets.add(ldSucKeys);
                    //Remove the POLoad from the plan
                    mro.mapPlan.remove(ld);
                }
            }

            //Create the jar of all functions reuired
            File submitJarFile = File.createTempFile("Job", ".jar");
            // ensure the job jar is deleted on exit
            submitJarFile.deleteOnExit();
            FileOutputStream fos = new FileOutputStream(submitJarFile);
            JarManager.createJar(fos, mro.UDFs, pigContext);
            
            //Start setting the JobConf properties
            jobConf.setJar(submitJarFile.getPath());
            jobConf.set("pig.inputs", ObjectSerializer.serialize(inp));
            jobConf.set("pig.inpTargets", ObjectSerializer.serialize(inpTargets));
            jobConf.set("pig.pigContext", ObjectSerializer.serialize(pigContext));
            // this is for unit tests since some don't create PigServer
            if (pigContext.getProperties().getProperty(PigContext.JOB_NAME) != null)
                jobConf.setJobName(pigContext.getProperties().getProperty(PigContext.JOB_NAME));
    
            // Setup the DistributedCache for this job
            setupDistributedCache(pigContext, jobConf, pigContext.getProperties(), 
                                  "pig.streaming.ship.files", true);
            setupDistributedCache(pigContext, jobConf, pigContext.getProperties(), 
                                  "pig.streaming.cache.files", false);

            jobConf.setInputFormat(PigInputFormat.class);
            
            //Process POStore and remove it from the plan
            List<POStore> mapStores = PlanHelper.getStores(mro.mapPlan);
            List<POStore> reduceStores = PlanHelper.getStores(mro.reducePlan);

            for (POStore st: mapStores) {
                storeLocations.add(st);
            }

            for (POStore st: reduceStores) {
                storeLocations.add(st);
            }

            if (mapStores.size() + reduceStores.size() == 1) { // single store case
                log.info("Setting up single store job");
                
                POStore st;
                if (reduceStores.isEmpty()) {
                    st = mapStores.remove(0);
                    mro.mapPlan.remove(st);
                }
                else {
                    st = reduceStores.remove(0);
                    mro.reducePlan.remove(st);
                }

                // If the StoreFunc associate with the POStore is implements
                // getStorePreparationClass() and returns a non null value,
                // then it could be wanting to implement OutputFormat for writing out to hadoop
                // Check if this is the case, if so, use the OutputFormat class the 
                // StoreFunc gives us else use our default PigOutputFormat
                Object storeFunc = PigContext.instantiateFuncFromSpec(st.getSFile().getFuncSpec());
                Class sPrepClass = null;
                try {
                    sPrepClass = ((StoreFunc)storeFunc).getStorePreparationClass();
                } catch(AbstractMethodError e) {
                    // this is for backward compatibility wherein some old StoreFunc
                    // which does not implement getStorePreparationClass() is being
                    // used. In this case, we want to just use PigOutputFormat
                    sPrepClass = null;
                }
                if(sPrepClass != null && OutputFormat.class.isAssignableFrom(sPrepClass)) {
                    jobConf.setOutputFormat(sPrepClass);
                } else {
                    jobConf.setOutputFormat(PigOutputFormat.class);
                }
                
                //set out filespecs
                String outputPath = st.getSFile().getFileName();
                FuncSpec outputFuncSpec = st.getSFile().getFuncSpec();
                FileOutputFormat.setOutputPath(jobConf, new Path(outputPath));
                
                // serialize the store func spec using ObjectSerializer
                // ObjectSerializer.serialize() uses default java serialization
                // and then further encodes the output so that control characters
                // get encoded as regular characters. Otherwise any control characters
                // in the store funcspec would break the job.xml which is created by
                // hadoop from the jobconf.
                jobConf.set("pig.storeFunc", ObjectSerializer.serialize(outputFuncSpec.toString()));
                jobConf.set(PIG_STORE_CONFIG, 
                            ObjectSerializer.serialize(new StoreConfig(outputPath, st.getSchema())));

                jobConf.set("pig.streaming.log.dir", 
                            new Path(outputPath, LOG_DIR).toString());
                jobConf.set("pig.streaming.task.output.dir", outputPath);
            } 
           else { // multi store case
                log.info("Setting up multi store job");

                tmpLocation = makeTmpPath();

                FileSystem fs = tmpLocation.getFileSystem(conf);
                for (POStore st: mapStores) {
                    Path tmpOut = new Path(
                        tmpLocation,
                        PlanHelper.makeStoreTmpPath(st.getSFile().getFileName()));
                    fs.mkdirs(tmpOut);
                }

                jobConf.setOutputFormat(PigOutputFormat.class);
                FileOutputFormat.setOutputPath(jobConf, tmpLocation);

                jobConf.set("pig.streaming.log.dir", 
                            new Path(tmpLocation, LOG_DIR).toString());
                jobConf.set("pig.streaming.task.output.dir", tmpLocation.toString());
           }

            // store map key type
            // this is needed when the key is null to create
            // an appropriate NullableXXXWritable object
            jobConf.set("pig.map.keytype", ObjectSerializer.serialize(new byte[] { mro.mapKeyType }));

            // set parent plan in all operators in map and reduce plans
            // currently the parent plan is really used only when POStream is present in the plan
            new PhyPlanSetter(mro.mapPlan).visit();
            new PhyPlanSetter(mro.reducePlan).visit();

            POPackage pack = null;
            if(mro.reducePlan.isEmpty()){
                //MapOnly Job
                jobConf.setMapperClass(PigMapOnly.Map.class);
                jobConf.setNumReduceTasks(0);
                jobConf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
                if(mro.isStreamInMap()) {
                    // this is used in Map.close() to decide whether the
                    // pipeline needs to be rerun one more time in the close()
                    // The pipeline is rerun only if there was a stream
                    jobConf.set("pig.stream.in.map", "true");
                }
            }
            else{
                //Map Reduce Job
                //Process the POPackage operator and remove it from the reduce plan
                if(!mro.combinePlan.isEmpty()){
                    POPackage combPack = (POPackage)mro.combinePlan.getRoots().get(0);
                    mro.combinePlan.remove(combPack);
                    jobConf.setCombinerClass(PigCombiner.Combine.class);
                    jobConf.set("pig.combinePlan", ObjectSerializer.serialize(mro.combinePlan));
                    jobConf.set("pig.combine.package", ObjectSerializer.serialize(combPack));
                } else if (mro.needsDistinctCombiner()) {
                    jobConf.setCombinerClass(DistinctCombiner.Combine.class);
                    log.info("Setting identity combiner class.");
                }
                pack = (POPackage)mro.reducePlan.getRoots().get(0);
                mro.reducePlan.remove(pack);
                jobConf.setMapperClass(PigMapReduce.Map.class);
                jobConf.setReducerClass(PigMapReduce.Reduce.class);
                if (mro.requestedParallelism>0)
                    jobConf.setNumReduceTasks(mro.requestedParallelism);

                jobConf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
                if(mro.isStreamInMap()) {
                    // this is used in Map.close() to decide whether the
                    // pipeline needs to be rerun one more time in the close()
                    // The pipeline is rerun only if there was a stream
                    jobConf.set("pig.stream.in.map", "true");
                }
                jobConf.set("pig.reducePlan", ObjectSerializer.serialize(mro.reducePlan));
                if(mro.isStreamInReduce()) {
                    // this is used in Map.close() to decide whether the
                    // pipeline needs to be rerun one more time in the close()
                    // The pipeline is rerun only if there was a stream
                    jobConf.set("pig.stream.in.reduce", "true");
                }
                jobConf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
                Class<? extends WritableComparable> keyClass = HDataType.getWritableComparableTypes(pack.getKeyType()).getClass();
                jobConf.setOutputKeyClass(keyClass);
                jobConf.set("pig.reduce.key.type", Byte.toString(pack.getKeyType())); 
                selectComparator(mro, pack.getKeyType(), jobConf);
                jobConf.setOutputValueClass(NullableTuple.class);
            }
        
            if(mro.isGlobalSort() || mro.isLimitAfterSort()){
                // Only set the quantiles file and sort partitioner if we're a
                // global sort, not for limit after sort.
                if (mro.isGlobalSort()) {
                    jobConf.set("pig.quantilesFile", mro.getQuantFile());
                    jobConf.setPartitionerClass(WeightedRangePartitioner.class);
                }
                if(mro.UDFs.size()==1){
                    String compFuncSpec = mro.UDFs.get(0);
                    Class comparator = PigContext.resolveClassName(compFuncSpec);
                    if(ComparisonFunc.class.isAssignableFrom(comparator)) {
                        jobConf.setMapperClass(PigMapReduce.MapWithComparator.class);
                        jobConf.setReducerClass(PigMapReduce.ReduceWithComparator.class);
                        jobConf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
                        jobConf.set("pig.usercomparator", "true");
                        jobConf.setOutputKeyClass(NullableTuple.class);
                        jobConf.setOutputKeyComparatorClass(comparator);
                    }
                } else {
                    jobConf.set("pig.sortOrder",
                        ObjectSerializer.serialize(mro.getSortOrder()));
                }
            }
            
            Job job = new Job(jobConf);
            jobStoreMap.put(job,new Pair(storeLocations, tmpLocation));
            return job;
        } catch (JobCreationException jce) {
        	throw jce;
        } catch(Exception e) {
            int errCode = 2017;
            String msg = "Internal error creating job configuration.";
            throw new JobCreationException(msg, errCode, PigException.BUG, e);
        }
    }
    
    private List<PhysicalOperator> getRoots(PhysicalPlan php){
        List<PhysicalOperator> ret = new ArrayList<PhysicalOperator>();
        for (PhysicalOperator operator : php.getRoots()) {
            ret.add(operator);
        }
        return ret;
    }

    public static class PigWritableComparator extends WritableComparator {
        protected PigWritableComparator(Class c) {
            super(c);
        }

        public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2){
            return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
        }
    }

    public static class PigIntWritableComparator extends PigWritableComparator {
        public PigIntWritableComparator() {
            super(NullableIntWritable.class);
        }
    }

    public static class PigLongWritableComparator extends PigWritableComparator {
        public PigLongWritableComparator() {
            super(NullableLongWritable.class);
        }
    }

    public static class PigFloatWritableComparator extends PigWritableComparator {
        public PigFloatWritableComparator() {
            super(NullableFloatWritable.class);
        }
    }

    public static class PigDoubleWritableComparator extends PigWritableComparator {
        public PigDoubleWritableComparator() {
            super(NullableDoubleWritable.class);
        }
    }

    public static class PigCharArrayWritableComparator extends PigWritableComparator {
        public PigCharArrayWritableComparator() {
            super(NullableText.class);
        }
    }

    public static class PigDBAWritableComparator extends PigWritableComparator {
        public PigDBAWritableComparator() {
            super(NullableBytesWritable.class);
        }
    }

    public static class PigTupleWritableComparator extends PigWritableComparator {
        public PigTupleWritableComparator() {
            super(TupleFactory.getInstance().tupleClass());
        }
    }

    public static class PigBagWritableComparator extends PigWritableComparator {
        public PigBagWritableComparator() {
            super(BagFactory.getInstance().newDefaultBag().getClass());
        }
    }

    private void selectComparator(
            MapReduceOper mro,
            byte keyType,
            JobConf jobConf) throws JobCreationException {
        // If this operator is involved in an order by, use the pig specific raw
        // comparators.  If it has a cogroup, we need to set the comparator class
        // to the raw comparator and the grouping comparator class to pig specific
        // raw comparators (which skip the index).  Otherwise use the hadoop provided
        // raw comparator.
        
        // An operator has an order by if global sort is set or if it's successor has
        // global sort set (because in that case it's the sampling job) or if
        // it's a limit after a sort. 
        boolean hasOrderBy = false;
        if (mro.isGlobalSort() || mro.isLimitAfterSort()) {
            hasOrderBy = true;
        } else {
            List<MapReduceOper> succs = plan.getSuccessors(mro);
            if (succs != null) {
                MapReduceOper succ = succs.get(0);
                if (succ.isGlobalSort()) hasOrderBy = true;
            }
        }
        if (hasOrderBy) {
            switch (keyType) {
            case DataType.INTEGER:
                jobConf.setOutputKeyComparatorClass(PigIntRawComparator.class);
                break;

            case DataType.LONG:
                jobConf.setOutputKeyComparatorClass(PigLongRawComparator.class);
                break;

            case DataType.FLOAT:
                jobConf.setOutputKeyComparatorClass(PigFloatRawComparator.class);
                break;

            case DataType.DOUBLE:
                jobConf.setOutputKeyComparatorClass(PigDoubleRawComparator.class);
                break;

            case DataType.CHARARRAY:
                jobConf.setOutputKeyComparatorClass(PigTextRawComparator.class);
                break;

            case DataType.BYTEARRAY:
                jobConf.setOutputKeyComparatorClass(PigBytesRawComparator.class);
                break;

            case DataType.MAP:
                int errCode = 1068;
                String msg = "Using Map as key not supported.";
                throw new JobCreationException(msg, errCode, PigException.INPUT);

            case DataType.TUPLE:
                jobConf.setOutputKeyComparatorClass(PigTupleRawComparator.class);
                break;

            case DataType.BAG:
                errCode = 1068;
                msg = "Using Bag as key not supported.";
                throw new JobCreationException(msg, errCode, PigException.INPUT);

            default:
                break;
            }
            return;
        }

        switch (keyType) {
        case DataType.INTEGER:
            jobConf.setOutputKeyComparatorClass(PigIntWritableComparator.class);
            break;

        case DataType.LONG:
            jobConf.setOutputKeyComparatorClass(PigLongWritableComparator.class);
            break;

        case DataType.FLOAT:
            jobConf.setOutputKeyComparatorClass(PigFloatWritableComparator.class);
            break;

        case DataType.DOUBLE:
            jobConf.setOutputKeyComparatorClass(PigDoubleWritableComparator.class);
            break;

        case DataType.CHARARRAY:
            jobConf.setOutputKeyComparatorClass(PigCharArrayWritableComparator.class);
            break;

        case DataType.BYTEARRAY:
            jobConf.setOutputKeyComparatorClass(PigDBAWritableComparator.class);
            break;

        case DataType.MAP:
            int errCode = 1068;
            String msg = "Using Map as key not supported.";
            throw new JobCreationException(msg, errCode, PigException.INPUT);

        case DataType.TUPLE:
            jobConf.setOutputKeyComparatorClass(PigTupleWritableComparator.class);
            break;

        case DataType.BAG:
            errCode = 1068;
            msg = "Using Bag as key not supported.";
            throw new JobCreationException(msg, errCode, PigException.INPUT);

        default:
            errCode = 2036;
            msg = "Unhandled key type " + DataType.findTypeName(keyType);
            throw new JobCreationException(msg, errCode, PigException.BUG);
        }
    }

    private static void setupDistributedCache(PigContext pigContext,
                                              Configuration conf, 
                                              Properties properties, String key, 
                                              boolean shipToCluster) 
    throws IOException {
        // Turn on the symlink feature
        DistributedCache.createSymlink(conf);

        // Set up the DistributedCache for this job        
        String fileNames = properties.getProperty(key);
        if (fileNames != null) {
            String[] paths = fileNames.split(",");
            
            for (String path : paths) {
                path = path.trim();
                if (path.length() != 0) {
                    Path src = new Path(path);
                    
                    // Ensure that 'src' is a valid URI
                    URI srcURI = null;
                    try {
                        srcURI = new URI(src.toString());
                    } catch (URISyntaxException ue) {
                        int errCode = 6003;
                        String msg = "Invalid cache specification. " +
                        "File doesn't exist: " + src;
                        throw new ExecException(msg, errCode, PigException.USER_ENVIRONMENT);
                    }
                    
                    // Ship it to the cluster if necessary and add to the
                    // DistributedCache
                    if (shipToCluster) {
                        Path dst = 
                            new Path(FileLocalizer.getTemporaryPath(null, pigContext).toString());
                        FileSystem fs = dst.getFileSystem(conf);
                        fs.copyFromLocalFile(src, dst);
                        
                        // Construct the dst#srcName uri for DistributedCache
                        URI dstURI = null;
                        try {
                            dstURI = new URI(dst.toString() + "#" + src.getName());
                        } catch (URISyntaxException ue) {
                            byte errSrc = pigContext.getErrorSource();
                            int errCode = 0;
                            switch(errSrc) {
                            case PigException.REMOTE_ENVIRONMENT:
                                errCode = 6004;
                                break;
                            case PigException.USER_ENVIRONMENT:
                                errCode = 4004;
                                break;
                            default:
                                errCode = 2037;
                                break;
                            }
                            String msg = "Invalid ship specification. " +
                            "File doesn't exist: " + dst;
                            throw new ExecException(msg, errCode, errSrc);
                        }
                        DistributedCache.addCacheFile(dstURI, conf);
                    } else {
                        DistributedCache.addCacheFile(srcURI, conf);
                    }
                }
            }
        }
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.pig.data.DataType;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LORegexp extends BinaryExpressionOperator {
    private static final long serialVersionUID = 2L;

    /**
     * The expression and the column to be projected.
     */
    private static Log log = LogFactory.getLog(LORegexp.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param key
     *            Operator key to assign to this node.
     */
    public LORegexp(LogicalPlan plan, OperatorKey key) {
        super(plan, key);
    }

    public ExpressionOperator getOperand() {
        return getLhsOperand();
    }

    public String getRegexp() {
        ExpressionOperator op = getRhsOperand();
        if (!(op instanceof LOConst)) {
            throw new RuntimeException(
                "Regular expression patterns must be a constant.");
        }
        Object o = ((LOConst)op).getValue();
        // better be a string
        if (!(o instanceof String)) {
            throw new RuntimeException(
                "Regular expression patterns must be a string.");
        }

        return (String)o;
    }
    
    @Override
    public String name() {
        return "Regexp " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }


}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LODivide extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LODivide.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LODivide(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.mergeType(getLhsOperand().getType(), getRhsOperand().getType()));
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Divide " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.test;

import org.junit.Test;
import junit.framework.TestCase;
import junit.framework.AssertionFailedError;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.pig.ExecType;
import org.apache.pig.PigException;
import org.apache.pig.PigServer;
import org.apache.pig.impl.PigContext;
import org.apache.pig.tools.grunt.Grunt;
import org.apache.pig.tools.pigscript.parser.ParseException;
import org.apache.pig.impl.util.LogUtils;

import java.io.ByteArrayInputStream;
import java.io.InputStreamReader;
import java.io.BufferedReader;

public class TestGrunt extends TestCase {
    MiniCluster cluster = MiniCluster.buildCluster();
    private String basedir;

    private final Log log = LogFactory.getLog(getClass());

    public TestGrunt(String name) {
        super(name);
        cluster.setProperty("opt.multiquery","true");
        basedir = "test/org/apache/pig/test/data";
    }
    
/*    @Test 
    public void testCopyFromLocal() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "copyFromLocal /bin/sh . ;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }*/

    @Test 
    public void testDefine() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "define myudf org.apache.pig.builtin.AVG();\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        try {
            grunt.exec();
        } catch (Exception e) {
            assertTrue(e.getMessage().contains("Encountered \"define\""));
        }
        assertTrue(null != context.getFuncSpecFromAlias("myudf"));
    }

    @Test 
    public void testBagSchema() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'input1' as (b: bag{t(i: int, c:chararray, f: float)});\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testBagSchemaFail() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'input1'as (b: bag{t(i: int, c:chararray, f: float)});\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        try {
            grunt.exec();
        } catch (Exception e) {
            PigException pe = LogUtils.getPigException(e);
            String msg = (pe == null? e.getMessage(): pe.getMessage());
            assertTrue(msg.contains("Encountered \" \";"));
        }
    }

    @Test 
    public void testBagConstant() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'input1'; b = foreach a generate {(1, '1', 0.4f),(2, '2', 0.45)};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testBagConstantWithSchema() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'input1'; b = foreach a generate {(1, '1', 0.4f),(2, '2', 0.45)} as b: bag{t(i: int, c:chararray, d: double)};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testBagConstantInForeachBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'input1'; b = foreach a {generate {(1, '1', 0.4f),(2, '2', 0.45)};};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testBagConstantWithSchemaInForeachBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'input1'; b = foreach a {generate {(1, '1', 0.4f),(2, '2', 0.45)} as b: bag{t(i: int, c:chararray, d: double)};};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingAsInForeachBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast); b = group a by foo; c = foreach b {generate SUM(a.fast) as fast;};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingAsInForeachWithOutBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast); b = group a by foo; c = foreach b generate SUM(a.fast) as fast;\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingWordWithAsInForeachBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast); b = group a by foo; c = foreach b {generate SUM(a.fast);};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingWordWithAsInForeachWithOutBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast); b = group a by foo; c = foreach b generate SUM(a.fast);\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingWordWithAsInForeachWithOutBlock2() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "cash = load 'foo' as (foo, fast); b = foreach cash generate fast * 2.0;\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }


    @Test 
    public void testParsingGenerateInForeachBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); b = group a by foo; c = foreach b {generate a.regenerate;};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingGenerateInForeachWithOutBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); b = group a by foo; c = foreach b generate a.regenerate;\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingAsGenerateInForeachBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); b = group a by foo; c = foreach b {generate {(1, '1', 0.4f),(2, '2', 0.45)} as b: bag{t(i: int, cease:chararray, degenerate: double)}, SUM(a.fast) as fast, a.regenerate as degenerated;};\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test 
    public void testParsingAsGenerateInForeachWithOutBlock() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); b = group a by foo; c = foreach b generate {(1, '1', 0.4f),(2, '2', 0.45)} as b: bag{t(i: int, cease:chararray, degenerate: double)}, SUM(a.fast) as fast, a.regenerate as degenerated;\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testRunStatment() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate);" +
                        " run -param LIMIT=5 -param_file " + basedir +
                        "/test_broken.ppf " + basedir + "/testsub.pig; explain bar";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testExecStatment() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        boolean caught = false;
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate);" +
                        " exec -param LIMIT=5 -param FUNCTION=COUNT " +
                        "-param FILE=foo " + basedir + "/testsub.pig; explain bar;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
        
        try {
            grunt.exec();
        } catch (Exception e) {
            caught = true;
            assertTrue(e.getMessage().contains("alias bar"));
        }
        assertTrue(caught);
    }

    @Test
    public void testRunStatmentNested() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); run "
            +basedir+"/testsubnested_run.pig; explain bar";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testExecStatmentNested() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        boolean caught = false;
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); exec "
            +basedir+"/testsubnested_exec.pig; explain bar";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
        
        try {
            grunt.exec();
        } catch (Exception e) {
            caught = true;
            assertTrue(e.getMessage().contains("alias bar"));
        }
        assertTrue(caught);
    }
    
    @Test
    public void testExplainEmpty() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); run "
            +basedir+"/testsubnested_run.pig; explain";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testExplainScript() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); explain -script "
            +basedir+"/testsubnested_run.pig;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testExplainBrief() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); explain -brief -script "
            +basedir+"/testsubnested_run.pig;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testExplainDot() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); explain -dot -script "
            +basedir+"/testsubnested_run.pig;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testExplainOut() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "a = load 'foo' as (foo, fast, regenerate); explain -out /tmp -script "
            +basedir+"/testsubnested_run.pig;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testPartialExecution() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = "rmf bar; rmf baz; a = load 'file:test/org/apache/pig/test/data/passwd';"
            +"store a into 'bar'; exec; a = load 'bar'; store a into 'baz';\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testFileCmds() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "rmf bar; rmf baz;"
            +"a = load 'file:test/org/apache/pig/test/data/passwd';"
            +"store a into 'bar';"
            +"cp bar baz;"
            +"rm bar; rm baz;"
            +"store a into 'baz';"
            +"store a into 'bar';"
            +"rm baz; rm bar;"
            +"store a into 'baz';"
            +"mv baz bar;"
            +"b = load 'bar';"
            +"store b into 'baz';"
            +"cat baz;"
            +"rm baz;"
            +"rm bar;\n";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testCD() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "mkdir /tmp;"
            +"mkdir /tmp/foo;"
            +"cd /tmp;"
            +"rmf bar; rmf foo/baz;"
            +"copyFromLocal test/org/apache/pig/test/data/passwd bar;"
            +"a = load 'bar';"
            +"cd foo;"
            +"store a into 'baz';"
            +"cd /;"
            +"rm /tmp/bar; rm /tmp/foo/baz;";
        
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }
    
    @Test
    public void testDump() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "rmf bla;"
            +"a = load 'file:test/org/apache/pig/test/data/passwd';"
            +"e = group a by $0;"
            +"f = foreach e generate group, COUNT($1);"
            +"store f into 'bla';"
            +"f1 = load 'bla';"
            +"g = order f1 by $1;"
            +"dump g;";

        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testIllustrate() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "rmf bla;"
            +"a = load 'file:test/org/apache/pig/test/data/passwd';"
            +"e = group a by $0;"
            +"f = foreach e generate group, COUNT($1);"
            +"store f into 'bla';"
            +"f1 = load 'bla' as (f:chararray);"
            +"g = order f1 by $1;"
            +"illustrate g;";

        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testKeepGoing() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "rmf bar;"
            +"rmf foo;"
            +"rmf baz;"
            +"A = load 'file:test/org/apache/pig/test/data/passwd';"
            +"B = foreach A generate 1;"
            +"C = foreach A generate 0/0;"
            +"store B into 'foo';"
            +"store C into 'bar';"
            +"A = load 'file:test/org/apache/pig/test/data/passwd';"
            +"B = stream A through `false`;"
            +"store B into 'baz';"
            +"cat bar;";
            
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);
    
        grunt.exec();
    }

    @Test
    public void testKeepGoigFailed() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "rmf bar;"
            +"rmf foo;"
            +"rmf baz;"
            +"A = load 'file:test/org/apache/pig/test/data/passwd';"
            +"B = foreach A generate 1;"
            +"C = foreach A generate 0/0;"
            +"store B into 'foo';"
            +"store C into 'bar';"
            +"A = load 'file:test/org/apache/pig/test/data/passwd';"
            +"B = stream A through `false`;"
            +"store B into 'baz';"
            +"cat baz;";
            
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);

        boolean caught = false;
        try {
            grunt.exec();
        } catch (Exception e) {
            caught = true;
            assertTrue(e.getMessage().contains("baz does not exist"));
        }
        assertTrue(caught);
    }

    @Test
    public void testInvalidParam() throws Throwable {
        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
        PigContext context = server.getPigContext();
        
        String strCmd = 
            "run -param -param;";
            
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);

        boolean caught = false;
        try {
            grunt.exec();
        } catch (ParseException e) {
            caught = true;
            assertTrue(e.getMessage().contains("Encountered"));
        }
        assertTrue(caught);
    }

    @Test
    public void testStopOnFailure() throws Throwable {
        PigServer server = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
        PigContext context = server.getPigContext();
        context.getProperties().setProperty("stop.on.failure", ""+true);
        
        String strCmd = 
            "rmf bar;\n"
            +"rmf foo;\n"
            +"rmf baz;\n"
            +"copyFromLocal test/org/apache/pig/test/data/passwd pre;\n"
            +"A = load 'file:test/org/apache/pig/test/data/passwd';\n"
            +"B = stream A through `false`;\n"
            +"store B into 'bar' using BinStorage();\n"
            +"A = load 'bar';\n"
            +"store A into 'foo';\n"
            +"cp pre done;\n";
            
        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
        InputStreamReader reader = new InputStreamReader(cmd);
        
        Grunt grunt = new Grunt(new BufferedReader(reader), context);

        boolean caught = false;
        try {
            grunt.exec();
        } catch (PigException e) {
            caught = true;
            assertTrue(e.getErrorCode() == 6017);
        }

        assertFalse(server.existsFile("done"));
        assertTrue(caught);
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans;

import java.io.PrintStream;
import java.util.List;
import java.util.LinkedList;
import java.util.Collection;
import org.apache.pig.impl.util.MultiMap;
import java.util.HashSet;
import java.util.Set;

import org.apache.pig.impl.plan.DotPlanDumper;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.DotPOPrinter;
import org.apache.pig.impl.plan.OperatorPlan;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.PlanException;

/**
 * This class can print an MR plan in the DOT format. It uses
 * clusters to illustrate nesting. If "verbose" is off, it will skip
 * any nesting in the associated physical plans.
 */
public class DotMRPrinter extends DotPlanDumper<MapReduceOper, MROperPlan, 
                                  DotMRPrinter.InnerOperator, 
                                  DotMRPrinter.InnerPlan> {

    static int counter = 0;
    boolean isVerboseNesting = true;

    public DotMRPrinter(MROperPlan plan, PrintStream ps) {
        this(plan, ps, false, new HashSet<Operator>(), new HashSet<Operator>(),
             new HashSet<Operator>());
    }

    private DotMRPrinter(MROperPlan plan, PrintStream ps, boolean isSubGraph,
                         Set<Operator> subgraphs, 
                         Set<Operator> multiInputSubgraphs,
                         Set<Operator> multiOutputSubgraphs) {
        super(plan, ps, isSubGraph, subgraphs, 
              multiInputSubgraphs, multiOutputSubgraphs);
    }

    @Override
    public void setVerbose(boolean verbose) {
        // leave the parents verbose set to true
        isVerboseNesting = verbose;
    }

    @Override
    protected DotPlanDumper makeDumper(InnerPlan plan, PrintStream ps) {
        return new InnerPrinter(plan, ps, mSubgraphs, mMultiInputSubgraphs, 
                                mMultiOutputSubgraphs);
    }

    @Override
    protected String getName(MapReduceOper op) {
        String name = "Map";
        if (!op.combinePlan.isEmpty()) {
            name += " - Combine";
        }
        if (!op.reducePlan.isEmpty()) {
            name += " - Reduce";
        }
        if (op.getRequestedParallelism()!=-1) {
            name += " Parallelism: "+op.getRequestedParallelism();
        }
        name += ", Global Sort: "+op.isGlobalSort();
        return name;
    }

    @Override
    protected Collection<InnerPlan> getNestedPlans(MapReduceOper op) {
        Collection<InnerPlan> plans = new LinkedList<InnerPlan>();
        plans.add(new InnerPlan(op.mapPlan, op.combinePlan, op.reducePlan));
        return plans;
    }
    
    @Override
    protected String[] getAttributes(MapReduceOper op) {
        String[] attributes = new String[3];
        attributes[0] = "label=\""+getName(op)+"\"";
        attributes[1] = "style=\"filled\"";
        attributes[2] = "fillcolor=\"#EEEEEE\"";
        return attributes;
    }


    /**
     * Helper class to represent the relationship of map, reduce and
     * combine phases in an MR operator.
     */
    public class InnerOperator extends Operator<PlanVisitor> {
        String name;
        PhysicalPlan plan;
        int code;
        
        public InnerOperator(PhysicalPlan plan, String name) {
            super(new OperatorKey());
            this.name = name;
            this.plan = plan;
            this.code = counter++;
        }
        
        @Override public void visit(PlanVisitor v) {}
        @Override public boolean supportsMultipleInputs() {return false;}
        @Override public boolean supportsMultipleOutputs() {return false;}
        @Override public String name() {return name;}
        public PhysicalPlan getPlan() {return plan;}
        @Override public int hashCode() {return code;}
    }

    /**
     * Helper class to represent the relationship of map, reduce and
     * combine phases in an MR operator. Each MR operator will have
     * an inner plan of map -> (combine)? -> (reduce)? inner
     * operators. The inner operators contain the physical plan of the
     * execution phase.
     */
    public class InnerPlan extends OperatorPlan<InnerOperator> {
        public InnerPlan(PhysicalPlan mapPlan, PhysicalPlan combinePlan, 
                         PhysicalPlan reducePlan) {
            try {
                InnerOperator map = new InnerOperator(mapPlan, "Map");
                
                this.add(map);
                if (!combinePlan.isEmpty()) {
                    InnerOperator combine = 
                        new InnerOperator(combinePlan, "Combine");
                    InnerOperator reduce = 
                        new InnerOperator(reducePlan, "Reduce");
                    this.add(combine);
                    this.connect(map, combine);
                    this.add(reduce);
                    this.connect(combine, reduce);
                } 
                else if (!reducePlan.isEmpty()){
                    InnerOperator reduce = 
                        new InnerOperator(reducePlan, "Reduce");
                    this.add(reduce);
                    this.connect(map, reduce);
                }
            } catch (PlanException e) {}
        }
    }

    /**
     * Helper class to represent the relationship of map, reduce and
     * combine phases in an MR operator.
     */    
    private class InnerPrinter extends DotPlanDumper<InnerOperator, InnerPlan,
                                       PhysicalOperator, PhysicalPlan> {

        public InnerPrinter(InnerPlan plan, PrintStream ps,
                            Set<Operator> subgraphs, 
                            Set<Operator> multiInputSubgraphs,
                            Set<Operator> multiOutputSubgraphs) {
            super(plan, ps, true, subgraphs, multiInputSubgraphs,
                  multiOutputSubgraphs);
        }

        @Override
        protected String[] getAttributes(InnerOperator op) {
            String[] attributes = new String[3];
            attributes[0] = "label=\""+getName(op)+"\"";
            attributes[1] = "style=\"filled\"";
            attributes[2] = "fillcolor=\"white\"";
            return attributes;
        }

        @Override
        protected Collection<PhysicalPlan> getNestedPlans(InnerOperator op) {
            Collection<PhysicalPlan> l = new LinkedList<PhysicalPlan>();
            l.add(op.getPlan());
            return l;
        }

        @Override
        protected DotPOPrinter makeDumper(PhysicalPlan plan, PrintStream ps) {
            DotPOPrinter printer = new DotPOPrinter(plan, ps, true, 
                                                    mSubgraphs, 
                                                    mMultiInputSubgraphs,
                                                    mMultiOutputSubgraphs);
            printer.setVerbose(isVerboseNesting);
            return printer;
        }
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.plan.optimizer;

import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorPlan;

/**
 * A rule for optimizing a plan. The rule contains a pattern that must be
 * matched in the plan before the optimizer can consider applying the rule and a
 * transformer to do further checks and possibly transform the plan. The rule
 * pattern is expressed as a list of node names, a map of edges in the plan, and
 * a list of boolean values indicating whether the node is required. For
 * example, a rule pattern could be expressed as: [Filter, Filter] {[0, 1]}
 * [true, true], which would indicate this rule matches two nodes of class name
 * Filter, with an edge between the two, and both are required.
 */
public class Rule<O extends Operator, P extends OperatorPlan<O>> {

    public enum WalkerAlgo {
        DepthFirstWalker, DependencyOrderWalker
    };

    private RulePlan mRulePlan;
    private Transformer<O, P> mTransformer;
    private WalkerAlgo mWalkerAlgo;
    private String mRuleName = null;

    /**
     * @param plan
     *            pattern to look for
     * @param t
     *            Transformer to apply if the rule matches.
     */
    public Rule(RulePlan plan, Transformer<O, P> t, String ruleName) {
        this(plan, t, ruleName, WalkerAlgo.DependencyOrderWalker);
    }

    /**
     * @param plan
     *            pattern to look for
     * @param t
     *            Transformer to apply if the rule matches.
     * @param al
     *            Walker algorithm to find rule match within the plan.
     */
    public Rule(RulePlan plan, Transformer<O, P> t, String ruleName,
            WalkerAlgo al) {
        mRulePlan = plan;
        mTransformer = t;
        mRuleName = ruleName;
        mWalkerAlgo = al;
    }

    public RulePlan getPlan() {
        return mRulePlan;
    }

    public Transformer<O, P> getTransformer() {
        return mTransformer;
    }

    public String getRuleName() {
        return mRuleName;
    }

    public WalkerAlgo getWalkerAlgo() {
        return mWalkerAlgo;
    }

}
/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LONotEqual extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LONotEqual.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LONotEqual(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.BOOLEAN);
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "NotEqual " + mKey.scope + "-" + mKey.id;
    }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.impl.logicalLayer;

import java.util.ArrayList;
import java.util.List;

import org.apache.pig.data.DataType;
import org.apache.pig.impl.logicalLayer.parser.ParseException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOMapLookup extends ExpressionOperator {
    private static final long serialVersionUID = 2L;

    /**
     * The key to lookup along with the type and schema corresponding to the
     * type and schema of the value linked to the key
     */
    private Object mMapKey;
    private byte mValueType;
    private Schema mValueSchema;
    private static Log log = LogFactory.getLog(LOMapLookup.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param key
     *            Operator key to assign to this node.
     * @param mapKey
     *            key to look up in the map. The key is of atomic type
     * @param valueType
     *            type of the value corresponding to the key
     * @param valueSchema
     *            schema of the value if the type is tuple
     */
    public LOMapLookup(LogicalPlan plan, OperatorKey key,
            Object mapKey, byte valueType, Schema valueSchema)
            throws ParseException {
        super(plan, key);

        if (!DataType.isAtomic(DataType.findType(mapKey))) {
            throw new ParseException("Map key " + mapKey.toString()
                    + " is not atomic");
        }
        mMapKey = mapKey;
        mValueType = valueType;
        mValueSchema = valueSchema;
        mType = mValueType;
    }

    public ExpressionOperator getMap() {
        List<LogicalOperator>preds = getPlan().getPredecessors(this);
        if(preds == null)
            return null;
        return (ExpressionOperator)preds.get(0);
    }

    public Object getLookUpKey() {
        return mMapKey;
    }

    public byte getValueType() {
        return mValueType;
    }

    @Override
    public String name() {
        return "MapLookup " + mKey.scope + "-" + mKey.id;
    }

    @Override
    public boolean supportsMultipleInputs() {
        return false;
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if (!mIsFieldSchemaComputed) {
            if (DataType.isSchemaType(mValueType)) {
                mFieldSchema = new Schema.FieldSchema(null, mValueSchema, mValueType);
            } else {
                mFieldSchema = new Schema.FieldSchema(null, mValueType);
            }
            ExpressionOperator map = getMap();
            mFieldSchema.setParent(map.getFieldSchema().canonicalName, map);

            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    /**
     * @see org.apache.pig.impl.logicalLayer.ExpressionOperator#clone()
     * Do not use the clone method directly. Operators are cloned when logical plans
     * are cloned using {@link LogicalPlanCloner}
     */
    @Override
    protected Object clone() throws CloneNotSupportedException {
        LOMapLookup clone = (LOMapLookup)super.clone();
        
        // deep copy project specific attributes
        if(mValueSchema != null) {
            clone.mValueSchema = mValueSchema.clone();
        }

        return clone;
    }

}

/* 
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.pig.impl.logicalLayer;

import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.PlanVisitor;
import org.apache.pig.impl.plan.VisitorException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

public class LOSubtract extends BinaryExpressionOperator {

    private static final long serialVersionUID = 2L;
    private static Log log = LogFactory.getLog(LOSubtract.class);

    /**
     * 
     * @param plan
     *            Logical plan this operator is a part of.
     * @param k
     *            Operator key to assign to this node.
     */
    public LOSubtract(LogicalPlan plan, OperatorKey k) {
        super(plan, k);
    }

    @Override
    public Schema getSchema() {
        return mSchema;
    }

    @Override
    public Schema.FieldSchema getFieldSchema() throws FrontendException {
        if(!mIsFieldSchemaComputed) {
            mFieldSchema = new Schema.FieldSchema(null, DataType.mergeType(getLhsOperand().getType(), getRhsOperand().getType()));
            mFieldSchema.setParent(getLhsOperand().getFieldSchema().canonicalName, getLhsOperand());
            mFieldSchema.setParent(getRhsOperand().getFieldSchema().canonicalName, getRhsOperand());
            mIsFieldSchemaComputed = true;
        }
        return mFieldSchema;
    }

    @Override
    public void visit(LOVisitor v) throws VisitorException {
        v.visit(this);
    }

    @Override
    public String name() {
        return "Subtract " + mKey.scope + "-" + mKey.id;
    }
}

